+ horovodrun -np 1 python3 /workspace/electra/run_pretraining.py --model_name=test '--pretrain_tfrecords=/workspace/electra/data/tfrecord_lower_case_1_seq_len_128_random_seed_12345/books_wiki_en_corpus/train/pretrain_data*' --model_size=test --train_batch_size=44 --max_seq_length=128 --disc_weight=50.0 --generator_hidden_size=0.3333333 --num_train_steps=10000 --num_warmup_steps=500 --save_checkpoints_steps=500 --learning_rate=6e-3 --optimizer=lamb --skip_adaptive --opt_beta_1=0.878 --opt_beta_2=0.974 --lr_decay_power=0.5 --seed=42 --amp --xla --gradient_accumulation_steps=48 --log_dir /workspace/electra/results
2022-01-11 01:04:36.435923: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stderr>:2022-01-11 01:04:37.465898: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324206 - PARAMETER model_name : test 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324268 - PARAMETER model_size : test 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324292 - PARAMETER pretrain_tfrecords : /workspace/electra/data/tfrecord_lower_case_1_seq_len_128_random_seed_12345/books_wiki_en_corpus/train/pretrain_data* 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324315 - PARAMETER phase2 : False 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324333 - PARAMETER fp16_compression : False 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324348 - PARAMETER amp : True 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324364 - PARAMETER xla : True 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324379 - PARAMETER seed : 42 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324402 - PARAMETER num_train_steps : 10000 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324444 - PARAMETER num_warmup_steps : 500 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324474 - PARAMETER learning_rate : 0.006 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324518 - PARAMETER train_batch_size : 44 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324533 - PARAMETER max_seq_length : 128 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324547 - PARAMETER mask_prob : None 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324561 - PARAMETER disc_weight : 50.0 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324577 - PARAMETER generator_hidden_size : 0.3333333 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324592 - PARAMETER log_freq : 10 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324607 - PARAMETER save_checkpoints_steps : 500 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324628 - PARAMETER keep_checkpoint_max : None 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324654 - PARAMETER restore_checkpoint : None 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324673 - PARAMETER load_weights : False 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324688 - PARAMETER weights_dir : None 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324704 - PARAMETER optimizer : lamb 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324719 - PARAMETER skip_adaptive : True 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324734 - PARAMETER gradient_accumulation_steps : 48 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324750 - PARAMETER lr_decay_power : 0.5 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324768 - PARAMETER opt_beta_1 : 0.878 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324785 - PARAMETER opt_beta_2 : 0.974 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324804 - PARAMETER end_lr : 0.0 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324829 - PARAMETER log_dir : /workspace/electra/results 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324856 - PARAMETER results_dir : None 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324879 - PARAMETER skip_checkpoint : False 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324904 - PARAMETER json_summary : None 
[1,0]<stdout>:DLL 2022-01-11 01:04:38.324949 - PARAMETER NVIDIA_TENSORFLOW_VERSION : 20.07-tf2  TENSORFLOW_VERSION : 2.2.0  CUBLAS_VERSION : 11.1.0.229  NCCL_VERSION : 2.7.6  CUDA_DRIVER_VERSION : 450.51.05  CUDNN_VERSION : 8.0.1.13  CUDA_VERSION : 11.0.194  NVIDIA_PIPELINE_ID : None  NVIDIA_BUILD_ID : 14714731  NVIDIA_TF32_OVERRIDE : None 
[1,0]<stderr>:2022-01-11 01:04:38.336501: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
[1,0]<stderr>:2022-01-11 01:04:38.347838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.348409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
[1,0]<stderr>:pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
[1,0]<stderr>:coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
[1,0]<stderr>:2022-01-11 01:04:38.348451: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stderr>:2022-01-11 01:04:38.350141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
[1,0]<stderr>:2022-01-11 01:04:38.350800: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
[1,0]<stderr>:2022-01-11 01:04:38.350975: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
[1,0]<stderr>:2022-01-11 01:04:38.352575: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
[1,0]<stderr>:2022-01-11 01:04:38.352949: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
[1,0]<stderr>:2022-01-11 01:04:38.353048: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
[1,0]<stderr>:2022-01-11 01:04:38.353104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.353595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.354040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
[1,0]<stderr>:2022-01-11 01:04:38.358503: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
[1,0]<stderr>:2022-01-11 01:04:38.358939: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f641c000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
[1,0]<stderr>:2022-01-11 01:04:38.358952: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
[1,0]<stderr>:2022-01-11 01:04:38.600857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.601752: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6330000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[1,0]<stderr>:2022-01-11 01:04:38.601767: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
[1,0]<stderr>:2022-01-11 01:04:38.601951: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.602489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
[1,0]<stderr>:pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
[1,0]<stderr>:coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
[1,0]<stderr>:2022-01-11 01:04:38.602508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stderr>:2022-01-11 01:04:38.602553: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
[1,0]<stderr>:2022-01-11 01:04:38.602564: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
[1,0]<stderr>:2022-01-11 01:04:38.602574: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
[1,0]<stderr>:2022-01-11 01:04:38.602585: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
[1,0]<stderr>:2022-01-11 01:04:38.602596: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
[1,0]<stderr>:2022-01-11 01:04:38.602606: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
[1,0]<stderr>:2022-01-11 01:04:38.602652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.603122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.603653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
[1,0]<stderr>:2022-01-11 01:04:38.603678: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
[1,0]<stderr>:2022-01-11 01:04:38.782155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
[1,0]<stderr>:2022-01-11 01:04:38.782184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
[1,0]<stderr>:2022-01-11 01:04:38.782189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
[1,0]<stderr>:2022-01-11 01:04:38.782358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.782955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
[1,0]<stderr>:2022-01-11 01:04:38.783531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22442 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
[1,0]<stdout>:Compute dtype: float16
[1,0]<stdout>:Variable dtype: float32
[1,0]<stdout>:================================================================================
[1,0]<stdout>:Config:
[1,0]<stdout>:================================================================================
[1,0]<stdout>:act_func gelu
[1,0]<stdout>:amp True
[1,0]<stdout>:attention_probs_dropout_prob 0.1
[1,0]<stdout>:checkpoints_dir results/models/test/checkpoints
[1,0]<stdout>:debug False
[1,0]<stdout>:disallow_correct False
[1,0]<stdout>:disc_weight 50.0
[1,0]<stdout>:do_eval False
[1,0]<stdout>:do_lower_case True
[1,0]<stdout>:do_train True
[1,0]<stdout>:electra_objective True
[1,0]<stdout>:embedding_size 768
[1,0]<stdout>:end_lr 0.0
[1,0]<stdout>:eval_batch_size 128
[1,0]<stdout>:fp16_compression False
[1,0]<stdout>:gen_weight 1.0
[1,0]<stdout>:generator_hidden_size 0.3333333
[1,0]<stdout>:generator_layers 1.0
[1,0]<stdout>:gradient_accumulation_steps 48
[1,0]<stdout>:hidden_dropout_prob 0.1
[1,0]<stdout>:hidden_size 768
[1,0]<stdout>:json_summary None
[1,0]<stdout>:keep_checkpoint_max 5
[1,0]<stdout>:learning_rate 0.006
[1,0]<stdout>:load_weights False
[1,0]<stdout>:log_dir /workspace/electra/results
[1,0]<stdout>:log_freq 10
[1,0]<stdout>:lr_decay_power 0.5
[1,0]<stdout>:mask_prob 0.15
[1,0]<stdout>:max_predictions_per_seq 19
[1,0]<stdout>:max_seq_length 128
[1,0]<stdout>:model_dir results/models/test
[1,0]<stdout>:model_hparam_overrides {}
[1,0]<stdout>:model_name test
[1,0]<stdout>:model_size test
[1,0]<stdout>:num_attention_heads 8
[1,0]<stdout>:num_eval_steps 100
[1,0]<stdout>:num_hidden_layers 8
[1,0]<stdout>:num_train_steps 10000
[1,0]<stdout>:num_warmup_steps 500
[1,0]<stdout>:opt_beta_1 0.878
[1,0]<stdout>:opt_beta_2 0.974
[1,0]<stdout>:optimizer lamb
[1,0]<stdout>:phase2 False
[1,0]<stdout>:pretrain_tfrecords /workspace/electra/data/tfrecord_lower_case_1_seq_len_128_random_seed_12345/books_wiki_en_corpus/train/pretrain_data*
[1,0]<stdout>:restore_checkpoint None
[1,0]<stdout>:results_dir results
[1,0]<stdout>:results_pkl results/unsup_results.pkl
[1,0]<stdout>:results_txt results/unsup_results.txt
[1,0]<stdout>:save_checkpoints_steps 500
[1,0]<stdout>:seed 42
[1,0]<stdout>:shared_embeddings True
[1,0]<stdout>:skip_adaptive True
[1,0]<stdout>:skip_checkpoint False
[1,0]<stdout>:temperature 1.0
[1,0]<stdout>:train_batch_size 44
[1,0]<stdout>:uniform_generator False
[1,0]<stdout>:vocab_file vocab/vocab.txt
[1,0]<stdout>:vocab_size 30528
[1,0]<stdout>:weight_decay_rate 0.01
[1,0]<stdout>:weights_dir results/models/test/weights
[1,0]<stdout>:xla True
[1,0]<stdout>:
[1,0]<stdout>:Configuration saved in results/models/test/checkpoints/pretrain_config.json
[1,0]<stdout>:Skip list for LAMB ['layer_norm', 'bias', 'LayerNorm']
[1,0]<stdout>: ** Initializing from scratch.
[1,0]<stdout>:================================================================================
[1,0]<stdout>:Running training
[1,0]<stdout>:================================================================================
[1,0]<stderr>:WARNING:tensorflow:Layer activation is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.
[1,0]<stderr>:
[1,0]<stderr>:If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.
[1,0]<stderr>:
[1,0]<stderr>:To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.
[1,0]<stderr>:
[1,0]<stderr>:/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
[1,0]<stderr>:  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
[1,0]<stderr>:WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
[1,0]<stderr>:Instructions for updating:
[1,0]<stderr>:If using Keras pass *_constraint arguments to layers.
[1,0]<stderr>:2022-01-11 01:04:54.269463: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
[1,0]<stderr>:2022-01-11 01:04:54.630157: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.710886: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:04:54.710906: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:04:54.710913: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:04:54.710915: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:04:54.710917: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:04:54.710920: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:04:54.711107: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[1,0]<stderr>:2022-01-11 01:04:54.713191: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.754840: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:04:54.754861: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:04:54.754867: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:04:54.754870: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:04:54.754872: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:04:54.754874: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:04:54.757110: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.761341: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.765254: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.774997: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.816278: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:04:54.816299: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:04:54.816306: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:04:54.816309: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:04:54.816311: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:04:54.816313: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:04:54.820938: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.827252: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.836765: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.877803: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:04:54.877823: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:04:54.877830: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:04:54.877833: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:04:54.877835: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:04:54.877837: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:04:54.880444: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:04:54.921387: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:04:54.921406: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:04:54.921412: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:04:54.921415: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:04:54.921419: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:04:54.921423: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:05:15.973080: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
[1,0]<stderr>:2022-01-11 01:05:17.539855: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:05:17.623032: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:05:17.623053: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:05:17.623060: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:05:17.623063: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:05:17.623065: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:05:17.623067: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:05:19.891742: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
[1,0]<stderr>:2022-01-11 01:05:20.236688: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
[1,0]<stderr>:2022-01-11 01:05:20.611563: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'
[1,0]<stderr>:
[1,0]<stderr>:Relying on driver to perform ptx compilation. 
[1,0]<stderr>:Modify $PATH to customize ptxas location.
[1,0]<stderr>:This message will be only logged once.
[1,0]<stderr>:2022-01-11 01:05:20.782636: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'
[1,0]<stderr>:
[1,0]<stderr>:Relying on driver to perform ptx compilation. 
[1,0]<stderr>:Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
[1,0]<stderr>:This message will only be logged once.
[1,0]<stderr>:2022-01-11 01:09:00.836559: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:09:08.858531: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:09:08.858552: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:09:08.858576: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:09:08.858578: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:09:08.858597: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:09:08.858599: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:09:12.587869: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:09:12.890683: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:09:12.890705: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:09:12.890712: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:09:12.890715: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:09:12.890717: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:09:12.890719: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:09:14.348927: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:09:19.579018: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:09:19.579041: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:09:19.579048: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:09:19.579051: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:09:19.579053: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:09:19.579055: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:09:22.984687: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:09:23.051356: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:09:23.051380: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:09:23.051388: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:09:23.051392: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:09:23.051396: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:09:23.051401: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:09:23.157288: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:09:23.322048: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:09:23.322072: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:09:23.322098: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:09:23.322101: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:09:23.322104: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:09:23.322109: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:DLL 2022-01-11 01:09:23.446156 - Training Iteration: 0  train_perf : 0.15473774075508118  total_loss : 44.72727966308594  masked_lm_accuracy : 0.0  masked_lm_loss : 10.526106834411621  sampled_masked_lm_accuracy : 0.0  disc_loss : 0.6840006113052368  disc_auc : 0.0  disc_accuracy : 57.80442953109741  disc_precision : 0.1371428519487381  disc_recall : 0.3774574100971222 
[1,0]<stdout>:Step:     0, Loss: 44.727280, Gen_loss: 10.526107, Disc_loss:  0.684001, Gen_acc:  0.00, Disc_acc: 57.80, Perf:   0, Loss Scaler: DynamicLossScale(current_loss_scale=16384.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 4m44s, ETA: 45h54m36s, 
[1,0]<stderr>:2022-01-11 01:09:48.419612: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:09:50.664953: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
[1,0]<stderr>:2022-01-11 01:09:53.989142: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:01.873140: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:01.873176: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:01.873200: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:01.873203: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:01.873205: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:01.873208: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:10:05.595174: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:05.932231: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:05.932251: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:05.932258: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:05.932261: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:05.932263: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:05.932265: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:10:07.285815: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:12.453353: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:12.453373: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:12.453380: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:12.453383: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:12.453385: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:12.453387: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:10:15.805074: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stderr>:2022-01-11 01:10:21.028802: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:23.304278: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
[1,0]<stderr>:2022-01-11 01:10:26.697405: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:34.705902: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:34.705924: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:34.705931: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:34.705933: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:34.705936: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:34.705938: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:DLL 2022-01-11 01:10:42.892764 - Training Iteration: 0  train_perf : 452.2454833984375  total_loss : 44.6865234375  masked_lm_accuracy : 0.0028939371986780316  masked_lm_loss : 10.507104873657227  sampled_masked_lm_accuracy : 0.011575748794712126  disc_loss : 0.6838119029998779  disc_auc : 0.0  disc_accuracy : 57.628101110458374  disc_precision : 0.13844211399555206  disc_recall : 0.39681845903396606 
[1,0]<stdout>:Step:     0, Loss: 44.686523, Gen_loss: 10.507105, Disc_loss:  0.683812, Gen_acc:  0.00, Disc_acc: 57.63, Perf: 452, Loss Scaler: DynamicLossScale(current_loss_scale=4096.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 6m 3s, ETA: 266h35m41s, 
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:DLL 2022-01-11 01:10:47.633905 - Training Iteration: 0  train_perf : 472.338134765625  total_loss : 44.69149398803711  masked_lm_accuracy : 0.0  masked_lm_loss : 10.50637149810791  sampled_masked_lm_accuracy : 0.0  disc_loss : 0.683891773223877  disc_auc : 0.0  disc_accuracy : 57.37416744232178  disc_precision : 0.13887451589107513  disc_recall : 0.39942947030067444 
[1,0]<stdout>:Step:     0, Loss: 44.691494, Gen_loss: 10.506371, Disc_loss:  0.683892, Gen_acc:  0.00, Disc_acc: 57.37, Perf: 472, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 6m 8s, ETA: 279h45m52s, 
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stderr>:2022-01-11 01:10:53.101725: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:55.811850: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:55.811870: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:55.811878: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:55.811880: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:55.811883: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:55.811885: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:10:56.188643: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:56.188643: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:56.188789: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:56.188967: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:10:56.278833: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:56.278866: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:56.278876: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:56.278879: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:56.278881: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:56.278883: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:10:56.353215: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:56.353259: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:56.353274: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:56.353280: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:56.353283: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:56.353288: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:10:56.370470: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:56.370500: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:56.370510: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:56.370513: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:56.370516: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:56.370518: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:10:56.408097: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:10:56.408116: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:10:56.408122: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:10:56.408125: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:10:56.408127: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:10:56.408129: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:10:57.510998: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:11:01.533277: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:11:01.533311: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:11:01.533336: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:11:01.533338: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:11:01.533341: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:11:01.533343: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:11:04.405782: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:11:04.525751: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:11:04.525770: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:11:04.525778: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:11:04.525781: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:11:04.525783: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:11:04.525786: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stderr>:2022-01-11 01:11:04.534418: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:11:04.660478: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:11:04.660498: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:11:04.660505: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:11:04.660508: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:11:04.660510: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:11:04.660513: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stdout>:Model: "pretraining_model"
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:Layer (type)                 Output Shape              Param #   
[1,0]<stdout>:=================================================================
[1,0]<stdout>:tf_electra_for_pre_training  multiple                  81136129  
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:tf_electra_for_masked_lm (TF multiple                  30586176  
[1,0]<stdout>:=================================================================
[1,0]<stdout>:Total params: 87,880,513
[1,0]<stdout>:Trainable params: 87,880,513
[1,0]<stdout>:Non-trainable params: 0
[1,0]<stdout>:_________________________________________________________________
[1,0]<stdout>:DLL 2022-01-11 01:11:04.776091 - Training Iteration: 0  train_perf : 465.5461730957031  total_loss : 44.69419479370117  masked_lm_accuracy : 0.0028672191547229886  masked_lm_loss : 10.5021390914917  sampled_masked_lm_accuracy : 0.008601657464168966  disc_loss : 0.6840333938598633  disc_auc : 0.0  disc_accuracy : 57.39102363586426  disc_precision : 0.13887162506580353  disc_recall : 0.39760035276412964 
[1,0]<stdout>:Step:     0, Loss: 44.694195, Gen_loss: 10.502139, Disc_loss:  0.684033, Gen_acc:  0.00, Disc_acc: 57.39, Perf: 466, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 6m25s, ETA: 327h22m55s, 
[1,0]<stdout>:DLL 2022-01-11 01:11:46.178318 - Training Iteration: 10  train_perf : 544.27685546875  total_loss : 42.86429214477539  masked_lm_accuracy : 0.001450216950615868  masked_lm_loss : 10.487802505493164  sampled_masked_lm_accuracy : 0.0020303037672420032  disc_loss : 0.6477643251419067  disc_auc : 0.0  disc_accuracy : 73.02834987640381  disc_precision : 0.1394319385290146  disc_recall : 0.18315839767456055 
[1,0]<stdout>:Step:    10, Loss: 42.864292, Gen_loss: 10.487803, Disc_loss:  0.647764, Gen_acc:  0.00, Disc_acc: 73.03, Perf: 544, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=11, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 7m 7s, ETA: 107h44m44s, 
[1,0]<stdout>:DLL 2022-01-11 01:12:27.979163 - Training Iteration: 20  train_perf : 538.8364868164062  total_loss : 34.79780578613281  masked_lm_accuracy : 0.0008684777640155517  masked_lm_loss : 10.377853393554688  sampled_masked_lm_accuracy : 0.003184418528690003  disc_loss : 0.4888074994087219  disc_auc : 0.0  disc_accuracy : 86.14152669906616  disc_precision : 0.11666666716337204  disc_recall : 2.1740615920862183e-05 
[1,0]<stdout>:Step:    20, Loss: 34.797806, Gen_loss: 10.377853, Disc_loss:  0.488807, Gen_acc:  0.00, Disc_acc: 86.14, Perf: 539, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=21, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 7m48s, ETA: 61h53m59s, 
[1,0]<stdout>:DLL 2022-01-11 01:13:09.810038 - Training Iteration: 30  train_perf : 538.322265625  total_loss : 30.34786605834961  masked_lm_accuracy : 1.9369974732398987  masked_lm_loss : 10.165214538574219  sampled_masked_lm_accuracy : 0.004930222712573595  disc_loss : 0.40431323647499084  disc_auc : 0.0  disc_accuracy : 86.15105748176575  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:    30, Loss: 30.347866, Gen_loss: 10.165215, Disc_loss:  0.404313, Gen_acc:  1.94, Disc_acc: 86.15, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=31, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 8m30s, ETA: 45h37m37s, 
[1,0]<stdout>:DLL 2022-01-11 01:13:51.633628 - Training Iteration: 40  train_perf : 538.15673828125  total_loss : 29.45983123779297  masked_lm_accuracy : 5.483027175068855  masked_lm_loss : 9.872611045837402  sampled_masked_lm_accuracy : 0.014467090659309179  disc_loss : 0.392451673746109  disc_auc : 0.0  disc_accuracy : 86.15146279335022  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:    40, Loss: 29.459831, Gen_loss:  9.872611, Disc_loss:  0.392452, Gen_acc:  5.48, Disc_acc: 86.15, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=41, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 9m12s, ETA: 37h17m10s, 
[1,0]<stdout>:DLL 2022-01-11 01:14:33.514370 - Training Iteration: 50  train_perf : 537.5164184570312  total_loss : 28.18145179748535  masked_lm_accuracy : 5.697696655988693  masked_lm_loss : 9.549622535705566  sampled_masked_lm_accuracy : 0.03789785550907254  disc_loss : 0.37300074100494385  disc_auc : 0.0  disc_accuracy : 86.16160154342651  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:    50, Loss: 28.181452, Gen_loss:  9.549623, Disc_loss:  0.373001, Gen_acc:  5.70, Disc_acc: 86.16, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h 9m54s, ETA: 32h12m53s, 
[1,0]<stdout>:DLL 2022-01-11 01:15:15.389442 - Training Iteration: 60  train_perf : 537.529541015625  total_loss : 26.941652297973633  masked_lm_accuracy : 5.666211247444153  masked_lm_loss : 9.252613067626953  sampled_masked_lm_accuracy : 0.09977868758141994  disc_loss : 0.3538092076778412  disc_auc : 0.0  disc_accuracy : 86.00477576255798  disc_precision : 0.35984334349632263  disc_recall : 0.01489220466464758 
[1,0]<stdout>:Step:    60, Loss: 26.941652, Gen_loss:  9.252613, Disc_loss:  0.353809, Gen_acc:  5.67, Disc_acc: 86.00, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=61, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h10m36s, ETA: 28h48m 7s, 
[1,0]<stdout>:DLL 2022-01-11 01:15:57.205277 - Training Iteration: 70  train_perf : 537.9285888671875  total_loss : 26.046886444091797  masked_lm_accuracy : 5.59343583881855  masked_lm_loss : 8.98595905303955  sampled_masked_lm_accuracy : 0.16219164244830608  disc_loss : 0.34060272574424744  disc_auc : 0.0  disc_accuracy : 85.98979711532593  disc_precision : 0.39705950021743774  disc_recall : 0.026547623798251152 
[1,0]<stdout>:Step:    70, Loss: 26.046886, Gen_loss:  8.985959, Disc_loss:  0.340603, Gen_acc:  5.59, Disc_acc: 85.99, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=71, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h11m18s, ETA: 26h20m42s, 
[1,0]<stdout>:DLL 2022-01-11 01:16:39.054664 - Training Iteration: 80  train_perf : 537.8483276367188  total_loss : 25.064411163330078  masked_lm_accuracy : 5.63150942325592  masked_lm_loss : 8.727124214172363  sampled_masked_lm_accuracy : 0.24931237567216158  disc_loss : 0.3260194659233093  disc_auc : 0.0  disc_accuracy : 86.1301600933075  disc_precision : 0.47328871488571167  disc_recall : 0.03410141170024872 
[1,0]<stdout>:Step:    80, Loss: 25.064411, Gen_loss:  8.727124, Disc_loss:  0.326019, Gen_acc:  5.63, Disc_acc: 86.13, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=81, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h11m59s, ETA: 24h29m34s, 
[1,0]<stdout>:DLL 2022-01-11 01:17:20.896262 - Training Iteration: 90  train_perf : 537.7975463867188  total_loss : 24.524938583374023  masked_lm_accuracy : 5.755441635847092  masked_lm_loss : 8.47860336303711  sampled_masked_lm_accuracy : 0.3000700147822499  disc_loss : 0.3197261095046997  disc_auc : 0.0  disc_accuracy : 86.21379137039185  disc_precision : 0.5103871822357178  disc_recall : 0.02695508487522602 
[1,0]<stdout>:Step:    90, Loss: 24.524939, Gen_loss:  8.478603, Disc_loss:  0.319726, Gen_acc:  5.76, Disc_acc: 86.21, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=91, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h12m41s, ETA: 23h 2m42s, 
[1,0]<stdout>:DLL 2022-01-11 01:18:02.753217 - Training Iteration: 100  train_perf : 537.5955810546875  total_loss : 24.30823516845703  masked_lm_accuracy : 6.282743811607361  masked_lm_loss : 8.237401008605957  sampled_masked_lm_accuracy : 0.35100746899843216  disc_loss : 0.32010674476623535  disc_auc : 0.0  disc_accuracy : 86.26825213432312  disc_precision : 0.5232478976249695  disc_recall : 0.05986350029706955 
[1,0]<stdout>:Step:   100, Loss: 24.308235, Gen_loss:  8.237401, Disc_loss:  0.320107, Gen_acc:  6.28, Disc_acc: 86.27, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h13m23s, ETA: 21h52m56s, 
[1,0]<stdout>:DLL 2022-01-11 01:18:44.620609 - Training Iteration: 110  train_perf : 537.364501953125  total_loss : 24.53729248046875  masked_lm_accuracy : 6.706816703081131  masked_lm_loss : 7.98107385635376  sampled_masked_lm_accuracy : 0.5098003894090652  disc_loss : 0.3298161029815674  disc_auc : 0.0  disc_accuracy : 86.30865812301636  disc_precision : 0.5312327742576599  disc_recall : 0.05424630641937256 
[1,0]<stdout>:Step:   110, Loss: 24.537292, Gen_loss:  7.981074, Disc_loss:  0.329816, Gen_acc:  6.71, Disc_acc: 86.31, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=111, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h14m 5s, ETA: 20h55m37s, 
[1,0]<stdout>:DLL 2022-01-11 01:19:26.535971 - Training Iteration: 120  train_perf : 536.9618530273438  total_loss : 24.97248649597168  masked_lm_accuracy : 7.142505794763565  masked_lm_loss : 7.713692665100098  sampled_masked_lm_accuracy : 0.9576544165611267  disc_loss : 0.3439823091030121  disc_auc : 0.0  disc_accuracy : 86.37654185295105  disc_precision : 0.5511195659637451  disc_recall : 0.036251503974199295 
[1,0]<stdout>:Step:   120, Loss: 24.972486, Gen_loss:  7.713693, Disc_loss:  0.343982, Gen_acc:  7.14, Disc_acc: 86.38, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=121, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h14m47s, ETA: 20h 7m43s, 
[1,0]<stdout>:DLL 2022-01-11 01:20:08.440930 - Training Iteration: 130  train_perf : 536.9097900390625  total_loss : 25.597103118896484  masked_lm_accuracy : 7.7375441789627075  masked_lm_loss : 7.455367088317871  sampled_masked_lm_accuracy : 1.7616109922528267  disc_loss : 0.36191290616989136  disc_auc : 0.0  disc_accuracy : 86.47825717926025  disc_precision : 0.6145184636116028  disc_recall : 0.01622329279780388 
[1,0]<stdout>:Step:   130, Loss: 25.597103, Gen_loss:  7.455367, Disc_loss:  0.361913, Gen_acc:  7.74, Disc_acc: 86.48, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=131, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h15m29s, ETA: 19h27m 1s, 
[1,0]<stdout>:DLL 2022-01-11 01:20:50.312011 - Training Iteration: 140  train_perf : 537.4451293945312  total_loss : 26.219942092895508  masked_lm_accuracy : 8.239969611167908  masked_lm_loss : 7.2604289054870605  sampled_masked_lm_accuracy : 2.6155414059758186  disc_loss : 0.37840017676353455  disc_auc : 0.0  disc_accuracy : 86.55787110328674  disc_precision : 0.620362401008606  disc_recall : 0.009180168621242046 
[1,0]<stdout>:Step:   140, Loss: 26.219942, Gen_loss:  7.260429, Disc_loss:  0.378400, Gen_acc:  8.24, Disc_acc: 86.56, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=141, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h16m11s, ETA: 18h51m57s, 
[1,0]<stdout>:DLL 2022-01-11 01:21:32.194864 - Training Iteration: 150  train_perf : 537.2686767578125  total_loss : 26.40658950805664  masked_lm_accuracy : 9.054651111364365  masked_lm_loss : 7.159629821777344  sampled_masked_lm_accuracy : 3.260621428489685  disc_loss : 0.3843855559825897  disc_auc : 0.0  disc_accuracy : 86.6735577583313  disc_precision : 0.6772693991661072  disc_recall : 0.00887646060436964 
[1,0]<stdout>:Step:   150, Loss: 26.406590, Gen_loss:  7.159630, Disc_loss:  0.384386, Gen_acc:  9.05, Disc_acc: 86.67, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h16m53s, ETA: 18h21m27s, 
[1,0]<stdout>:DLL 2022-01-11 01:22:14.074588 - Training Iteration: 160  train_perf : 537.6636352539062  total_loss : 26.39321517944336  masked_lm_accuracy : 9.294036775827408  masked_lm_loss : 7.113490104675293  sampled_masked_lm_accuracy : 3.8904327899217606  disc_loss : 0.38509002327919006  disc_auc : 0.0  disc_accuracy : 86.76401376724243  disc_precision : 0.5955720543861389  disc_recall : 0.011427867226302624 
[1,0]<stdout>:Step:   160, Loss: 26.393215, Gen_loss:  7.113490, Disc_loss:  0.385090, Gen_acc:  9.29, Disc_acc: 86.76, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=161, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h17m34s, ETA: 17h54m39s, 
[1,0]<stdout>:DLL 2022-01-11 01:22:56.345591 - Training Iteration: 170  train_perf : 535.9824829101562  total_loss : 26.290813446044922  masked_lm_accuracy : 9.537774324417114  masked_lm_loss : 7.073327541351318  sampled_masked_lm_accuracy : 4.265541583299637  disc_loss : 0.38397759199142456  disc_auc : 0.0  disc_accuracy : 86.86588406562805  disc_precision : 0.8393808603286743  disc_recall : 0.010757023468613625 
[1,0]<stdout>:Step:   170, Loss: 26.290813, Gen_loss:  7.073328, Disc_loss:  0.383978, Gen_acc:  9.54, Disc_acc: 86.87, Perf: 536, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=171, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h18m17s, ETA: 17h31m16s, 
[1,0]<stdout>:DLL 2022-01-11 01:23:38.218777 - Training Iteration: 180  train_perf : 537.786865234375  total_loss : 26.135709762573242  masked_lm_accuracy : 9.902236610651016  masked_lm_loss : 7.000855922698975  sampled_masked_lm_accuracy : 4.635224863886833  disc_loss : 0.38198330998420715  disc_auc : 0.0  disc_accuracy : 86.94559335708618  disc_precision : 0.8514944911003113  disc_recall : 0.011807669885456562 
[1,0]<stdout>:Step:   180, Loss: 26.135710, Gen_loss:  7.000856, Disc_loss:  0.381983, Gen_acc:  9.90, Disc_acc: 86.95, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=181, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h18m59s, ETA: 17h10m 3s, 
[1,0]<stdout>:DLL 2022-01-11 01:24:20.057598 - Training Iteration: 190  train_perf : 537.603759765625  total_loss : 26.050432205200195  masked_lm_accuracy : 10.293861478567123  masked_lm_loss : 6.949615478515625  sampled_masked_lm_accuracy : 5.00950925052166  disc_loss : 0.38158947229385376  disc_auc : 0.0  disc_accuracy : 86.98209524154663  disc_precision : 0.8456323146820068  disc_recall : 0.010643360204994678 
[1,0]<stdout>:Step:   190, Loss: 26.050432, Gen_loss:  6.949615, Disc_loss:  0.381589, Gen_acc: 10.29, Disc_acc: 86.98, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=191, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h19m40s, ETA: 16h50m56s, 
[1,0]<stdout>:DLL 2022-01-11 01:25:01.915251 - Training Iteration: 200  train_perf : 537.6956176757812  total_loss : 25.92945098876953  masked_lm_accuracy : 11.15875095129013  masked_lm_loss : 6.893888473510742  sampled_masked_lm_accuracy : 5.4474495351314545  disc_loss : 0.38027095794677734  disc_auc : 0.0  disc_accuracy : 87.05896735191345  disc_precision : 0.8998671770095825  disc_recall : 0.011148624122142792 
[1,0]<stdout>:Step:   200, Loss: 25.929451, Gen_loss:  6.893888, Disc_loss:  0.380271, Gen_acc: 11.16, Disc_acc: 87.06, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h20m22s, ETA: 16h33m40s, 
[1,0]<stdout>:DLL 2022-01-11 01:25:43.825502 - Training Iteration: 210  train_perf : 536.9324951171875  total_loss : 25.790672302246094  masked_lm_accuracy : 12.180238217115402  masked_lm_loss : 6.825157165527344  sampled_masked_lm_accuracy : 5.850794538855553  disc_loss : 0.3789564073085785  disc_auc : 0.0  disc_accuracy : 87.13327646255493  disc_precision : 0.8662751317024231  disc_recall : 0.01267764437943697 
[1,0]<stdout>:Step:   210, Loss: 25.790672, Gen_loss:  6.825157, Disc_loss:  0.378956, Gen_acc: 12.18, Disc_acc: 87.13, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=211, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h21m 4s, ETA: 16h18m 1s, 
[1,0]<stdout>:DLL 2022-01-11 01:26:25.705673 - Training Iteration: 220  train_perf : 537.0904541015625  total_loss : 25.68059730529785  masked_lm_accuracy : 13.14181238412857  masked_lm_loss : 6.782532691955566  sampled_masked_lm_accuracy : 6.185456365346909  disc_loss : 0.37745794653892517  disc_auc : 0.0  disc_accuracy : 87.18190789222717  disc_precision : 0.8996967673301697  disc_recall : 0.01282038539648056 
[1,0]<stdout>:Step:   220, Loss: 25.680597, Gen_loss:  6.782533, Disc_loss:  0.377458, Gen_acc: 13.14, Disc_acc: 87.18, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=221, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h21m46s, ETA: 16h 3m42s, 
[1,0]<stdout>:DLL 2022-01-11 01:27:07.576315 - Training Iteration: 230  train_perf : 537.2203979492188  total_loss : 25.571062088012695  masked_lm_accuracy : 14.442338049411774  masked_lm_loss : 6.725137233734131  sampled_masked_lm_accuracy : 6.564540416002274  disc_loss : 0.3762601912021637  disc_auc : 0.0  disc_accuracy : 87.24877834320068  disc_precision : 0.9010294675827026  disc_recall : 0.012885631993412971 
[1,0]<stdout>:Step:   230, Loss: 25.571062, Gen_loss:  6.725137, Disc_loss:  0.376260, Gen_acc: 14.44, Disc_acc: 87.25, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=231, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h22m28s, ETA: 15h50m33s, 
[1,0]<stdout>:DLL 2022-01-11 01:27:49.472586 - Training Iteration: 240  train_perf : 537.1221923828125  total_loss : 25.436349868774414  masked_lm_accuracy : 15.612015128135681  masked_lm_loss : 6.668547630310059  sampled_masked_lm_accuracy : 7.12340772151947  disc_loss : 0.37493714690208435  disc_auc : 0.0  disc_accuracy : 87.33720779418945  disc_precision : 0.8917353749275208  disc_recall : 0.013412002474069595 
[1,0]<stdout>:Step:   240, Loss: 25.436350, Gen_loss:  6.668548, Disc_loss:  0.374937, Gen_acc: 15.61, Disc_acc: 87.34, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=241, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h23m10s, ETA: 15h38m28s, 
[1,0]<stdout>:DLL 2022-01-11 01:28:31.386547 - Training Iteration: 250  train_perf : 536.6930541992188  total_loss : 25.32834815979004  masked_lm_accuracy : 16.35982245206833  masked_lm_loss : 6.62119722366333  sampled_masked_lm_accuracy : 7.656334340572357  disc_loss : 0.37368667125701904  disc_auc : 0.0  disc_accuracy : 87.39321827888489  disc_precision : 0.8527358770370483  disc_recall : 0.012801790609955788 
[1,0]<stdout>:Step:   250, Loss: 25.328348, Gen_loss:  6.621197, Disc_loss:  0.373687, Gen_acc: 16.36, Disc_acc: 87.39, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h23m52s, ETA: 15h27m17s, 
[1,0]<stdout>:DLL 2022-01-11 01:29:13.285967 - Training Iteration: 260  train_perf : 537.0771484375  total_loss : 25.20560646057129  masked_lm_accuracy : 17.557169497013092  masked_lm_loss : 6.57595157623291  sampled_masked_lm_accuracy : 8.167196810245514  disc_loss : 0.37203532457351685  disc_auc : 0.0  disc_accuracy : 87.46914267539978  disc_precision : 0.8574811816215515  disc_recall : 0.013536108657717705 
[1,0]<stdout>:Step:   260, Loss: 25.205606, Gen_loss:  6.575952, Disc_loss:  0.372035, Gen_acc: 17.56, Disc_acc: 87.47, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=261, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h24m34s, ETA: 15h16m54s, 
[1,0]<stdout>:DLL 2022-01-11 01:29:55.169436 - Training Iteration: 270  train_perf : 537.2517700195312  total_loss : 25.07465362548828  masked_lm_accuracy : 17.600153386592865  masked_lm_loss : 6.532777786254883  sampled_masked_lm_accuracy : 8.789633959531784  disc_loss : 0.36997634172439575  disc_auc : 0.0  disc_accuracy : 87.57455348968506  disc_precision : 0.9223112463951111  disc_recall : 0.013026021420955658 
[1,0]<stdout>:Step:   270, Loss: 25.074654, Gen_loss:  6.532778, Disc_loss:  0.369976, Gen_acc: 17.60, Disc_acc: 87.57, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=271, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h25m16s, ETA: 15h 7m13s, 
[1,0]<stdout>:DLL 2022-01-11 01:30:37.101574 - Training Iteration: 280  train_perf : 536.6273193359375  total_loss : 24.97235679626465  masked_lm_accuracy : 17.800477147102356  masked_lm_loss : 6.494489669799805  sampled_masked_lm_accuracy : 9.283536672592163  disc_loss : 0.3689012825489044  disc_auc : 0.0  disc_accuracy : 87.64219880104065  disc_precision : 0.9398742318153381  disc_recall : 0.012830903753638268 
[1,0]<stdout>:Step:   280, Loss: 24.972357, Gen_loss:  6.494490, Disc_loss:  0.368901, Gen_acc: 17.80, Disc_acc: 87.64, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=281, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h25m58s, ETA: 14h58m13s, 
[1,0]<stdout>:DLL 2022-01-11 01:31:18.994786 - Training Iteration: 290  train_perf : 537.3370361328125  total_loss : 24.903278350830078  masked_lm_accuracy : 17.955125868320465  masked_lm_loss : 6.470591068267822  sampled_masked_lm_accuracy : 9.628871828317642  disc_loss : 0.3682449162006378  disc_auc : 0.0  disc_accuracy : 87.71708011627197  disc_precision : 0.9254175424575806  disc_recall : 0.014010355807840824 
[1,0]<stdout>:Step:   290, Loss: 24.903278, Gen_loss:  6.470591, Disc_loss:  0.368245, Gen_acc: 17.96, Disc_acc: 87.72, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=291, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h26m39s, ETA: 14h49m45s, 
[1,0]<stdout>:DLL 2022-01-11 01:32:00.825280 - Training Iteration: 300  train_perf : 537.5272827148438  total_loss : 24.868764877319336  masked_lm_accuracy : 18.231165409088135  masked_lm_loss : 6.445173740386963  sampled_masked_lm_accuracy : 10.128778964281082  disc_loss : 0.36800405383110046  disc_auc : 0.0  disc_accuracy : 87.71587014198303  disc_precision : 0.7425501942634583  disc_recall : 0.012714600190520287 
[1,0]<stdout>:Step:   300, Loss: 24.868765, Gen_loss:  6.445174, Disc_loss:  0.368004, Gen_acc: 18.23, Disc_acc: 87.72, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h27m21s, ETA: 14h41m46s, 
[1,0]<stdout>:DLL 2022-01-11 01:32:42.680890 - Training Iteration: 310  train_perf : 537.4756469726562  total_loss : 24.729036331176758  masked_lm_accuracy : 18.52341741323471  masked_lm_loss : 6.404317855834961  sampled_masked_lm_accuracy : 10.679546743631363  disc_loss : 0.36606839299201965  disc_auc : 0.0  disc_accuracy : 87.84022331237793  disc_precision : 0.9588350653648376  disc_recall : 0.012003968469798565 
[1,0]<stdout>:Step:   310, Loss: 24.729036, Gen_loss:  6.404318, Disc_loss:  0.366068, Gen_acc: 18.52, Disc_acc: 87.84, Perf: 537, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=311, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h28m 3s, ETA: 14h34m16s, 
[1,0]<stdout>:DLL 2022-01-11 01:33:24.539626 - Training Iteration: 320  train_perf : 537.6152954101562  total_loss : 24.600605010986328  masked_lm_accuracy : 18.684324622154236  masked_lm_loss : 6.3624138832092285  sampled_masked_lm_accuracy : 11.157171428203583  disc_loss : 0.3642560839653015  disc_auc : 0.0  disc_accuracy : 87.91853785514832  disc_precision : 0.9285714030265808  disc_recall : 0.01293014083057642 
[1,0]<stdout>:Step:   320, Loss: 24.600605, Gen_loss:  6.362414, Disc_loss:  0.364256, Gen_acc: 18.68, Disc_acc: 87.92, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=321, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h28m45s, ETA: 14h27m12s, 
[1,0]<stdout>:DLL 2022-01-11 01:34:06.392601 - Training Iteration: 330  train_perf : 537.5625  total_loss : 24.544824600219727  masked_lm_accuracy : 18.626777827739716  masked_lm_loss : 6.349381923675537  sampled_masked_lm_accuracy : 11.548943817615509  disc_loss : 0.3632436692714691  disc_auc : 0.0  disc_accuracy : 87.9157543182373  disc_precision : 0.6552538275718689  disc_recall : 0.015702806413173676 
[1,0]<stdout>:Step:   330, Loss: 24.544825, Gen_loss:  6.349382, Disc_loss:  0.363244, Gen_acc: 18.63, Disc_acc: 87.92, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=331, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h29m27s, ETA: 14h20m31s, 
[1,0]<stdout>:DLL 2022-01-11 01:34:48.217192 - Training Iteration: 340  train_perf : 537.895751953125  total_loss : 24.450700759887695  masked_lm_accuracy : 18.741917610168457  masked_lm_loss : 6.307971954345703  sampled_masked_lm_accuracy : 11.845515668392181  disc_loss : 0.36227113008499146  disc_auc : 0.0  disc_accuracy : 88.0112886428833  disc_precision : 0.9444889426231384  disc_recall : 0.012572606094181538 
[1,0]<stdout>:Step:   340, Loss: 24.450701, Gen_loss:  6.307972, Disc_loss:  0.362271, Gen_acc: 18.74, Disc_acc: 88.01, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=341, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h30m 9s, ETA: 14h14m10s, 
[1,0]<stdout>:DLL 2022-01-11 01:35:30.047214 - Training Iteration: 350  train_perf : 537.687744140625  total_loss : 24.457561492919922  masked_lm_accuracy : 18.361657857894897  masked_lm_loss : 6.319159030914307  sampled_masked_lm_accuracy : 12.058432400226593  disc_loss : 0.3621358871459961  disc_auc : 0.0  disc_accuracy : 88.0294144153595  disc_precision : 0.9053322076797485  disc_recall : 0.011264617554843426 
[1,0]<stdout>:Step:   350, Loss: 24.457561, Gen_loss:  6.319159, Disc_loss:  0.362136, Gen_acc: 18.36, Disc_acc: 88.03, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h30m50s, ETA: 14h 8m 8s, 
[1,0]<stdout>:DLL 2022-01-11 01:36:11.890787 - Training Iteration: 360  train_perf : 537.533447265625  total_loss : 24.408626556396484  masked_lm_accuracy : 18.480966985225677  masked_lm_loss : 6.294990062713623  sampled_masked_lm_accuracy : 12.341748923063278  disc_loss : 0.3617059886455536  disc_auc : 0.0  disc_accuracy : 88.04636597633362  disc_precision : 0.9437946677207947  disc_recall : 0.010390957817435265 
[1,0]<stdout>:Step:   360, Loss: 24.408627, Gen_loss:  6.294990, Disc_loss:  0.361706, Gen_acc: 18.48, Disc_acc: 88.05, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=361, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h31m32s, ETA: 14h 2m24s, 
[1,0]<stdout>:DLL 2022-01-11 01:36:53.740264 - Training Iteration: 370  train_perf : 537.8292236328125  total_loss : 24.381587982177734  masked_lm_accuracy : 18.494069576263428  masked_lm_loss : 6.265481948852539  sampled_masked_lm_accuracy : 12.588706612586975  disc_loss : 0.3616776764392853  disc_auc : 0.0  disc_accuracy : 88.01940679550171  disc_precision : 0.5665910840034485  disc_recall : 0.013422578573226929 
[1,0]<stdout>:Step:   370, Loss: 24.381588, Gen_loss:  6.265482, Disc_loss:  0.361678, Gen_acc: 18.49, Disc_acc: 88.02, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=371, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h32m14s, ETA: 13h56m57s, 
[1,0]<stdout>:DLL 2022-01-11 01:37:35.554328 - Training Iteration: 380  train_perf : 537.8572998046875  total_loss : 24.352924346923828  masked_lm_accuracy : 18.39384287595749  masked_lm_loss : 6.275592803955078  sampled_masked_lm_accuracy : 12.759436666965485  disc_loss : 0.36096638441085815  disc_auc : 0.0  disc_accuracy : 88.14916610717773  disc_precision : 0.9540372490882874  disc_recall : 0.011013831943273544 
[1,0]<stdout>:Step:   380, Loss: 24.352924, Gen_loss:  6.275593, Disc_loss:  0.360966, Gen_acc: 18.39, Disc_acc: 88.15, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=381, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h32m56s, ETA: 13h51m44s, 
[1,0]<stdout>:DLL 2022-01-11 01:38:17.322510 - Training Iteration: 390  train_perf : 538.4952392578125  total_loss : 24.62262535095215  masked_lm_accuracy : 18.50561648607254  masked_lm_loss : 6.242886066436768  sampled_masked_lm_accuracy : 12.906962633132935  disc_loss : 0.3687046468257904  disc_auc : 0.0  disc_accuracy : 87.90485858917236  disc_precision : 0.31736886501312256  disc_recall : 0.009513597935438156 
[1,0]<stdout>:Step:   390, Loss: 24.622625, Gen_loss:  6.242886, Disc_loss:  0.368705, Gen_acc: 18.51, Disc_acc: 87.90, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=391, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h33m38s, ETA: 13h46m44s, 
[1,0]<stdout>:DLL 2022-01-11 01:38:59.110313 - Training Iteration: 400  train_perf : 538.0213623046875  total_loss : 24.24983024597168  masked_lm_accuracy : 18.567603826522827  masked_lm_loss : 6.220954418182373  sampled_masked_lm_accuracy : 13.034294545650482  disc_loss : 0.3601475656032562  disc_auc : 0.0  disc_accuracy : 88.1574034690857  disc_precision : 0.9366497993469238  disc_recall : 0.008694334886968136 
[1,0]<stdout>:Step:   400, Loss: 24.249830, Gen_loss:  6.220954, Disc_loss:  0.360148, Gen_acc: 18.57, Disc_acc: 88.16, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=2048.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h34m20s, ETA: 13h41m57s, 
[1,0]<stdout>:DLL 2022-01-11 01:39:49.234199 - Training Iteration: 410  train_perf : 538.3588256835938  total_loss : 24.305133819580078  masked_lm_accuracy : 18.69845986366272  masked_lm_loss : 6.21291971206665  sampled_masked_lm_accuracy : 13.24293166399002  disc_loss : 0.3615053594112396  disc_auc : 0.0  disc_accuracy : 88.15475106239319  disc_precision : 0.7643518447875977  disc_recall : 0.00994263868778944 
[1,0]<stdout>:Step:   410, Loss: 24.305134, Gen_loss:  6.212920, Disc_loss:  0.361505, Gen_acc: 18.70, Disc_acc: 88.15, Perf: 538, Loss Scaler: DynamicLossScale(current_loss_scale=512.0, num_good_steps=2, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h35m10s, ETA: 13h40m36s, 
[1,0]<stdout>:DLL 2022-01-11 01:40:30.947300 - Training Iteration: 420  train_perf : 539.4689331054688  total_loss : 24.262102127075195  masked_lm_accuracy : 18.602752685546875  masked_lm_loss : 6.214118957519531  sampled_masked_lm_accuracy : 13.15261721611023  disc_loss : 0.36041784286499023  disc_auc : 0.0  disc_accuracy : 88.0977213382721  disc_precision : 0.6627907156944275  disc_recall : 0.005138319917023182 
[1,0]<stdout>:Step:   420, Loss: 24.262102, Gen_loss:  6.214119, Disc_loss:  0.360418, Gen_acc: 18.60, Disc_acc: 88.10, Perf: 539, Loss Scaler: DynamicLossScale(current_loss_scale=512.0, num_good_steps=12, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h35m51s, ETA: 13h36m 6s, 
[1,0]<stdout>:DLL 2022-01-11 01:41:12.670656 - Training Iteration: 430  train_perf : 539.44580078125  total_loss : 24.231361389160156  masked_lm_accuracy : 18.767040967941284  masked_lm_loss : 6.181169509887695  sampled_masked_lm_accuracy : 13.343213498592377  disc_loss : 0.36032384634017944  disc_auc : 0.0  disc_accuracy : 88.1428062915802  disc_precision : 0.9826147556304932  disc_recall : 0.005101691000163555 
[1,0]<stdout>:Step:   430, Loss: 24.231361, Gen_loss:  6.181170, Disc_loss:  0.360324, Gen_acc: 18.77, Disc_acc: 88.14, Perf: 539, Loss Scaler: DynamicLossScale(current_loss_scale=512.0, num_good_steps=22, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h36m33s, ETA: 13h31m46s, 
[1,0]<stdout>:DLL 2022-01-11 01:41:54.348336 - Training Iteration: 440  train_perf : 539.925048828125  total_loss : 24.169200897216797  masked_lm_accuracy : 18.855445086956024  masked_lm_loss : 6.155038833618164  sampled_masked_lm_accuracy : 13.561172783374786  disc_loss : 0.3594021201133728  disc_auc : 0.0  disc_accuracy : 88.18599581718445  disc_precision : 0.9603615999221802  disc_recall : 0.004997069016098976 
[1,0]<stdout>:Step:   440, Loss: 24.169201, Gen_loss:  6.155039, Disc_loss:  0.359402, Gen_acc: 18.86, Disc_acc: 88.19, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=512.0, num_good_steps=32, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h37m15s, ETA: 13h27m36s, 
[1,0]<stdout>:DLL 2022-01-11 01:42:36.066043 - Training Iteration: 450  train_perf : 539.4237670898438  total_loss : 24.446989059448242  masked_lm_accuracy : 18.665921688079834  masked_lm_loss : 6.164017677307129  sampled_masked_lm_accuracy : 13.671667873859406  disc_loss : 0.3658643662929535  disc_auc : 0.0  disc_accuracy : 87.95865178108215  disc_precision : 0.3125116229057312  disc_recall : 0.012172790244221687 
[1,0]<stdout>:Step:   450, Loss: 24.446989, Gen_loss:  6.164018, Disc_loss:  0.365864, Gen_acc: 18.67, Disc_acc: 87.96, Perf: 539, Loss Scaler: DynamicLossScale(current_loss_scale=512.0, num_good_steps=42, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h37m56s, ETA: 13h23m35s, 
[1,0]<stdout>:DLL 2022-01-11 01:43:21.931706 - Training Iteration: 460  train_perf : 539.6419677734375  total_loss : 24.526395797729492  masked_lm_accuracy : 18.342912197113037  masked_lm_loss : 6.171722412109375  sampled_masked_lm_accuracy : 13.518297672271729  disc_loss : 0.36798804998397827  disc_auc : 0.0  disc_accuracy : 88.115394115448  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   460, Loss: 24.526396, Gen_loss:  6.171722, Disc_loss:  0.367988, Gen_acc: 18.34, Disc_acc: 88.12, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=256.0, num_good_steps=4, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h38m42s, ETA: 13h21m 9s, 
[1,0]<stdout>:DLL 2022-01-11 01:44:03.597689 - Training Iteration: 470  train_perf : 539.964111328125  total_loss : 24.36014747619629  masked_lm_accuracy : 18.496842682361603  masked_lm_loss : 6.151749134063721  sampled_masked_lm_accuracy : 13.632920384407043  disc_loss : 0.36407285928726196  disc_auc : 0.0  disc_accuracy : 88.13467025756836  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   470, Loss: 24.360147, Gen_loss:  6.151749, Disc_loss:  0.364073, Gen_acc: 18.50, Disc_acc: 88.13, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=256.0, num_good_steps=14, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h39m24s, ETA: 13h17m22s, 
[1,0]<stdout>:DLL 2022-01-11 01:44:49.436724 - Training Iteration: 480  train_perf : 539.7019653320312  total_loss : 24.32836151123047  masked_lm_accuracy : 18.696393072605133  masked_lm_loss : 6.131225109100342  sampled_masked_lm_accuracy : 13.693554699420929  disc_loss : 0.3642769753932953  disc_auc : 0.0  disc_accuracy : 88.14452290534973  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   480, Loss: 24.328362, Gen_loss:  6.131225, Disc_loss:  0.364277, Gen_acc: 18.70, Disc_acc: 88.14, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=8, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h40m10s, ETA: 13h15m 6s, 
[1,0]<stdout>:DLL 2022-01-11 01:45:31.096895 - Training Iteration: 490  train_perf : 540.2269897460938  total_loss : 24.327903747558594  masked_lm_accuracy : 19.072386622428894  masked_lm_loss : 6.118912696838379  sampled_masked_lm_accuracy : 13.803459703922272  disc_loss : 0.3642052114009857  disc_auc : 0.0  disc_accuracy : 88.15937638282776  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   490, Loss: 24.327904, Gen_loss:  6.118913, Disc_loss:  0.364205, Gen_acc: 19.07, Disc_acc: 88.16, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=18, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h40m52s, ETA: 13h11m32s, 
[1,0]<stdout>: ** Saved model checkpoint for step 493: results/models/test/checkpoints/ckpt-493
[1,0]<stdout>: ** Saved iterator checkpoint for step 493: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-493
[1,0]<stderr>:2022-01-11 01:46:13.659136: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
[1,0]<stderr>:2022-01-11 01:46:13.780806: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
[1,0]<stderr>:2022-01-11 01:46:13.780826: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
[1,0]<stderr>:2022-01-11 01:46:13.780834: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
[1,0]<stderr>:2022-01-11 01:46:13.780836: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
[1,0]<stderr>:2022-01-11 01:46:13.780838: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
[1,0]<stderr>:2022-01-11 01:46:13.780841: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
[1,0]<stdout>:DLL 2022-01-11 01:46:13.835993 - Training Iteration: 500  train_perf : 539.9861450195312  total_loss : 24.339567184448242  masked_lm_accuracy : 18.71078759431839  masked_lm_loss : 6.132354259490967  sampled_masked_lm_accuracy : 13.710178434848785  disc_loss : 0.36464425921440125  disc_auc : 0.0  disc_accuracy : 88.15193772315979  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   500, Loss: 24.339567, Gen_loss:  6.132354, Disc_loss:  0.364644, Gen_acc: 18.71, Disc_acc: 88.15, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=28, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h41m34s, ETA: 13h 8m25s, 
[1,0]<stdout>:DLL 2022-01-11 01:46:55.507832 - Training Iteration: 510  train_perf : 540.2935180664062  total_loss : 24.266319274902344  masked_lm_accuracy : 19.02710199356079  masked_lm_loss : 6.091798782348633  sampled_masked_lm_accuracy : 13.930004835128784  disc_loss : 0.36379897594451904  disc_auc : 0.0  disc_accuracy : 88.18355202674866  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   510, Loss: 24.266319, Gen_loss:  6.091799, Disc_loss:  0.363799, Gen_acc: 19.03, Disc_acc: 88.18, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=38, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h42m16s, ETA: 13h 5m 5s, 
[1,0]<stdout>:DLL 2022-01-11 01:47:37.119579 - Training Iteration: 520  train_perf : 540.6812133789062  total_loss : 24.20760154724121  masked_lm_accuracy : 19.090743362903595  masked_lm_loss : 6.059229850769043  sampled_masked_lm_accuracy : 14.05133605003357  disc_loss : 0.3630700409412384  disc_auc : 0.0  disc_accuracy : 88.20769786834717  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   520, Loss: 24.207602, Gen_loss:  6.059230, Disc_loss:  0.363070, Gen_acc: 19.09, Disc_acc: 88.21, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=48, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h42m58s, ETA: 13h 1m49s, 
[1,0]<stdout>:DLL 2022-01-11 01:48:18.766031 - Training Iteration: 530  train_perf : 540.6483764648438  total_loss : 24.216861724853516  masked_lm_accuracy : 19.207942485809326  masked_lm_loss : 6.052646160125732  sampled_masked_lm_accuracy : 14.039762318134308  disc_loss : 0.36334291100502014  disc_auc : 0.0  disc_accuracy : 88.18992972373962  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   530, Loss: 24.216862, Gen_loss:  6.052646, Disc_loss:  0.363343, Gen_acc: 19.21, Disc_acc: 88.19, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=58, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h43m39s, ETA: 12h58m40s, 
[1,0]<stdout>:DLL 2022-01-11 01:49:00.392792 - Training Iteration: 540  train_perf : 540.6713256835938  total_loss : 24.195940017700195  masked_lm_accuracy : 19.210700690746307  masked_lm_loss : 6.0365447998046875  sampled_masked_lm_accuracy : 14.091888070106506  disc_loss : 0.363244891166687  disc_auc : 0.0  disc_accuracy : 88.1948471069336  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   540, Loss: 24.195940, Gen_loss:  6.036545, Disc_loss:  0.363245, Gen_acc: 19.21, Disc_acc: 88.19, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=68, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h44m21s, ETA: 12h55m36s, 
[1,0]<stdout>:DLL 2022-01-11 01:49:42.115949 - Training Iteration: 550  train_perf : 539.7918701171875  total_loss : 24.19949722290039  masked_lm_accuracy : 19.235248863697052  masked_lm_loss : 6.040648937225342  sampled_masked_lm_accuracy : 14.052225649356842  disc_loss : 0.36342141032218933  disc_auc : 0.0  disc_accuracy : 88.19704055786133  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   550, Loss: 24.199497, Gen_loss:  6.040649, Disc_loss:  0.363421, Gen_acc: 19.24, Disc_acc: 88.20, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=78, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h45m 3s, ETA: 12h52m38s, 
[1,0]<stdout>:DLL 2022-01-11 01:50:23.786984 - Training Iteration: 560  train_perf : 540.2003784179688  total_loss : 24.19751739501953  masked_lm_accuracy : 19.30159628391266  masked_lm_loss : 6.043323993682861  sampled_masked_lm_accuracy : 14.077341556549072  disc_loss : 0.3633536696434021  disc_auc : 0.0  disc_accuracy : 88.20536136627197  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   560, Loss: 24.197517, Gen_loss:  6.043324, Disc_loss:  0.363354, Gen_acc: 19.30, Disc_acc: 88.21, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=88, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h45m44s, ETA: 12h49m45s, 
[1,0]<stdout>:DLL 2022-01-11 01:51:05.442637 - Training Iteration: 570  train_perf : 540.2703247070312  total_loss : 24.141338348388672  masked_lm_accuracy : 19.680601358413696  masked_lm_loss : 6.004481792449951  sampled_masked_lm_accuracy : 14.24897313117981  disc_loss : 0.3625205159187317  disc_auc : 0.0  disc_accuracy : 88.22052478790283  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   570, Loss: 24.141338, Gen_loss:  6.004482, Disc_loss:  0.362521, Gen_acc: 19.68, Disc_acc: 88.22, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=98, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h46m26s, ETA: 12h46m56s, 
[1,0]<stdout>:DLL 2022-01-11 01:51:47.089804 - Training Iteration: 580  train_perf : 540.5491333007812  total_loss : 24.190834045410156  masked_lm_accuracy : 19.365200400352478  masked_lm_loss : 6.0205397605896  sampled_masked_lm_accuracy : 14.00015652179718  disc_loss : 0.3633864223957062  disc_auc : 0.0  disc_accuracy : 88.19182515144348  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   580, Loss: 24.190834, Gen_loss:  6.020540, Disc_loss:  0.363386, Gen_acc: 19.37, Disc_acc: 88.19, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=108, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h47m 8s, ETA: 12h44m11s, 
[1,0]<stdout>:DLL 2022-01-11 01:52:28.710339 - Training Iteration: 590  train_perf : 540.61669921875  total_loss : 24.0787353515625  masked_lm_accuracy : 19.86127644777298  masked_lm_loss : 5.9635443687438965  sampled_masked_lm_accuracy : 14.385175704956055  disc_loss : 0.3620262145996094  disc_auc : 0.0  disc_accuracy : 88.2412850856781  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   590, Loss: 24.078735, Gen_loss:  5.963544, Disc_loss:  0.362026, Gen_acc: 19.86, Disc_acc: 88.24, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=118, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h47m49s, ETA: 12h41m30s, 
[1,0]<stdout>:DLL 2022-01-11 01:53:10.375710 - Training Iteration: 600  train_perf : 540.1087036132812  total_loss : 24.061553955078125  masked_lm_accuracy : 19.79549527168274  masked_lm_loss : 5.9471259117126465  sampled_masked_lm_accuracy : 14.376965165138245  disc_loss : 0.36244329810142517  disc_auc : 0.0  disc_accuracy : 88.24355006217957  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   600, Loss: 24.061554, Gen_loss:  5.947126, Disc_loss:  0.362443, Gen_acc: 19.80, Disc_acc: 88.24, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=128, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h48m31s, ETA: 12h38m54s, 
[1,0]<stdout>:DLL 2022-01-11 01:53:52.048219 - Training Iteration: 610  train_perf : 540.1820678710938  total_loss : 24.044536590576172  masked_lm_accuracy : 19.953273236751556  masked_lm_loss : 5.923011779785156  sampled_masked_lm_accuracy : 14.370889961719513  disc_loss : 0.3625909090042114  disc_auc : 0.0  disc_accuracy : 88.23741674423218  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   610, Loss: 24.044537, Gen_loss:  5.923012, Disc_loss:  0.362591, Gen_acc: 19.95, Disc_acc: 88.24, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=138, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h49m12s, ETA: 12h36m21s, 
[1,0]<stdout>:DLL 2022-01-11 01:54:33.725559 - Training Iteration: 620  train_perf : 540.0820922851562  total_loss : 24.017024993896484  masked_lm_accuracy : 20.27641534805298  masked_lm_loss : 5.902698516845703  sampled_masked_lm_accuracy : 14.500315487384796  disc_loss : 0.3617263436317444  disc_auc : 0.0  disc_accuracy : 88.24326395988464  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   620, Loss: 24.017025, Gen_loss:  5.902699, Disc_loss:  0.361726, Gen_acc: 20.28, Disc_acc: 88.24, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=148, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h49m54s, ETA: 12h33m53s, 
[1,0]<stdout>:DLL 2022-01-11 01:55:15.382723 - Training Iteration: 630  train_perf : 540.1281127929688  total_loss : 24.025062561035156  masked_lm_accuracy : 20.07223516702652  masked_lm_loss : 5.899357318878174  sampled_masked_lm_accuracy : 14.324131608009338  disc_loss : 0.3621845245361328  disc_auc : 0.0  disc_accuracy : 88.23296427726746  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   630, Loss: 24.025063, Gen_loss:  5.899357, Disc_loss:  0.362185, Gen_acc: 20.07, Disc_acc: 88.23, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=158, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h50m36s, ETA: 12h31m27s, 
[1,0]<stdout>:DLL 2022-01-11 01:55:57.063540 - Training Iteration: 640  train_perf : 540.124755859375  total_loss : 23.981346130371094  masked_lm_accuracy : 20.40996551513672  masked_lm_loss : 5.872259140014648  sampled_masked_lm_accuracy : 14.53491747379303  disc_loss : 0.3619922995567322  disc_auc : 0.0  disc_accuracy : 88.25217485427856  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   640, Loss: 23.981346, Gen_loss:  5.872259, Disc_loss:  0.361992, Gen_acc: 20.41, Disc_acc: 88.25, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=168, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h51m17s, ETA: 12h29m 5s, 
[1,0]<stdout>:DLL 2022-01-11 01:56:38.685508 - Training Iteration: 650  train_perf : 540.3375854492188  total_loss : 23.94575309753418  masked_lm_accuracy : 20.737630128860474  masked_lm_loss : 5.853643894195557  sampled_masked_lm_accuracy : 14.56487625837326  disc_loss : 0.36136001348495483  disc_auc : 0.0  disc_accuracy : 88.26841711997986  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   650, Loss: 23.945753, Gen_loss:  5.853644, Disc_loss:  0.361360, Gen_acc: 20.74, Disc_acc: 88.27, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=178, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h51m59s, ETA: 12h26m45s, 
[1,0]<stdout>:DLL 2022-01-11 01:57:20.320269 - Training Iteration: 660  train_perf : 540.413330078125  total_loss : 23.93644142150879  masked_lm_accuracy : 20.886263251304626  masked_lm_loss : 5.836409568786621  sampled_masked_lm_accuracy : 14.566554129123688  disc_loss : 0.3618733584880829  disc_auc : 0.0  disc_accuracy : 88.25843930244446  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   660, Loss: 23.936441, Gen_loss:  5.836410, Disc_loss:  0.361873, Gen_acc: 20.89, Disc_acc: 88.26, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=188, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h52m41s, ETA: 12h24m28s, 
[1,0]<stdout>:DLL 2022-01-11 01:58:01.980339 - Training Iteration: 670  train_perf : 540.132568359375  total_loss : 23.905120849609375  masked_lm_accuracy : 20.93109041452408  masked_lm_loss : 5.826069355010986  sampled_masked_lm_accuracy : 14.677970111370087  disc_loss : 0.36085259914398193  disc_auc : 0.0  disc_accuracy : 88.27421069145203  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   670, Loss: 23.905121, Gen_loss:  5.826069, Disc_loss:  0.360853, Gen_acc: 20.93, Disc_acc: 88.27, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=198, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h53m22s, ETA: 12h22m15s, 
[1,0]<stdout>:DLL 2022-01-11 01:58:43.643985 - Training Iteration: 680  train_perf : 540.1006469726562  total_loss : 23.839181900024414  masked_lm_accuracy : 21.296194195747375  masked_lm_loss : 5.781700611114502  sampled_masked_lm_accuracy : 14.903692901134491  disc_loss : 0.3605054020881653  disc_auc : 0.0  disc_accuracy : 88.298100233078  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   680, Loss: 23.839182, Gen_loss:  5.781701, Disc_loss:  0.360505, Gen_acc: 21.30, Disc_acc: 88.30, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=208, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h54m 4s, ETA: 12h20m 4s, 
[1,0]<stdout>:DLL 2022-01-11 01:59:25.301945 - Training Iteration: 690  train_perf : 540.3546752929688  total_loss : 23.731294631958008  masked_lm_accuracy : 22.019466757774353  masked_lm_loss : 5.7105021476745605  sampled_masked_lm_accuracy : 15.104766190052032  disc_loss : 0.35950541496276855  disc_auc : 0.0  disc_accuracy : 88.33626508712769  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   690, Loss: 23.731295, Gen_loss:  5.710502, Disc_loss:  0.359505, Gen_acc: 22.02, Disc_acc: 88.34, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=218, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h54m46s, ETA: 12h17m56s, 
[1,0]<stdout>:DLL 2022-01-11 02:00:06.975889 - Training Iteration: 700  train_perf : 539.9307861328125  total_loss : 23.663999557495117  masked_lm_accuracy : 22.54207283258438  masked_lm_loss : 5.66146183013916  sampled_masked_lm_accuracy : 15.277644991874695  disc_loss : 0.35915395617485046  disc_auc : 0.0  disc_accuracy : 88.35330009460449  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   700, Loss: 23.664000, Gen_loss:  5.661462, Disc_loss:  0.359154, Gen_acc: 22.54, Disc_acc: 88.35, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=228, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h55m27s, ETA: 12h15m50s, 
[1,0]<stdout>:DLL 2022-01-11 02:00:48.667554 - Training Iteration: 710  train_perf : 540.1641235351562  total_loss : 23.546316146850586  masked_lm_accuracy : 23.18360209465027  masked_lm_loss : 5.593032360076904  sampled_masked_lm_accuracy : 15.688513219356537  disc_loss : 0.35826340317726135  disc_auc : 0.0  disc_accuracy : 88.40153813362122  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   710, Loss: 23.546316, Gen_loss:  5.593032, Disc_loss:  0.358263, Gen_acc: 23.18, Disc_acc: 88.40, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=238, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h56m 9s, ETA: 12h13m47s, 
[1,0]<stdout>:DLL 2022-01-11 02:01:30.326852 - Training Iteration: 720  train_perf : 540.4217529296875  total_loss : 23.447370529174805  masked_lm_accuracy : 23.652900755405426  masked_lm_loss : 5.5346360206604  sampled_masked_lm_accuracy : 15.935565531253815  disc_loss : 0.3574478030204773  disc_auc : 0.0  disc_accuracy : 88.44056725502014  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   720, Loss: 23.447371, Gen_loss:  5.534636, Disc_loss:  0.357448, Gen_acc: 23.65, Disc_acc: 88.44, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=248, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h56m51s, ETA: 12h11m46s, 
[1,0]<stdout>:DLL 2022-01-11 02:02:11.996193 - Training Iteration: 730  train_perf : 540.181640625  total_loss : 23.384984970092773  masked_lm_accuracy : 23.9357590675354  masked_lm_loss : 5.490029811859131  sampled_masked_lm_accuracy : 16.07307940721512  disc_loss : 0.3569961190223694  disc_auc : 0.0  disc_accuracy : 88.45932483673096  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   730, Loss: 23.384985, Gen_loss:  5.490030, Disc_loss:  0.356996, Gen_acc: 23.94, Disc_acc: 88.46, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=258, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h57m32s, ETA: 12h 9m47s, 
[1,0]<stdout>:DLL 2022-01-11 02:02:53.676865 - Training Iteration: 740  train_perf : 539.8756103515625  total_loss : 23.420955657958984  masked_lm_accuracy : 23.86501133441925  masked_lm_loss : 5.4947991371154785  sampled_masked_lm_accuracy : 15.893031656742096  disc_loss : 0.3574630916118622  disc_auc : 0.0  disc_accuracy : 88.4289026260376  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   740, Loss: 23.420956, Gen_loss:  5.494799, Disc_loss:  0.357463, Gen_acc: 23.87, Disc_acc: 88.43, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=268, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h58m14s, ETA: 12h 7m50s, 
[1,0]<stdout>:DLL 2022-01-11 02:03:35.341633 - Training Iteration: 750  train_perf : 540.0803833007812  total_loss : 23.478891372680664  masked_lm_accuracy : 23.884686827659607  masked_lm_loss : 5.515166282653809  sampled_masked_lm_accuracy : 15.669940412044525  disc_loss : 0.35853561758995056  disc_auc : 0.0  disc_accuracy : 88.39544653892517  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   750, Loss: 23.478891, Gen_loss:  5.515166, Disc_loss:  0.358536, Gen_acc: 23.88, Disc_acc: 88.40, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=278, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h58m56s, ETA: 12h 5m55s, 
[1,0]<stdout>:DLL 2022-01-11 02:04:16.981873 - Training Iteration: 760  train_perf : 540.5025634765625  total_loss : 23.253400802612305  masked_lm_accuracy : 24.782322347164154  masked_lm_loss : 5.41375732421875  sampled_masked_lm_accuracy : 16.46491140127182  disc_loss : 0.35554632544517517  disc_auc : 0.0  disc_accuracy : 88.51175308227539  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   760, Loss: 23.253401, Gen_loss:  5.413757, Disc_loss:  0.355546, Gen_acc: 24.78, Disc_acc: 88.51, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=288, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  0h59m37s, ETA: 12h 4m 2s, 
[1,0]<stdout>:DLL 2022-01-11 02:04:58.668349 - Training Iteration: 770  train_perf : 540.2174682617188  total_loss : 23.08540916442871  masked_lm_accuracy : 25.248202681541443  masked_lm_loss : 5.340907096862793  sampled_masked_lm_accuracy : 17.124085128307343  disc_loss : 0.3537457585334778  disc_auc : 0.0  disc_accuracy : 88.59725594520569  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   770, Loss: 23.085409, Gen_loss:  5.340907, Disc_loss:  0.353746, Gen_acc: 25.25, Disc_acc: 88.60, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=298, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 0m19s, ETA: 12h 2m11s, 
[1,0]<stdout>:DLL 2022-01-11 02:05:40.327056 - Training Iteration: 780  train_perf : 540.316162109375  total_loss : 22.979509353637695  masked_lm_accuracy : 25.960135459899902  masked_lm_loss : 5.284850120544434  sampled_masked_lm_accuracy : 17.481134831905365  disc_loss : 0.3524014353752136  disc_auc : 0.0  disc_accuracy : 88.64495158195496  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   780, Loss: 22.979509, Gen_loss:  5.284850, Disc_loss:  0.352401, Gen_acc: 25.96, Disc_acc: 88.64, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=308, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 1m 1s, ETA: 12h 0m22s, 
[1,0]<stdout>:DLL 2022-01-11 02:06:21.995573 - Training Iteration: 790  train_perf : 540.189208984375  total_loss : 22.98015594482422  masked_lm_accuracy : 26.075851917266846  masked_lm_loss : 5.273512840270996  sampled_masked_lm_accuracy : 17.486709356307983  disc_loss : 0.352926641702652  disc_auc : 0.0  disc_accuracy : 88.63694667816162  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   790, Loss: 22.980156, Gen_loss:  5.273513, Disc_loss:  0.352927, Gen_acc: 26.08, Disc_acc: 88.64, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=318, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 1m42s, ETA: 11h58m34s, 
[1,0]<stdout>:DLL 2022-01-11 02:07:03.608451 - Training Iteration: 800  train_perf : 540.6581420898438  total_loss : 22.798587799072266  masked_lm_accuracy : 26.814094185829163  masked_lm_loss : 5.191208362579346  sampled_masked_lm_accuracy : 18.0926114320755  disc_loss : 0.3508933186531067  disc_auc : 0.0  disc_accuracy : 88.7309193611145  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   800, Loss: 22.798588, Gen_loss:  5.191208, Disc_loss:  0.350893, Gen_acc: 26.81, Disc_acc: 88.73, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=328, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 2m24s, ETA: 11h56m48s, 
[1,0]<stdout>:DLL 2022-01-11 02:07:45.255428 - Training Iteration: 810  train_perf : 540.6382446289062  total_loss : 22.720741271972656  masked_lm_accuracy : 27.203944325447083  masked_lm_loss : 5.141732692718506  sampled_masked_lm_accuracy : 18.33680123090744  disc_loss : 0.35013115406036377  disc_auc : 0.0  disc_accuracy : 88.75749707221985  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   810, Loss: 22.720741, Gen_loss:  5.141733, Disc_loss:  0.350131, Gen_acc: 27.20, Disc_acc: 88.76, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=338, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 3m 6s, ETA: 11h55m 3s, 
[1,0]<stdout>:DLL 2022-01-11 02:08:26.850964 - Training Iteration: 820  train_perf : 541.1117553710938  total_loss : 22.60325813293457  masked_lm_accuracy : 27.694088220596313  masked_lm_loss : 5.094132900238037  sampled_masked_lm_accuracy : 18.78005713224411  disc_loss : 0.34864717721939087  disc_auc : 0.0  disc_accuracy : 88.82343173027039  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   820, Loss: 22.603258, Gen_loss:  5.094133, Disc_loss:  0.348647, Gen_acc: 27.69, Disc_acc: 88.82, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=348, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 3m47s, ETA: 11h53m20s, 
[1,0]<stdout>:DLL 2022-01-11 02:09:08.514117 - Training Iteration: 830  train_perf : 540.3710327148438  total_loss : 22.645660400390625  masked_lm_accuracy : 27.78840661048889  masked_lm_loss : 5.094155311584473  sampled_masked_lm_accuracy : 18.65818351507187  disc_loss : 0.3495248854160309  disc_auc : 0.0  disc_accuracy : 88.78732919692993  disc_precision : 0.5  disc_recall : 3.838285465462832e-06 
[1,0]<stdout>:Step:   830, Loss: 22.645660, Gen_loss:  5.094155, Disc_loss:  0.349525, Gen_acc: 27.79, Disc_acc: 88.79, Perf: 540, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=358, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 4m29s, ETA: 11h51m38s, 
[1,0]<stdout>:DLL 2022-01-11 02:09:50.138226 - Training Iteration: 840  train_perf : 540.6464233398438  total_loss : 22.462989807128906  masked_lm_accuracy : 28.383907675743103  masked_lm_loss : 5.024936676025391  sampled_masked_lm_accuracy : 19.338691234588623  disc_loss : 0.3471720814704895  disc_auc : 0.0  disc_accuracy : 88.89480233192444  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   840, Loss: 22.462990, Gen_loss:  5.024937, Disc_loss:  0.347172, Gen_acc: 28.38, Disc_acc: 88.89, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=368, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 5m11s, ETA: 11h49m58s, 
[1,0]<stdout>:DLL 2022-01-11 02:10:31.748849 - Training Iteration: 850  train_perf : 540.9203491210938  total_loss : 22.39128303527832  masked_lm_accuracy : 28.95924150943756  masked_lm_loss : 4.961066722869873  sampled_masked_lm_accuracy : 19.45202499628067  disc_loss : 0.34707799553871155  disc_auc : 0.0  disc_accuracy : 88.90451192855835  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   850, Loss: 22.391283, Gen_loss:  4.961067, Disc_loss:  0.347078, Gen_acc: 28.96, Disc_acc: 88.90, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=378, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 5m52s, ETA: 11h48m19s, 
[1,0]<stdout>:DLL 2022-01-11 02:11:13.385908 - Training Iteration: 860  train_perf : 540.579345703125  total_loss : 22.203012466430664  masked_lm_accuracy : 29.713422060012817  masked_lm_loss : 4.886417388916016  sampled_masked_lm_accuracy : 20.26691734790802  disc_loss : 0.34478166699409485  disc_auc : 0.0  disc_accuracy : 89.0116035938263  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   860, Loss: 22.203012, Gen_loss:  4.886417, Disc_loss:  0.344782, Gen_acc: 29.71, Disc_acc: 89.01, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=388, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 6m34s, ETA: 11h46m41s, 
[1,0]<stdout>:DLL 2022-01-11 02:11:54.994201 - Training Iteration: 870  train_perf : 540.8729858398438  total_loss : 22.095720291137695  masked_lm_accuracy : 30.045399069786072  masked_lm_loss : 4.826014995574951  sampled_masked_lm_accuracy : 20.56707888841629  disc_loss : 0.34396475553512573  disc_auc : 0.0  disc_accuracy : 89.05720114707947  disc_precision : 0.0  disc_recall : 0.0 
[1,0]<stdout>:Step:   870, Loss: 22.095720, Gen_loss:  4.826015, Disc_loss:  0.343965, Gen_acc: 30.05, Disc_acc: 89.06, Perf: 541, Loss Scaler: DynamicLossScale(current_loss_scale=128.0, num_good_steps=398, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), Elapsed:  1h 7m15s, ETA: 11h45m 5s, 
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 95
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 153
[clarity1:54218] [[21729,0],0] tcp_peer_recv_blocking: recv() failed for [[1,790],451]: Connection reset by peer (104)
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 129
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 125
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 69
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 97
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 128
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 58
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 99
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 58
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 121
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 50
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 210
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 63
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 169
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 19
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 40
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 66
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 53
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 128
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 58
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 99
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 58
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 121
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 50
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 151
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 247
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 145
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 9
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 59
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 66
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 113
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 128
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 58
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 99
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 58
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 121
[clarity1:54218] tcp_peer_recv_connect_ack: invalid header type: 50
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
[1,0]<stdout>: ** Saved model checkpoint for step 877: results/models/test/checkpoints/ckpt-877
[1,0]<stdout>: ** Saved iterator checkpoint for step 877: results/models/test/checkpoints/iter_ckpt_rank_00/iter_ckpt_rank_00-877
