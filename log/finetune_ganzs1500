==================================== START results/models/base/checkpoints/ckpt-1497 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/base/checkpoints/ckpt-1497 at step 1496
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/base/checkpoints/discriminator/config.json
Model weights saved in results/models/base/checkpoints/discriminator/tf_model.h5: {'electra', 'discriminator_predictions'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=test_results/models/base/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-17 18:26:29.669871: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-17 18:26:30.523468: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-17 18:26:30.539482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.539940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 18:26:30.539971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:26:30.541402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 18:26:30.542074: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 18:26:30.542238: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 18:26:30.543683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 18:26:30.544020: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 18:26:30.544097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:26:30.544141: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.544656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.545063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 18:26:30.550266: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-17 18:26:30.550705: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc818000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-17 18:26:30.550716: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-17 18:26:30.694784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.695285: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc73c000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-17 18:26:30.695295: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-17 18:26:30.695431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.695863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 18:26:30.695898: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:26:30.695923: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 18:26:30.695935: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 18:26:30.695945: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 18:26:30.695955: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 18:26:30.695964: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 18:26:30.695997: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:26:30.696042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.696502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.696924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 18:26:30.696956: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:26:30.858734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-17 18:26:30.858777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-17 18:26:30.858781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-17 18:26:30.858904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.859360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:26:30.859800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-17 18:26:30.522635 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: test_results/models/base/checkpoints/discriminator
loading configuration file test_results/models/base/checkpoints/discriminator/config.json
loading weights file test_results/models/base/checkpoints/discriminator/tf_model.h5
2022-02-17 18:26:30.924909: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-17 18:26:30.963841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at test_results/models/base/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at test_results/models/base/checkpoints/discriminator and are newly initialized: ['end_logits', 'start_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 36%|███▌      | 159/442 [00:05<00:08, 31.79it/s] 66%|██████▌   | 292/442 [00:10<00:04, 30.01it/s] 98%|█████████▊| 431/442 [00:15<00:00, 29.30it/s]100%|██████████| 442/442 [00:15<00:00, 28.63it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 26.26it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-17 18:27:25.863230: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-17 18:27:26.754739: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 18:27:26.781813: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:27:27.392560: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-17 18:27:27.666242: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-17 18:30:28.709053: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:32.059497: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:30:32.059516: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:30:32.059522: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:30:32.059525: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:30:32.059526: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:30:32.059528: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 18:30:32.073799: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-17 18:30:34.055735: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:34.192636: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:30:34.192654: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:30:34.192659: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:30:34.192661: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:30:34.192663: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:30:34.192665: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 18:30:34.255442: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:34.440036: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:30:34.440055: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:30:34.440062: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:30:34.440064: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:30:34.440066: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:30:34.440068: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 18:30:34.670397: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:35.442183: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:30:35.442202: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:30:35.442208: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:30:35.442210: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:30:35.442212: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:30:35.442214: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.27780342, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:18<152:38:43, 198.38s/it]2022-02-17 18:30:38.537381: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 18:30:39.867287: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:43.125396: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:30:43.125414: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:30:43.125421: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:30:43.125423: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:30:43.125425: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:30:43.125426: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 18:30:45.066743: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:45.154887: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:45.442211: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:46.453841: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:30:46.575423: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:30:46.575456: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:30:46.575462: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:30:46.575464: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:30:46.575466: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:30:46.575467: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:29<109:17:21, 142.09s/it]DLL 2022-02-17 18:30:35.882148 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.277803421020508  train_perf : 0.16154609620571136 

Epoch: 000, Step:    50, Loss:  5.06274414, Perf:  470, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:34<74:25:03, 99.48s/it] DLL 2022-02-17 18:30:49.882594 - Training Epoch: 0 Training Iteration: 50  step_loss : 5.062744140625  train_perf : 469.99761962890625 

Epoch: 000, Step:   100, Loss:  4.21479130, Perf:  482, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-17 18:30:53.189673 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.214791297912598  train_perf : 481.56903076171875 

Epoch: 000, Step:   150, Loss:  3.55495214, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 154/2771 [03:39<50:38:11, 69.66s/it]DLL 2022-02-17 18:30:56.501567 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.5549521446228027  train_perf : 485.2020263671875 

Epoch: 000, Step:   200, Loss:  3.81469727, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 230/2771 [03:44<34:25:49, 48.78s/it]DLL 2022-02-17 18:30:59.816926 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.814697265625  train_perf : 486.8938903808594 

Epoch: 000, Step:   250, Loss:  3.29263282, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-17 18:31:04.911525: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:31:05.027182: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:31:05.027215: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:31:05.027221: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:31:05.027223: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:31:05.027225: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:31:05.027227: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 18:31:03.124227 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.292632818222046  train_perf : 488.12646484375 

Epoch: 000, Step:   300, Loss:  3.29212356, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 304/2771 [03:49<23:24:48, 34.17s/it]DLL 2022-02-17 18:31:06.593945 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.292123556137085  train_perf : 487.50384521484375 

Epoch: 000, Step:   350, Loss:  3.02208042, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 380/2771 [03:54<15:53:51, 23.94s/it]DLL 2022-02-17 18:31:09.905208 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.022080421447754  train_perf : 488.2150573730469 

Epoch: 000, Step:   400, Loss:  3.07906795, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-17 18:31:13.222297 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.0790679454803467  train_perf : 488.6304016113281 

Epoch: 000, Step:   450, Loss:  2.69897032, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 456/2771 [03:59<10:47:14, 16.78s/it]DLL 2022-02-17 18:31:16.537722 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.698970317840576  train_perf : 488.97430419921875 

Epoch: 000, Step:   500, Loss:  3.05643678, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 532/2771 [04:04<7:18:56, 11.76s/it] DLL 2022-02-17 18:31:19.863855 - Training Epoch: 0 Training Iteration: 500  step_loss : 3.056436777114868  train_perf : 489.0979919433594 

Epoch: 000, Step:   550, Loss:  3.11415672, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-17 18:31:23.186318 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.114156723022461  train_perf : 489.24530029296875 

Epoch: 000, Step:   600, Loss:  2.92156696, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 608/2771 [04:09<4:57:32,  8.25s/it]DLL 2022-02-17 18:31:26.505584 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.921566963195801  train_perf : 489.40264892578125 

Epoch: 000, Step:   650, Loss:  3.11503053, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 684/2771 [04:14<3:21:39,  5.80s/it]DLL 2022-02-17 18:31:29.833954 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.115030527114868  train_perf : 489.4420471191406 

Epoch: 000, Step:   700, Loss:  3.24515295, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
DLL 2022-02-17 18:31:33.157002 - Training Epoch: 0 Training Iteration: 700  step_loss : 3.2451529502868652  train_perf : 489.5030212402344 

Epoch: 000, Step:   750, Loss:  2.78582978, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:751
Iteration:  27%|██▋       | 760/2771 [04:19<2:16:41,  4.08s/it]DLL 2022-02-17 18:31:36.488172 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.785829782485962  train_perf : 489.4817199707031 

Epoch: 000, Step:   800, Loss:  3.04106760, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:801
Iteration:  30%|███       | 836/2771 [04:24<1:32:42,  2.87s/it]DLL 2022-02-17 18:31:39.816357 - Training Epoch: 0 Training Iteration: 800  step_loss : 3.041067600250244  train_perf : 489.51287841796875 

Epoch: 000, Step:   850, Loss:  3.59274626, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:851
DLL 2022-02-17 18:31:43.145309 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.5927462577819824  train_perf : 489.5345764160156 

Epoch: 000, Step:   900, Loss:  2.43402481, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:901
Iteration:  33%|███▎      | 912/2771 [04:29<1:02:58,  2.03s/it]DLL 2022-02-17 18:31:46.478151 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.4340248107910156  train_perf : 489.5091857910156 

Epoch: 000, Step:   950, Loss:  3.33446980, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:951
Iteration:  36%|███▌      | 988/2771 [04:34<42:52,  1.44s/it]  DLL 2022-02-17 18:31:49.815731 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.334469795227051  train_perf : 489.4562072753906 

Epoch: 000, Step:  1000, Loss:  3.13849163, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1001, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1001
DLL 2022-02-17 18:31:53.143753 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.138491630554199  train_perf : 489.46990966796875 

Epoch: 000, Step:  1050, Loss:  3.24827504, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1051, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1051
Iteration:  38%|███▊      | 1063/2771 [04:39<29:18,  1.03s/it]DLL 2022-02-17 18:31:56.475253 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.2482750415802  train_perf : 489.4696960449219 

Epoch: 000, Step:  1100, Loss:  3.58100653, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1101
Iteration:  41%|████      | 1139/2771 [04:44<20:09,  1.35it/s]DLL 2022-02-17 18:31:59.809749 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.5810065269470215  train_perf : 489.4407958984375 

Epoch: 000, Step:  1150, Loss:  3.59225631, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1151
DLL 2022-02-17 18:32:03.141259 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.5922563076019287  train_perf : 489.4166564941406 

Epoch: 000, Step:  1200, Loss:  3.11425233, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1201
Iteration:  44%|████▍     | 1214/2771 [04:49<13:58,  1.86it/s]DLL 2022-02-17 18:32:06.474287 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.1142523288726807  train_perf : 489.3963928222656 

Epoch: 000, Step:  1250, Loss:  3.52659774, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1251
Iteration:  47%|████▋     | 1289/2771 [04:54<09:48,  2.52it/s]DLL 2022-02-17 18:32:09.811336 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.526597738265991  train_perf : 489.36456298828125 

Epoch: 000, Step:  1300, Loss:  3.11327648, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1301
DLL 2022-02-17 18:32:13.147237 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.113276481628418  train_perf : 489.3409423828125 

Epoch: 000, Step:  1350, Loss:  3.03119802, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1351
Iteration:  49%|████▉     | 1365/2771 [04:59<06:58,  3.36it/s]DLL 2022-02-17 18:32:16.481181 - Training Epoch: 0 Training Iteration: 1350  step_loss : 3.031198024749756  train_perf : 489.32421875 

Epoch: 000, Step:  1400, Loss:  3.18542528, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1401
Iteration:  52%|█████▏    | 1440/2771 [05:04<05:04,  4.37it/s]DLL 2022-02-17 18:32:19.815023 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.185425281524658  train_perf : 489.3190612792969 

Epoch: 000, Step:  1450, Loss:  2.82744932, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1451
DLL 2022-02-17 18:32:23.160903 - Training Epoch: 0 Training Iteration: 1450  step_loss : 2.827449321746826  train_perf : 489.2441711425781 

Epoch: 000, Step:  1500, Loss:  3.58127880, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1501
Iteration:  55%|█████▍    | 1515/2771 [05:09<03:46,  5.55it/s]DLL 2022-02-17 18:32:26.508520 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.5812788009643555  train_perf : 489.16009521484375 

Epoch: 000, Step:  1550, Loss:  3.33618879, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1551
Iteration:  57%|█████▋    | 1590/2771 [05:14<02:52,  6.84it/s]DLL 2022-02-17 18:32:29.847242 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.336188793182373  train_perf : 489.1226501464844 

Epoch: 000, Step:  1600, Loss:  3.08978772, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1601
DLL 2022-02-17 18:32:33.195510 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.089787721633911  train_perf : 489.0560607910156 

Epoch: 000, Step:  1650, Loss:  3.31862354, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1651
Iteration:  60%|██████    | 1665/2771 [05:19<02:15,  8.17it/s]DLL 2022-02-17 18:32:36.544617 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.3186235427856445  train_perf : 488.9858703613281 

Epoch: 000, Step:  1700, Loss:  3.03306246, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1701
Iteration:  63%|██████▎   | 1740/2771 [05:24<01:49,  9.46it/s]DLL 2022-02-17 18:32:39.885713 - Training Epoch: 0 Training Iteration: 1700  step_loss : 3.03306245803833  train_perf : 488.9456481933594 

Epoch: 000, Step:  1750, Loss:  3.12455678, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1751
DLL 2022-02-17 18:32:43.232012 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.12455677986145  train_perf : 488.8931579589844 

Epoch: 000, Step:  1800, Loss:  3.04999566, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1801
Iteration:  65%|██████▌   | 1815/2771 [05:30<01:29, 10.63it/s]DLL 2022-02-17 18:32:46.579920 - Training Epoch: 0 Training Iteration: 1800  step_loss : 3.0499956607818604  train_perf : 488.83526611328125 

Epoch: 000, Step:  1850, Loss:  3.25270438, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1851
Iteration:  68%|██████▊   | 1890/2771 [05:35<01:15, 11.64it/s]DLL 2022-02-17 18:32:49.926190 - Training Epoch: 0 Training Iteration: 1850  step_loss : 3.252704381942749  train_perf : 488.78985595703125 

Epoch: 000, Step:  1900, Loss:  2.72983456, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1901
DLL 2022-02-17 18:32:53.272466 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.72983455657959  train_perf : 488.7483215332031 

Epoch: 000, Step:  1950, Loss:  2.59722996, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1951
Iteration:  71%|███████   | 1965/2771 [05:40<01:04, 12.46it/s]2022-02-17 18:32:59.900468: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:33:00.018144: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:33:00.018177: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:33:00.018183: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:33:00.018203: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:33:00.018204: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:33:00.018207: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 18:32:56.619484 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.5972299575805664  train_perf : 488.70526123046875 

Epoch: 000, Step:  2000, Loss:  3.02520633, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2001
Iteration:  74%|███████▎  | 2040/2771 [05:45<00:56, 13.00it/s]2022-02-17 18:33:02.869979: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:33:02.984122: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:33:02.984142: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:33:02.984148: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:33:02.984150: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:33:02.984152: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:33:02.984154: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 18:33:00.120299 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.0252063274383545  train_perf : 488.48968505859375 

Epoch: 000, Step:  2050, Loss:  2.93630791, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=9, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2050
DLL 2022-02-17 18:33:03.622488 - Training Epoch: 0 Training Iteration: 2050  step_loss : 2.936307907104492  train_perf : 488.2547607421875 

Epoch: 000, Step:  2100, Loss:  3.30637050, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=59, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2100
Iteration:  76%|███████▋  | 2115/2771 [05:50<00:48, 13.42it/s]DLL 2022-02-17 18:33:06.974976 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.306370496749878  train_perf : 488.2076721191406 

Epoch: 000, Step:  2150, Loss:  3.06678915, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=109, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2150
Iteration:  79%|███████▉  | 2190/2771 [05:55<00:41, 13.84it/s]DLL 2022-02-17 18:33:10.321954 - Training Epoch: 0 Training Iteration: 2150  step_loss : 3.066789150238037  train_perf : 488.1780700683594 

Epoch: 000, Step:  2200, Loss:  3.06399870, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=159, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-17 18:33:13.668625 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.0639986991882324  train_perf : 488.1502685546875 

Epoch: 000, Step:  2250, Loss:  2.84801102, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=209, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  82%|████████▏ | 2265/2771 [06:00<00:35, 14.16it/s]DLL 2022-02-17 18:33:17.012119 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.848011016845703  train_perf : 488.1302795410156 

Epoch: 000, Step:  2300, Loss:  2.93189573, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=259, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  84%|████████▍ | 2340/2771 [06:05<00:29, 14.39it/s]DLL 2022-02-17 18:33:20.356706 - Training Epoch: 0 Training Iteration: 2300  step_loss : 2.9318957328796387  train_perf : 488.1090393066406 

Epoch: 000, Step:  2350, Loss:  2.75976896, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=309, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-17 18:33:23.705269 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.7597689628601074  train_perf : 488.07965087890625 

Epoch: 000, Step:  2400, Loss:  2.79626274, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=359, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2415/2771 [06:10<00:24, 14.54it/s]DLL 2022-02-17 18:33:27.051966 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.796262741088867  train_perf : 488.0553283691406 

Epoch: 000, Step:  2450, Loss:  3.25941443, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=409, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2490/2771 [06:15<00:19, 14.66it/s]DLL 2022-02-17 18:33:30.399590 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.2594144344329834  train_perf : 488.03033447265625 

Epoch: 000, Step:  2500, Loss:  3.24035883, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=459, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-17 18:33:33.746616 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.240358829498291  train_perf : 488.010009765625 

Epoch: 000, Step:  2550, Loss:  3.01288319, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=509, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  93%|█████████▎| 2565/2771 [06:20<00:13, 14.74it/s]DLL 2022-02-17 18:33:37.092985 - Training Epoch: 0 Training Iteration: 2550  step_loss : 3.012883186340332  train_perf : 487.9877624511719 

Epoch: 000, Step:  2600, Loss:  2.96707153, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=559, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2640/2771 [06:25<00:08, 14.80it/s]DLL 2022-02-17 18:33:40.440866 - Training Epoch: 0 Training Iteration: 2600  step_loss : 2.967071533203125  train_perf : 487.96795654296875 

Epoch: 000, Step:  2650, Loss:  3.03706312, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=609, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-17 18:33:43.789252 - Training Epoch: 0 Training Iteration: 2650  step_loss : 3.0370631217956543  train_perf : 487.9416198730469 

Epoch: 000, Step:  2700, Loss:  3.00697923, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=659, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
Iteration:  98%|█████████▊| 2715/2771 [06:30<00:03, 14.84it/s]DLL 2022-02-17 18:33:47.139428 - Training Epoch: 0 Training Iteration: 2700  step_loss : 3.00697922706604  train_perf : 487.91619873046875 

Epoch: 000, Step:  2750, Loss:  3.11273050, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=709, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:34<00:00,  7.03it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-17 18:33:50.481198 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.1127305030822754  train_perf : 487.9083557128906 
DLL 2022-02-17 18:33:51.750549 -  e2e_train_time : 394.2513978481293  training_sequences_per_second : 487.9090270996094  final_loss : 3.188775062561035 

Epoch: 001, Step:     0, Loss:  2.67648363, Perf:  457, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=729, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-17 18:33:56.756867 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.676483631134033  train_perf : 457.07012939453125 

Epoch: 001, Step:    50, Loss:  2.54796982, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=779, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
Iteration:   3%|▎         | 71/2771 [00:05<03:11, 14.07it/s]DLL 2022-02-17 18:34:00.102670 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.5479698181152344  train_perf : 486.4976501464844 

Epoch: 001, Step:   100, Loss:  2.52520704, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=829, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 146/2771 [00:10<03:03, 14.32it/s]DLL 2022-02-17 18:34:03.444805 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.525207042694092  train_perf : 486.87860107421875 

Epoch: 001, Step:   150, Loss:  2.36962295, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=879, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-17 18:34:06.788339 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.3696229457855225  train_perf : 487.040771484375 

Epoch: 001, Step:   200, Loss:  2.28387976, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=929, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 221/2771 [00:15<02:55, 14.50it/s]DLL 2022-02-17 18:34:10.133791 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.2838797569274902  train_perf : 486.9896545410156 

Epoch: 001, Step:   250, Loss:  2.53743958, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=979, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 296/2771 [00:20<02:49, 14.64it/s]DLL 2022-02-17 18:34:13.477018 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.5374395847320557  train_perf : 487.1181945800781 

Epoch: 001, Step:   300, Loss:  2.49023628, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1029, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-17 18:34:16.820433 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.490236282348633  train_perf : 487.13861083984375 

Epoch: 001, Step:   350, Loss:  2.32045221, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1079, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  13%|█▎        | 371/2771 [00:25<02:42, 14.73it/s]DLL 2022-02-17 18:34:20.165046 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.3204522132873535  train_perf : 487.1186218261719 

Epoch: 001, Step:   400, Loss:  2.04827070, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1129, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 446/2771 [00:30<02:37, 14.80it/s]DLL 2022-02-17 18:34:23.511842 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.0482707023620605  train_perf : 487.07672119140625 

Epoch: 001, Step:   450, Loss:  2.12694550, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1179, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-17 18:34:26.852658 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.1269454956054688  train_perf : 487.13629150390625 

Epoch: 001, Step:   500, Loss:  2.47586393, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1229, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▉        | 521/2771 [00:35<02:31, 14.84it/s]DLL 2022-02-17 18:34:30.197658 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.4758639335632324  train_perf : 487.1300048828125 

Epoch: 001, Step:   550, Loss:  2.14429712, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1279, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
Iteration:  22%|██▏       | 596/2771 [00:40<02:26, 14.87it/s]DLL 2022-02-17 18:34:33.541737 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.1442971229553223  train_perf : 487.1357116699219 

Epoch: 001, Step:   600, Loss:  2.09038353, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1329, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
DLL 2022-02-17 18:34:36.887591 - Training Epoch: 1 Training Iteration: 600  step_loss : 2.090383529663086  train_perf : 487.1208801269531 

Epoch: 001, Step:   650, Loss:  2.68438196, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1379, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 671/2771 [00:45<02:21, 14.89it/s]DLL 2022-02-17 18:34:40.235950 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.6843819618225098  train_perf : 487.08392333984375 

Epoch: 001, Step:   700, Loss:  2.32040596, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1429, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
Iteration:  27%|██▋       | 746/2771 [00:50<02:15, 14.91it/s]DLL 2022-02-17 18:34:43.581435 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.320405960083008  train_perf : 487.0804748535156 

Epoch: 001, Step:   750, Loss:  2.10703993, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1479, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3520
DLL 2022-02-17 18:34:46.922953 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.1070399284362793  train_perf : 487.0982971191406 

Epoch: 001, Step:   800, Loss:  2.79639745, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1529, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3570
Iteration:  30%|██▉       | 821/2771 [00:55<02:10, 14.92it/s]DLL 2022-02-17 18:34:50.267678 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.7963974475860596  train_perf : 487.1042785644531 

Epoch: 001, Step:   850, Loss:  2.88697672, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1579, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3620
Iteration:  32%|███▏      | 896/2771 [01:00<02:05, 14.93it/s]DLL 2022-02-17 18:34:53.614946 - Training Epoch: 1 Training Iteration: 850  step_loss : 2.886976718902588  train_perf : 487.0802307128906 

Epoch: 001, Step:   900, Loss:  2.45092654, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1629, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3670
DLL 2022-02-17 18:34:56.962707 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.4509265422821045  train_perf : 487.05035400390625 

Epoch: 001, Step:   950, Loss:  2.66703629, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1679, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3720
Iteration:  35%|███▌      | 971/2771 [01:05<02:00, 14.93it/s]DLL 2022-02-17 18:35:00.312400 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.667036294937134  train_perf : 487.00897216796875 

Epoch: 001, Step:  1000, Loss:  2.73173690, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1729, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3770
Iteration:  38%|███▊      | 1046/2771 [01:10<01:55, 14.93it/s]DLL 2022-02-17 18:35:03.660612 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.731736898422241  train_perf : 486.9812316894531 

Epoch: 001, Step:  1050, Loss:  2.86924601, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1779, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3820
DLL 2022-02-17 18:35:07.011099 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.869246006011963  train_perf : 486.949951171875 

Epoch: 001, Step:  1100, Loss:  3.30592942, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1829, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3870
Iteration:  40%|████      | 1121/2771 [01:15<01:50, 14.93it/s]DLL 2022-02-17 18:35:10.353992 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.30592942237854  train_perf : 486.9640808105469 

Epoch: 001, Step:  1150, Loss:  3.07945871, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1879, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3920
Iteration:  43%|████▎     | 1196/2771 [01:20<01:45, 14.94it/s]DLL 2022-02-17 18:35:13.706060 - Training Epoch: 1 Training Iteration: 1150  step_loss : 3.079458713531494  train_perf : 486.9270324707031 

Epoch: 001, Step:  1200, Loss:  2.63975716, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1929, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3970
DLL 2022-02-17 18:35:17.048503 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.6397571563720703  train_perf : 486.9407653808594 

Epoch: 001, Step:  1250, Loss:  3.10970259, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1979, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4020
Iteration:  46%|████▌     | 1271/2771 [01:25<01:40, 14.93it/s]DLL 2022-02-17 18:35:20.404093 - Training Epoch: 1 Training Iteration: 1250  step_loss : 3.1097025871276855  train_perf : 486.89434814453125 

Epoch: 001, Step:  1300, Loss:  2.52277946, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=5, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4069
Iteration:  49%|████▊     | 1346/2771 [01:30<01:35, 14.94it/s]DLL 2022-02-17 18:35:23.743118 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.5227794647216797  train_perf : 486.9268798828125 

Epoch: 001, Step:  1350, Loss:  2.52517080, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=55, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4119
DLL 2022-02-17 18:35:27.088086 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.5251708030700684  train_perf : 486.9244689941406 

Epoch: 001, Step:  1400, Loss:  2.62052727, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=105, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4169
Iteration:  51%|█████▏    | 1421/2771 [01:35<01:30, 14.94it/s]DLL 2022-02-17 18:35:30.432550 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.6205272674560547  train_perf : 486.9306335449219 

Epoch: 001, Step:  1450, Loss:  2.20857096, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=155, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4219
Iteration:  54%|█████▍    | 1496/2771 [01:40<01:25, 14.94it/s]DLL 2022-02-17 18:35:33.780166 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.208570957183838  train_perf : 486.92169189453125 

Epoch: 001, Step:  1500, Loss:  3.41562200, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=205, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4269
DLL 2022-02-17 18:35:37.127107 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.4156219959259033  train_perf : 486.917724609375 

Epoch: 001, Step:  1550, Loss:  2.59577823, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=255, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4319
Iteration:  57%|█████▋    | 1571/2771 [01:45<01:20, 14.94it/s]DLL 2022-02-17 18:35:40.477362 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.595778226852417  train_perf : 486.8966369628906 

Epoch: 001, Step:  1600, Loss:  2.57792401, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=305, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4369
Iteration:  59%|█████▉    | 1646/2771 [01:50<01:15, 14.94it/s]DLL 2022-02-17 18:35:43.825327 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.5779240131378174  train_perf : 486.8777770996094 

Epoch: 001, Step:  1650, Loss:  2.60888839, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=355, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4419
DLL 2022-02-17 18:35:47.174176 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.6088883876800537  train_perf : 486.8605041503906 

Epoch: 001, Step:  1700, Loss:  2.66637301, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=405, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4469
Iteration:  62%|██████▏   | 1721/2771 [01:55<01:10, 14.93it/s]DLL 2022-02-17 18:35:50.525325 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.6663730144500732  train_perf : 486.8424072265625 

Epoch: 001, Step:  1750, Loss:  2.98955822, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=455, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4519
Iteration:  65%|██████▍   | 1796/2771 [02:00<01:05, 14.94it/s]DLL 2022-02-17 18:35:53.872317 - Training Epoch: 1 Training Iteration: 1750  step_loss : 2.989558219909668  train_perf : 486.82989501953125 

Epoch: 001, Step:  1800, Loss:  2.50095415, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=505, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4569
DLL 2022-02-17 18:35:57.213676 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.5009541511535645  train_perf : 486.84564208984375 

Epoch: 001, Step:  1850, Loss:  2.73627067, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=555, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4619
Iteration:  68%|██████▊   | 1871/2771 [02:05<01:00, 14.94it/s]DLL 2022-02-17 18:36:00.563278 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.7362706661224365  train_perf : 486.82818603515625 

Epoch: 001, Step:  1900, Loss:  2.53891015, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=605, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4669
Iteration:  70%|███████   | 1946/2771 [02:10<00:55, 14.94it/s]DLL 2022-02-17 18:36:03.912219 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.538910150527954  train_perf : 486.821533203125 

Epoch: 001, Step:  1950, Loss:  2.41534877, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=655, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4719
DLL 2022-02-17 18:36:07.258791 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.415348768234253  train_perf : 486.822021484375 

Epoch: 001, Step:  2000, Loss:  2.57992172, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=705, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4769
Iteration:  73%|███████▎  | 2021/2771 [02:15<00:50, 14.93it/s]DLL 2022-02-17 18:36:10.606801 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.5799217224121094  train_perf : 486.81494140625 

Epoch: 001, Step:  2050, Loss:  2.38413596, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=755, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
Iteration:  76%|███████▌  | 2096/2771 [02:20<00:45, 14.93it/s]DLL 2022-02-17 18:36:13.956007 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.3841359615325928  train_perf : 486.8069152832031 

Epoch: 001, Step:  2100, Loss:  2.89060974, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=805, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-17 18:36:17.308164 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.8906097412109375  train_perf : 486.7829284667969 

Epoch: 001, Step:  2150, Loss:  2.69590282, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=855, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2171/2771 [02:25<00:40, 14.93it/s]DLL 2022-02-17 18:36:20.660180 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.6959028244018555  train_perf : 486.763916015625 

Epoch: 001, Step:  2200, Loss:  2.58461404, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=905, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2246/2771 [02:30<00:35, 14.93it/s]DLL 2022-02-17 18:36:24.009453 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.5846140384674072  train_perf : 486.75634765625 

Epoch: 001, Step:  2250, Loss:  2.53443742, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=955, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-17 18:36:27.353667 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.534437417984009  train_perf : 486.7596740722656 

Epoch: 001, Step:  2300, Loss:  2.43249989, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1005, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▍ | 2321/2771 [02:35<00:30, 14.93it/s]DLL 2022-02-17 18:36:30.701133 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.432499885559082  train_perf : 486.7525634765625 

Epoch: 001, Step:  2350, Loss:  2.32154942, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1055, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  86%|████████▋ | 2396/2771 [02:40<00:25, 14.94it/s]DLL 2022-02-17 18:36:34.050269 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.321549415588379  train_perf : 486.7445373535156 

Epoch: 001, Step:  2400, Loss:  2.42237401, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1105, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-17 18:36:37.392856 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.4223740100860596  train_perf : 486.7562561035156 

Epoch: 001, Step:  2450, Loss:  2.95367169, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1155, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2471/2771 [02:45<00:20, 14.94it/s]DLL 2022-02-17 18:36:40.739602 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.95367169380188  train_perf : 486.7538757324219 

Epoch: 001, Step:  2500, Loss:  2.80779314, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1205, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2546/2771 [02:50<00:15, 14.94it/s]DLL 2022-02-17 18:36:44.082344 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.807793140411377  train_perf : 486.7609558105469 

Epoch: 001, Step:  2550, Loss:  2.64263678, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1255, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-17 18:36:47.427653 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.642636775970459  train_perf : 486.7591247558594 

Epoch: 001, Step:  2600, Loss:  2.91827345, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1305, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▍| 2621/2771 [02:55<00:10, 14.94it/s]DLL 2022-02-17 18:36:50.776297 - Training Epoch: 1 Training Iteration: 2600  step_loss : 2.918273448944092  train_perf : 486.752197265625 

Epoch: 001, Step:  2650, Loss:  2.77285767, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1355, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2696/2771 [03:00<00:05, 14.94it/s]DLL 2022-02-17 18:36:54.124334 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.772857666015625  train_perf : 486.747314453125 

Epoch: 001, Step:  2700, Loss:  2.84455609, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1405, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-17 18:36:57.475623 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.8445560932159424  train_perf : 486.7357482910156 

Epoch: 001, Step:  2750, Loss:  2.65517521, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1455, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.92it/s]DLL 2022-02-17 18:37:00.821799 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.65517520904541  train_perf : 486.73822021484375 
DLL 2022-02-17 18:37:02.095440 -  e2e_train_time : 579.9546296596527  training_sequences_per_second : 486.7353210449219  final_loss : 2.6496479511260986 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-17 18:37:06.672213: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 18:38:30.781319: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:38:31.819735: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:38:31.819754: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:38:31.819761: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:38:31.819763: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:38:31.819764: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:38:31.819766: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:29<31:17, 89.41s/it]Iteration:  14%|█▎        | 3/22 [01:35<20:05, 63.45s/it]Iteration:  23%|██▎       | 5/22 [01:41<12:50, 45.31s/it]Iteration:  32%|███▏      | 7/22 [01:47<08:09, 32.63s/it]Iteration:  41%|████      | 9/22 [01:53<05:08, 23.76s/it]Iteration:  50%|█████     | 11/22 [01:59<03:13, 17.56s/it]Iteration:  59%|█████▉    | 13/22 [02:05<01:59, 13.23s/it]Iteration:  68%|██████▊   | 15/22 [02:12<01:11, 10.23s/it]Iteration:  77%|███████▋  | 17/22 [02:18<00:40,  8.14s/it]Iteration:  86%|████████▋ | 19/22 [02:25<00:20,  6.67s/it]Iteration:  95%|█████████▌| 21/22 [02:31<00:05,  5.66s/it]2022-02-17 18:39:38.171963: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.66s/it]2022-02-17 18:40:47.413552: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:40:48.465909: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:40:48.465928: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:40:48.465934: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:40:48.465936: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:40:48.465938: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:40:48.465940: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:43<00:00, 25.54s/it]Iteration: 100%|██████████| 22/22 [03:43<00:00, 10.17s/it]{"exact_match": 12.109744560075686, "f1": 16.20422739061888}

Epoch: 001 Results: {"exact_match": 12.109744560075686, "f1": 16.20422739061888}

**EVAL SUMMARY** - Epoch: 001,  EM: 12.110, F1: 16.204, Infer_Perf: 1051 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 443.074 ms, 90%: 442.709 ms, 95%: 442.900 ms, 99%: 442.900 ms
DLL 2022-02-17 18:41:05.962997 -  inference_sequences_per_second : 1051.12548828125  e2e_inference_time : 238.04838943481445 
**RESULTS SUMMARY** - EM: 12.110, F1: 16.204, Train_Time:  580 s, Train_Perf:  487 seq/s, Infer_Perf: 1051 seq/s

DLL 2022-02-17 18:41:05.963754 -  exact_match : 12.109744560075686  F1 : 16.20422739061888 
====================================  END results/models/base/checkpoints/ckpt-1497  ====================================
==================================== START results/models/base/checkpoints/ckpt-1997 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/base/checkpoints/ckpt-1997 at step 1996
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/base/checkpoints/discriminator/config.json
Model weights saved in results/models/base/checkpoints/discriminator/tf_model.h5: {'electra', 'discriminator_predictions'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=test_results/models/base/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-17 18:41:11.995339: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-17 18:41:12.832406: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-17 18:41:12.848576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:12.849026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 18:41:12.849041: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:41:12.850501: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 18:41:12.851072: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 18:41:12.851204: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 18:41:12.852630: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 18:41:12.852963: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 18:41:12.853042: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:41:12.853086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:12.853565: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:12.853977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 18:41:12.859021: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-17 18:41:12.859445: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0a04000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-17 18:41:12.859457: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-17 18:41:13.002683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:13.003190: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0904000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-17 18:41:13.003203: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-17 18:41:13.003316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:13.003734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 18:41:13.003752: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:41:13.003772: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 18:41:13.003784: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 18:41:13.003792: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 18:41:13.003801: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 18:41:13.003808: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 18:41:13.003816: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:41:13.003849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:13.004273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:13.004673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 18:41:13.004689: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:41:13.169127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-17 18:41:13.169154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-17 18:41:13.169158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-17 18:41:13.169272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:13.169732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:41:13.170150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-17 18:41:12.831640 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: test_results/models/base/checkpoints/discriminator
loading configuration file test_results/models/base/checkpoints/discriminator/config.json
loading weights file test_results/models/base/checkpoints/discriminator/tf_model.h5
2022-02-17 18:41:13.237739: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-17 18:41:13.277372: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at test_results/models/base/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at test_results/models/base/checkpoints/discriminator and are newly initialized: ['start_logits', 'end_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 36%|███▋      | 161/442 [00:05<00:08, 32.20it/s] 67%|██████▋   | 297/442 [00:10<00:04, 30.42it/s] 98%|█████████▊| 435/442 [00:15<00:00, 29.51it/s]100%|██████████| 442/442 [00:15<00:00, 28.92it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 26.56it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-17 18:42:08.259895: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-17 18:42:09.143615: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 18:42:09.170487: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:42:09.794806: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-17 18:42:10.077657: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-17 18:45:17.044278: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:20.356730: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:45:20.356751: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:45:20.356757: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:45:20.356759: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:45:20.356761: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:45:20.356763: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 18:45:20.370244: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-17 18:45:22.349363: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:22.492569: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:45:22.492588: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:45:22.492594: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:45:22.492596: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:45:22.492598: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:45:22.492600: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 18:45:22.552974: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:22.734478: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:45:22.734496: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:45:22.734502: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:45:22.734504: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:45:22.734506: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:45:22.734508: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 18:45:22.954401: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:23.717573: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:45:23.717591: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:45:23.717596: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:45:23.717598: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:45:23.717600: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:45:23.717602: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.27780342, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:24<157:01:40, 204.08s/it]2022-02-17 18:45:26.788625: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 18:45:28.123246: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:31.401399: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:45:31.401418: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:45:31.401425: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:45:31.401427: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:45:31.401428: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:45:31.401430: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 18:45:33.344343: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:33.433475: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:33.727560: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:34.744392: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:34.869878: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:45:34.869926: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:45:34.869933: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:45:34.869953: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:45:34.869954: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:45:34.869956: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:34<112:21:39, 146.08s/it]DLL 2022-02-17 18:45:24.155100 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.277803421020508  train_perf : 0.15702871978282928 

Epoch: 000, Step:    50, Loss:  5.06271648, Perf:  472, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:39<76:30:31, 102.28s/it]DLL 2022-02-17 18:45:38.165247 - Training Epoch: 0 Training Iteration: 50  step_loss : 5.062716484069824  train_perf : 471.7156066894531 

Epoch: 000, Step:   100, Loss:  4.21517467, Perf:  482, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-17 18:45:41.473451 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.215174674987793  train_perf : 482.317138671875 

Epoch: 000, Step:   150, Loss:  3.56988740, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 154/2771 [03:44<52:03:33, 71.61s/it]DLL 2022-02-17 18:45:44.790657 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.569887399673462  train_perf : 485.4457702636719 

Epoch: 000, Step:   200, Loss:  3.65981221, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 230/2771 [03:49<35:23:49, 50.15s/it]DLL 2022-02-17 18:45:48.098626 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.6598122119903564  train_perf : 487.2847900390625 

Epoch: 000, Step:   250, Loss:  3.24752283, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-17 18:45:53.199401: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:45:53.313736: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:45:53.313754: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:45:53.313761: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:45:53.313764: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:45:53.313765: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:45:53.313767: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 18:45:51.415156 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.2475228309631348  train_perf : 488.2001647949219 

Epoch: 000, Step:   300, Loss:  3.02325630, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 304/2771 [03:55<24:04:13, 35.13s/it]DLL 2022-02-17 18:45:54.882068 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.023256301879883  train_perf : 487.52783203125 

Epoch: 000, Step:   350, Loss:  2.98560810, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 380/2771 [04:00<16:20:36, 24.61s/it]DLL 2022-02-17 18:45:58.192804 - Training Epoch: 0 Training Iteration: 350  step_loss : 2.9856081008911133  train_perf : 488.2762145996094 

Epoch: 000, Step:   400, Loss:  3.35188365, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-17 18:46:01.531479 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.35188364982605  train_perf : 488.3568420410156 

Epoch: 000, Step:   450, Loss:  2.84963250, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 455/2771 [04:05<11:05:40, 17.25s/it]DLL 2022-02-17 18:46:04.859537 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.849632501602173  train_perf : 488.54693603515625 

Epoch: 000, Step:   500, Loss:  3.02195716, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 531/2771 [04:10<7:31:25, 12.09s/it] DLL 2022-02-17 18:46:08.188031 - Training Epoch: 0 Training Iteration: 500  step_loss : 3.0219571590423584  train_perf : 488.7045593261719 

Epoch: 000, Step:   550, Loss:  3.22617197, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-17 18:46:11.519494 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.2261719703674316  train_perf : 488.77239990234375 

Epoch: 000, Step:   600, Loss:  2.81953001, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 607/2771 [04:15<5:05:59,  8.48s/it]DLL 2022-02-17 18:46:14.850038 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.8195300102233887  train_perf : 488.8441162109375 

Epoch: 000, Step:   650, Loss:  3.18718052, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 683/2771 [04:20<3:27:22,  5.96s/it]DLL 2022-02-17 18:46:18.179390 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.187180519104004  train_perf : 488.9083251953125 

Epoch: 000, Step:   700, Loss:  3.08231640, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
DLL 2022-02-17 18:46:21.507154 - Training Epoch: 0 Training Iteration: 700  step_loss : 3.0823163986206055  train_perf : 488.9865417480469 

Epoch: 000, Step:   750, Loss:  2.78761506, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:751
Iteration:  27%|██▋       | 758/2771 [04:25<2:20:36,  4.19s/it]DLL 2022-02-17 18:46:24.840829 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.7876150608062744  train_perf : 489.0057373046875 

Epoch: 000, Step:   800, Loss:  3.08954334, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:801
Iteration:  30%|███       | 833/2771 [04:30<1:35:24,  2.95s/it]DLL 2022-02-17 18:46:28.177125 - Training Epoch: 0 Training Iteration: 800  step_loss : 3.089543342590332  train_perf : 489.0155944824219 

Epoch: 000, Step:   850, Loss:  3.49365091, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:851
DLL 2022-02-17 18:46:31.514755 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.4936509132385254  train_perf : 488.99554443359375 

Epoch: 000, Step:   900, Loss:  2.56239605, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:901
Iteration:  33%|███▎      | 908/2771 [04:35<1:04:49,  2.09s/it]DLL 2022-02-17 18:46:34.849839 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.5623960494995117  train_perf : 488.9861755371094 

Epoch: 000, Step:   950, Loss:  3.27539587, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:951
Iteration:  35%|███▌      | 983/2771 [04:40<44:08,  1.48s/it]  DLL 2022-02-17 18:46:38.184950 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.2753958702087402  train_perf : 488.9886169433594 

Epoch: 000, Step:  1000, Loss:  3.12691975, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1001, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1001
DLL 2022-02-17 18:46:41.521285 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.126919746398926  train_perf : 488.9705505371094 

Epoch: 000, Step:  1050, Loss:  3.21691895, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1051, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1051
Iteration:  38%|███▊      | 1058/2771 [04:45<30:10,  1.06s/it]DLL 2022-02-17 18:46:44.867208 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.2169189453125  train_perf : 488.90216064453125 

Epoch: 000, Step:  1100, Loss:  3.54488230, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1101
Iteration:  41%|████      | 1133/2771 [04:50<20:44,  1.32it/s]DLL 2022-02-17 18:46:48.214675 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.544882297515869  train_perf : 488.8179016113281 

Epoch: 000, Step:  1150, Loss:  3.76049829, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1151
DLL 2022-02-17 18:46:51.563831 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.760498285293579  train_perf : 488.724853515625 

Epoch: 000, Step:  1200, Loss:  3.06601477, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1201
Iteration:  44%|████▎     | 1208/2771 [04:55<14:22,  1.81it/s]DLL 2022-02-17 18:46:54.913582 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.0660147666931152  train_perf : 488.6351013183594 

Epoch: 000, Step:  1250, Loss:  3.49968338, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1251
Iteration:  46%|████▋     | 1283/2771 [05:00<10:05,  2.46it/s]DLL 2022-02-17 18:46:58.267779 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.499683380126953  train_perf : 488.5294189453125 

Epoch: 000, Step:  1300, Loss:  3.23738265, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1301
DLL 2022-02-17 18:47:01.614350 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.237382650375366  train_perf : 488.4706115722656 

Epoch: 000, Step:  1350, Loss:  2.98752260, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1351
Iteration:  49%|████▉     | 1358/2771 [05:05<07:10,  3.28it/s]DLL 2022-02-17 18:47:04.961288 - Training Epoch: 0 Training Iteration: 1350  step_loss : 2.987522602081299  train_perf : 488.4126892089844 

Epoch: 000, Step:  1400, Loss:  3.14268017, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1401
Iteration:  52%|█████▏    | 1433/2771 [05:10<05:12,  4.29it/s]DLL 2022-02-17 18:47:08.304392 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.1426801681518555  train_perf : 488.3842468261719 

Epoch: 000, Step:  1450, Loss:  3.07393503, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1451
DLL 2022-02-17 18:47:11.654082 - Training Epoch: 0 Training Iteration: 1450  step_loss : 3.073935031890869  train_perf : 488.3280029296875 

Epoch: 000, Step:  1500, Loss:  3.60901713, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1501
Iteration:  54%|█████▍    | 1508/2771 [05:15<03:51,  5.45it/s]DLL 2022-02-17 18:47:15.002351 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.6090171337127686  train_perf : 488.278564453125 

Epoch: 000, Step:  1550, Loss:  3.36948299, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1551
Iteration:  57%|█████▋    | 1583/2771 [05:20<02:56,  6.73it/s]DLL 2022-02-17 18:47:18.348401 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.36948299407959  train_perf : 488.23883056640625 

Epoch: 000, Step:  1600, Loss:  3.19757748, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1601
2022-02-17 18:47:23.642022: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:47:23.757492: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:47:23.757551: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:47:23.757557: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:47:23.757559: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:47:23.757561: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:47:23.757563: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 18:47:21.704030 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.197577476501465  train_perf : 488.1734924316406 

Epoch: 000, Step:  1650, Loss:  3.25870895, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=21, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1650
Iteration:  60%|█████▉    | 1658/2771 [05:25<02:18,  8.02it/s]DLL 2022-02-17 18:47:25.203948 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.258708953857422  train_perf : 487.9070129394531 

Epoch: 000, Step:  1700, Loss:  3.07536459, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=71, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1700
Iteration:  63%|██████▎   | 1733/2771 [05:30<01:51,  9.31it/s]DLL 2022-02-17 18:47:28.550948 - Training Epoch: 0 Training Iteration: 1700  step_loss : 3.075364589691162  train_perf : 487.8886413574219 

Epoch: 000, Step:  1750, Loss:  3.24133587, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=121, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1750
DLL 2022-02-17 18:47:31.898856 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.241335868835449  train_perf : 487.85992431640625 

Epoch: 000, Step:  1800, Loss:  3.15503407, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=171, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1800
Iteration:  65%|██████▌   | 1808/2771 [05:35<01:31, 10.50it/s]DLL 2022-02-17 18:47:35.243960 - Training Epoch: 0 Training Iteration: 1800  step_loss : 3.155034065246582  train_perf : 487.84320068359375 

Epoch: 000, Step:  1850, Loss:  3.02864289, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=221, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1850
Iteration:  68%|██████▊   | 1883/2771 [05:40<01:17, 11.53it/s]DLL 2022-02-17 18:47:38.592727 - Training Epoch: 0 Training Iteration: 1850  step_loss : 3.0286428928375244  train_perf : 487.8174743652344 

Epoch: 000, Step:  1900, Loss:  2.70329094, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=271, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1900
DLL 2022-02-17 18:47:41.943040 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.7032909393310547  train_perf : 487.7856750488281 

Epoch: 000, Step:  1950, Loss:  2.76995468, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=321, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1950
Iteration:  71%|███████   | 1958/2771 [05:45<01:05, 12.37it/s]DLL 2022-02-17 18:47:45.290478 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.7699546813964844  train_perf : 487.7657165527344 

Epoch: 000, Step:  2000, Loss:  3.17105341, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=371, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2000
Iteration:  73%|███████▎  | 2033/2771 [05:50<00:56, 13.04it/s]DLL 2022-02-17 18:47:48.635964 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.171053409576416  train_perf : 487.7508239746094 

Epoch: 000, Step:  2050, Loss:  2.97135687, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=421, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2050
DLL 2022-02-17 18:47:51.992264 - Training Epoch: 0 Training Iteration: 2050  step_loss : 2.9713568687438965  train_perf : 487.7037353515625 

Epoch: 000, Step:  2100, Loss:  3.25836301, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=471, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2100
Iteration:  76%|███████▌  | 2108/2771 [05:55<00:48, 13.55it/s]DLL 2022-02-17 18:47:55.343538 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.2583630084991455  train_perf : 487.6739807128906 

Epoch: 000, Step:  2150, Loss:  3.04362679, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=521, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2150
Iteration:  79%|███████▉  | 2183/2771 [06:00<00:42, 13.94it/s]DLL 2022-02-17 18:47:58.694525 - Training Epoch: 0 Training Iteration: 2150  step_loss : 3.0436267852783203  train_perf : 487.64788818359375 

Epoch: 000, Step:  2200, Loss:  3.17890167, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=571, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-17 18:48:02.042098 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.1789016723632812  train_perf : 487.6302795410156 

Epoch: 000, Step:  2250, Loss:  2.87043834, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=621, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  81%|████████▏ | 2258/2771 [06:05<00:36, 14.22it/s]DLL 2022-02-17 18:48:05.399788 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.87043833732605  train_perf : 487.58697509765625 

Epoch: 000, Step:  2300, Loss:  3.03767133, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=671, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  84%|████████▍ | 2333/2771 [06:10<00:30, 14.41it/s]DLL 2022-02-17 18:48:08.757275 - Training Epoch: 0 Training Iteration: 2300  step_loss : 3.0376713275909424  train_perf : 487.54608154296875 

Epoch: 000, Step:  2350, Loss:  2.67759466, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=721, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-17 18:48:12.114997 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.6775946617126465  train_perf : 487.5019226074219 

Epoch: 000, Step:  2400, Loss:  2.94285631, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=771, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2408/2771 [06:15<00:24, 14.56it/s]DLL 2022-02-17 18:48:15.464110 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.9428563117980957  train_perf : 487.4768371582031 

Epoch: 000, Step:  2450, Loss:  3.25355315, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=821, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2483/2771 [06:20<00:19, 14.67it/s]DLL 2022-02-17 18:48:18.816067 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.2535531520843506  train_perf : 487.4479675292969 

Epoch: 000, Step:  2500, Loss:  3.36793494, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=871, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-17 18:48:22.165203 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.3679349422454834  train_perf : 487.4288635253906 

Epoch: 000, Step:  2550, Loss:  2.95975924, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=921, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  92%|█████████▏| 2558/2771 [06:25<00:14, 14.74it/s]DLL 2022-02-17 18:48:25.515449 - Training Epoch: 0 Training Iteration: 2550  step_loss : 2.95975923538208  train_perf : 487.4075012207031 

Epoch: 000, Step:  2600, Loss:  3.02760482, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=971, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2633/2771 [06:30<00:09, 14.79it/s]DLL 2022-02-17 18:48:28.866727 - Training Epoch: 0 Training Iteration: 2600  step_loss : 3.027604818344116  train_perf : 487.3853759765625 

Epoch: 000, Step:  2650, Loss:  3.09583378, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1021, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-17 18:48:32.222686 - Training Epoch: 0 Training Iteration: 2650  step_loss : 3.0958337783813477  train_perf : 487.3565673828125 

Epoch: 000, Step:  2700, Loss:  2.98628426, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1071, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
Iteration:  98%|█████████▊| 2708/2771 [06:35<00:04, 14.83it/s]DLL 2022-02-17 18:48:35.570765 - Training Epoch: 0 Training Iteration: 2700  step_loss : 2.9862842559814453  train_perf : 487.34307861328125 

Epoch: 000, Step:  2750, Loss:  3.06118488, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1121, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:40<00:00,  6.92it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-17 18:48:38.924936 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.061184883117676  train_perf : 487.31402587890625 
DLL 2022-02-17 18:48:40.197094 -  e2e_train_time : 400.12106466293335  training_sequences_per_second : 487.3121337890625  final_loss : 3.214693546295166 

Epoch: 001, Step:     0, Loss:  2.41563845, Perf:  458, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1141, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-17 18:48:44.084912 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.4156384468078613  train_perf : 458.4424133300781 

Epoch: 001, Step:    50, Loss:  2.60967875, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1191, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
Iteration:   3%|▎         | 71/2771 [00:05<03:12, 14.05it/s]DLL 2022-02-17 18:48:47.436176 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.6096787452697754  train_perf : 485.7539367675781 

Epoch: 001, Step:   100, Loss:  2.48435688, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1241, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 146/2771 [00:10<03:03, 14.30it/s]DLL 2022-02-17 18:48:50.783436 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.4843568801879883  train_perf : 486.253173828125 

Epoch: 001, Step:   150, Loss:  2.44152713, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1291, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-17 18:48:54.131677 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.4415271282196045  train_perf : 486.3453063964844 

Epoch: 001, Step:   200, Loss:  2.57479572, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1341, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 221/2771 [00:15<02:56, 14.49it/s]DLL 2022-02-17 18:48:57.481720 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.574795722961426  train_perf : 486.33551025390625 

Epoch: 001, Step:   250, Loss:  2.56356478, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1391, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 296/2771 [00:20<02:49, 14.62it/s]DLL 2022-02-17 18:49:00.822097 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.5635647773742676  train_perf : 486.5623474121094 

Epoch: 001, Step:   300, Loss:  2.54813838, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1441, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-17 18:49:04.168495 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.548138380050659  train_perf : 486.5849304199219 

Epoch: 001, Step:   350, Loss:  2.44607067, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1491, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  13%|█▎        | 371/2771 [00:25<02:43, 14.72it/s]DLL 2022-02-17 18:49:07.515117 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.446070671081543  train_perf : 486.6155700683594 

Epoch: 001, Step:   400, Loss:  2.16952348, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1541, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 446/2771 [00:30<02:37, 14.79it/s]DLL 2022-02-17 18:49:10.861141 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.1695234775543213  train_perf : 486.6878356933594 

Epoch: 001, Step:   450, Loss:  2.23301125, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1591, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-17 18:49:14.206115 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.233011245727539  train_perf : 486.75531005859375 

Epoch: 001, Step:   500, Loss:  2.61200380, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1641, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▉        | 521/2771 [00:35<02:31, 14.83it/s]DLL 2022-02-17 18:49:17.552904 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.612003803253174  train_perf : 486.75909423828125 

Epoch: 001, Step:   550, Loss:  2.26247311, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1691, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
Iteration:  22%|██▏       | 596/2771 [00:40<02:26, 14.86it/s]DLL 2022-02-17 18:49:20.901761 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.2624731063842773  train_perf : 486.7422180175781 

Epoch: 001, Step:   600, Loss:  1.99746490, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1741, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
DLL 2022-02-17 18:49:24.249276 - Training Epoch: 1 Training Iteration: 600  step_loss : 1.997464895248413  train_perf : 486.7381591796875 

Epoch: 001, Step:   650, Loss:  2.80309558, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1791, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 671/2771 [00:45<02:21, 14.89it/s]DLL 2022-02-17 18:49:27.593248 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.803095579147339  train_perf : 486.7783203125 

Epoch: 001, Step:   700, Loss:  2.46229267, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1841, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
Iteration:  27%|██▋       | 746/2771 [00:50<02:15, 14.91it/s]DLL 2022-02-17 18:49:30.936271 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.4622926712036133  train_perf : 486.8135681152344 

Epoch: 001, Step:   750, Loss:  2.00477672, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1891, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3520
DLL 2022-02-17 18:49:34.277499 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.0047767162323  train_perf : 486.8687438964844 

Epoch: 001, Step:   800, Loss:  2.80399728, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1941, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3570
Iteration:  30%|██▉       | 821/2771 [00:55<02:10, 14.92it/s]DLL 2022-02-17 18:49:37.623064 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.803997278213501  train_perf : 486.8646240234375 

Epoch: 001, Step:   850, Loss:  2.99206209, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1991, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3620
2022-02-17 18:49:41.573068: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:49:41.688610: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:49:41.688642: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:49:41.688668: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:49:41.688671: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:49:41.688673: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:49:41.688675: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:  32%|███▏      | 895/2771 [01:00<02:06, 14.78it/s]DLL 2022-02-17 18:49:40.967839 - Training Epoch: 1 Training Iteration: 850  step_loss : 2.9920620918273926  train_perf : 486.8765563964844 

Epoch: 001, Step:   900, Loss:  2.47528696, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=41, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3670
DLL 2022-02-17 18:49:44.479910 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.4752869606018066  train_perf : 486.38104248046875 

Epoch: 001, Step:   950, Loss:  2.72523475, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=91, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3720
Iteration:  35%|███▌      | 970/2771 [01:05<02:01, 14.82it/s]DLL 2022-02-17 18:49:47.831868 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.7252347469329834  train_perf : 486.3589172363281 

Epoch: 001, Step:  1000, Loss:  2.69584465, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=141, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3770
Iteration:  38%|███▊      | 1045/2771 [01:10<01:56, 14.85it/s]DLL 2022-02-17 18:49:51.181549 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.6958446502685547  train_perf : 486.3628234863281 

Epoch: 001, Step:  1050, Loss:  2.93266463, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=191, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3820
DLL 2022-02-17 18:49:54.531102 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.932664632797241  train_perf : 486.35589599609375 

Epoch: 001, Step:  1100, Loss:  3.39875746, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=241, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3870
Iteration:  40%|████      | 1120/2771 [01:15<01:50, 14.87it/s]DLL 2022-02-17 18:49:57.882142 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.3987574577331543  train_perf : 486.3460388183594 

Epoch: 001, Step:  1150, Loss:  3.10350227, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=291, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3920
Iteration:  43%|████▎     | 1195/2771 [01:20<01:45, 14.89it/s]DLL 2022-02-17 18:50:01.232759 - Training Epoch: 1 Training Iteration: 1150  step_loss : 3.1035022735595703  train_perf : 486.3470764160156 

Epoch: 001, Step:  1200, Loss:  2.60616636, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=341, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3970
DLL 2022-02-17 18:50:04.579042 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.606166362762451  train_perf : 486.3695983886719 

Epoch: 001, Step:  1250, Loss:  3.04022980, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=391, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4020
Iteration:  46%|████▌     | 1270/2771 [01:25<01:40, 14.90it/s]DLL 2022-02-17 18:50:07.925737 - Training Epoch: 1 Training Iteration: 1250  step_loss : 3.0402297973632812  train_perf : 486.3803405761719 

Epoch: 001, Step:  1300, Loss:  2.39365435, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=441, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4070
Iteration:  49%|████▊     | 1345/2771 [01:30<01:35, 14.91it/s]DLL 2022-02-17 18:50:11.274802 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.3936543464660645  train_perf : 486.3875732421875 

Epoch: 001, Step:  1350, Loss:  2.55270815, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=491, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4120
DLL 2022-02-17 18:50:14.621959 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.552708148956299  train_perf : 486.400634765625 

Epoch: 001, Step:  1400, Loss:  2.57172155, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=541, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4170
Iteration:  51%|█████     | 1420/2771 [01:35<01:30, 14.92it/s]DLL 2022-02-17 18:50:17.971721 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.5717215538024902  train_perf : 486.40252685546875 

Epoch: 001, Step:  1450, Loss:  2.40612960, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=24, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4219
Iteration:  54%|█████▍    | 1495/2771 [01:40<01:25, 14.92it/s]DLL 2022-02-17 18:50:21.320023 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.4061295986175537  train_perf : 486.39752197265625 

Epoch: 001, Step:  1500, Loss:  3.43952775, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=74, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4269
DLL 2022-02-17 18:50:24.672114 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.439527750015259  train_perf : 486.38824462890625 

Epoch: 001, Step:  1550, Loss:  2.60133862, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=124, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4319
Iteration:  57%|█████▋    | 1570/2771 [01:45<01:20, 14.92it/s]DLL 2022-02-17 18:50:28.026954 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.6013386249542236  train_perf : 486.3679504394531 

Epoch: 001, Step:  1600, Loss:  2.63185310, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=174, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4369
Iteration:  59%|█████▉    | 1645/2771 [01:50<01:15, 14.92it/s]DLL 2022-02-17 18:50:31.377473 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.6318531036376953  train_perf : 486.3616943359375 

Epoch: 001, Step:  1650, Loss:  2.65652609, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=224, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4419
DLL 2022-02-17 18:50:34.729580 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.6565260887145996  train_perf : 486.3472595214844 

Epoch: 001, Step:  1700, Loss:  2.69106722, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=274, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4469
Iteration:  62%|██████▏   | 1720/2771 [01:55<01:10, 14.91it/s]DLL 2022-02-17 18:50:38.083014 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.6910672187805176  train_perf : 486.3292541503906 

Epoch: 001, Step:  1750, Loss:  2.90482521, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=324, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4519
Iteration:  65%|██████▍   | 1795/2771 [02:00<01:05, 14.92it/s]DLL 2022-02-17 18:50:41.432740 - Training Epoch: 1 Training Iteration: 1750  step_loss : 2.904825210571289  train_perf : 486.33349609375 

Epoch: 001, Step:  1800, Loss:  2.48857594, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=374, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4569
DLL 2022-02-17 18:50:44.785088 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.4885759353637695  train_perf : 486.32757568359375 

Epoch: 001, Step:  1850, Loss:  2.67999601, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=424, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4619
Iteration:  67%|██████▋   | 1870/2771 [02:05<01:00, 14.92it/s]DLL 2022-02-17 18:50:48.131595 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.6799960136413574  train_perf : 486.33331298828125 

Epoch: 001, Step:  1900, Loss:  2.67668962, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=474, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4669
Iteration:  70%|███████   | 1945/2771 [02:10<00:55, 14.92it/s]DLL 2022-02-17 18:50:51.481558 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.676689624786377  train_perf : 486.3290710449219 

Epoch: 001, Step:  1950, Loss:  2.67379951, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=524, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4719
DLL 2022-02-17 18:50:54.832302 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.673799514770508  train_perf : 486.3235168457031 

Epoch: 001, Step:  2000, Loss:  2.72250342, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=574, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4769
Iteration:  73%|███████▎  | 2020/2771 [02:15<00:50, 14.93it/s]DLL 2022-02-17 18:50:58.177095 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.722503423690796  train_perf : 486.34100341796875 

Epoch: 001, Step:  2050, Loss:  2.34611225, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=624, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
Iteration:  76%|███████▌  | 2095/2771 [02:20<00:45, 14.93it/s]DLL 2022-02-17 18:51:01.528136 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.3461122512817383  train_perf : 486.3388366699219 

Epoch: 001, Step:  2100, Loss:  2.92393494, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=674, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-17 18:51:04.879417 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.9239349365234375  train_perf : 486.3306579589844 

Epoch: 001, Step:  2150, Loss:  2.66928053, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=724, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2170/2771 [02:25<00:40, 14.92it/s]DLL 2022-02-17 18:51:08.230687 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.669280529022217  train_perf : 486.3255920410156 

Epoch: 001, Step:  2200, Loss:  2.57660961, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=774, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2245/2771 [02:30<00:35, 14.92it/s]DLL 2022-02-17 18:51:11.584608 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.5766096115112305  train_perf : 486.3151550292969 

Epoch: 001, Step:  2250, Loss:  2.56801343, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=824, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-17 18:51:14.939861 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.5680134296417236  train_perf : 486.29852294921875 

Epoch: 001, Step:  2300, Loss:  2.41882157, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=874, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▎ | 2320/2771 [02:35<00:30, 14.92it/s]DLL 2022-02-17 18:51:18.286605 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.4188215732574463  train_perf : 486.3077392578125 

Epoch: 001, Step:  2350, Loss:  2.43593359, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=924, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  86%|████████▋ | 2395/2771 [02:40<00:25, 14.92it/s]DLL 2022-02-17 18:51:21.634679 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.4359335899353027  train_perf : 486.3131103515625 

Epoch: 001, Step:  2400, Loss:  2.38981843, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=974, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-17 18:51:24.984691 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.3898184299468994  train_perf : 486.30938720703125 

Epoch: 001, Step:  2450, Loss:  2.75341654, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1024, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2470/2771 [02:45<00:20, 14.92it/s]DLL 2022-02-17 18:51:28.337325 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.7534165382385254  train_perf : 486.29998779296875 

Epoch: 001, Step:  2500, Loss:  2.73663211, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1074, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2545/2771 [02:50<00:15, 14.92it/s]DLL 2022-02-17 18:51:31.686712 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.7366321086883545  train_perf : 486.2989807128906 

Epoch: 001, Step:  2550, Loss:  2.69906569, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1124, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-17 18:51:35.037259 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.699065685272217  train_perf : 486.29119873046875 

Epoch: 001, Step:  2600, Loss:  3.04154086, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1174, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▍| 2620/2771 [02:55<00:10, 14.92it/s]DLL 2022-02-17 18:51:38.389316 - Training Epoch: 1 Training Iteration: 2600  step_loss : 3.0415408611297607  train_perf : 486.28460693359375 

Epoch: 001, Step:  2650, Loss:  2.79960752, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1224, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2695/2771 [03:00<00:05, 14.93it/s]DLL 2022-02-17 18:51:41.735727 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.799607515335083  train_perf : 486.28912353515625 

Epoch: 001, Step:  2700, Loss:  2.76094580, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1274, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-17 18:51:45.086811 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.7609457969665527  train_perf : 486.28826904296875 

Epoch: 001, Step:  2750, Loss:  2.59643602, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1324, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.93it/s]Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.89it/s]DLL 2022-02-17 18:51:48.433356 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.596436023712158  train_perf : 486.2954406738281 
DLL 2022-02-17 18:51:49.708466 -  e2e_train_time : 586.1088769435883  training_sequences_per_second : 486.2947692871094  final_loss : 2.6766393184661865 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-17 18:51:55.144963: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 18:53:20.349759: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:53:21.421920: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:53:21.421938: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:53:21.421945: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:53:21.421947: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:53:21.421949: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:53:21.421951: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:30<31:40, 90.52s/it]Iteration:  14%|█▎        | 3/22 [01:36<20:19, 64.20s/it]Iteration:  23%|██▎       | 5/22 [01:41<12:58, 45.81s/it]Iteration:  32%|███▏      | 7/22 [01:47<08:13, 32.93s/it]Iteration:  41%|████      | 9/22 [01:53<05:11, 23.97s/it]Iteration:  50%|█████     | 11/22 [01:59<03:14, 17.70s/it]Iteration:  59%|█████▉    | 13/22 [02:06<02:00, 13.34s/it]Iteration:  68%|██████▊   | 15/22 [02:12<01:12, 10.30s/it]Iteration:  77%|███████▋  | 17/22 [02:19<00:40,  8.19s/it]Iteration:  86%|████████▋ | 19/22 [02:25<00:20,  6.72s/it]Iteration:  95%|█████████▌| 21/22 [02:32<00:05,  5.72s/it]2022-02-17 18:54:27.357611: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.72s/it]2022-02-17 18:55:37.683940: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 18:55:38.740858: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 18:55:38.740875: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 18:55:38.740881: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 18:55:38.740883: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 18:55:38.740885: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 18:55:38.740887: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:45<00:00, 25.92s/it]Iteration: 100%|██████████| 22/22 [03:45<00:00, 10.25s/it]{"exact_match": 10.993377483443709, "f1": 14.972793713079303}

Epoch: 001 Results: {"exact_match": 10.993377483443709, "f1": 14.972793713079303}

**EVAL SUMMARY** - Epoch: 001,  EM: 10.993, F1: 14.973, Infer_Perf: 1052 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 442.561 ms, 90%: 441.987 ms, 95%: 442.268 ms, 99%: 442.268 ms
DLL 2022-02-17 18:55:55.952258 -  inference_sequences_per_second : 1052.3614501953125  e2e_inference_time : 239.592200756073 
**RESULTS SUMMARY** - EM: 10.993, F1: 14.973, Train_Time:  586 s, Train_Perf:  486 seq/s, Infer_Perf: 1052 seq/s

DLL 2022-02-17 18:55:55.953076 -  exact_match : 10.993377483443709  F1 : 14.972793713079303 
====================================  END results/models/base/checkpoints/ckpt-1997  ====================================
==================================== START results/models/base/checkpoints/ckpt-2497 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/base/checkpoints/ckpt-2497 at step 2496
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/base/checkpoints/discriminator/config.json
Model weights saved in results/models/base/checkpoints/discriminator/tf_model.h5: {'discriminator_predictions', 'electra'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=test_results/models/base/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-17 18:56:02.834589: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-17 18:56:03.684507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-17 18:56:03.700079: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:03.700514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 18:56:03.700530: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:56:03.701960: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 18:56:03.702525: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 18:56:03.702658: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 18:56:03.704061: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 18:56:03.704387: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 18:56:03.704463: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:56:03.704503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:03.704936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:03.705338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 18:56:03.710504: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-17 18:56:03.710958: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8e48000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-17 18:56:03.710970: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-17 18:56:03.850815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:03.851318: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8e14000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-17 18:56:03.851332: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-17 18:56:03.851442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:03.851862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 18:56:03.851880: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:56:03.851903: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 18:56:03.851915: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 18:56:03.851925: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 18:56:03.851935: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 18:56:03.851945: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 18:56:03.851954: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:56:03.851984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:03.852405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:03.852803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 18:56:03.852819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 18:56:04.017589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-17 18:56:04.017614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-17 18:56:04.017619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-17 18:56:04.017735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:04.018187: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 18:56:04.018607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-17 18:56:03.683946 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: test_results/models/base/checkpoints/discriminator
loading configuration file test_results/models/base/checkpoints/discriminator/config.json
loading weights file test_results/models/base/checkpoints/discriminator/tf_model.h5
2022-02-17 18:56:04.086016: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-17 18:56:04.125091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at test_results/models/base/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at test_results/models/base/checkpoints/discriminator and are newly initialized: ['start_logits', 'end_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 37%|███▋      | 162/442 [00:05<00:08, 32.35it/s] 67%|██████▋   | 297/442 [00:10<00:04, 30.46it/s] 99%|█████████▊| 436/442 [00:15<00:00, 29.57it/s]100%|██████████| 442/442 [00:15<00:00, 28.93it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 26.63it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-17 18:56:58.805314: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-17 18:56:59.666687: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 18:56:59.692452: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 18:57:00.297713: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-17 18:57:00.586379: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-17 19:00:05.644822: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:08.915188: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:00:08.915207: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:00:08.915213: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:00:08.915215: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:00:08.915217: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:00:08.915219: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:00:08.928632: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-17 19:00:10.896299: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:11.040146: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:00:11.040164: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:00:11.040170: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:00:11.040172: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:00:11.040173: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:00:11.040175: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:00:11.099637: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:11.283508: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:00:11.283526: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:00:11.283532: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:00:11.283534: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:00:11.283536: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:00:11.283538: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:00:11.502896: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:12.258142: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:00:12.258161: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:00:12.258167: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:00:12.258169: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:00:12.258171: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:00:12.258173: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.27780342, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:22<155:26:51, 202.03s/it]2022-02-17 19:00:15.323080: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 19:00:16.644343: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:19.920611: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:00:19.920630: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:00:19.920637: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:00:19.920639: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:00:19.920641: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:00:19.920642: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:00:21.847871: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:21.934879: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:22.219595: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:23.202254: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:23.327404: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:00:23.327424: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:00:23.327430: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:00:23.327433: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:00:23.327434: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:00:23.327436: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:32<111:14:09, 144.62s/it]DLL 2022-02-17 19:00:12.694444 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.277803421020508  train_perf : 0.15862756967544556 

Epoch: 000, Step:    50, Loss:  5.06272221, Perf:  471, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:37<75:44:34, 101.25s/it]DLL 2022-02-17 19:00:26.625771 - Training Epoch: 0 Training Iteration: 50  step_loss : 5.062722206115723  train_perf : 470.80584716796875 

Epoch: 000, Step:   100, Loss:  4.21479559, Perf:  482, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-17 19:00:29.930802 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.2147955894470215  train_perf : 481.9837646484375 

Epoch: 000, Step:   150, Loss:  3.55074930, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 154/2771 [03:42<51:32:17, 70.90s/it]DLL 2022-02-17 19:00:33.251134 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.5507493019104004  train_perf : 485.03466796875 

Epoch: 000, Step:   200, Loss:  3.86752796, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 230/2771 [03:47<35:02:35, 49.65s/it]DLL 2022-02-17 19:00:36.572952 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.867527961730957  train_perf : 486.5437316894531 

Epoch: 000, Step:   250, Loss:  3.20227289, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-17 19:00:41.681234: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:00:41.800500: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:00:41.800532: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:00:41.800538: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:00:41.800540: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:00:41.800542: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:00:41.800544: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 19:00:39.892468 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.202272891998291  train_perf : 487.5115661621094 

Epoch: 000, Step:   300, Loss:  3.14249444, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 303/2771 [03:52<23:50:22, 34.77s/it]DLL 2022-02-17 19:00:43.367173 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.1424944400787354  train_perf : 486.9754333496094 

Epoch: 000, Step:   350, Loss:  3.05513430, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 379/2771 [03:57<16:11:13, 24.36s/it]DLL 2022-02-17 19:00:46.684667 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.0551342964172363  train_perf : 487.680908203125 

Epoch: 000, Step:   400, Loss:  3.31266069, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-17 19:00:50.010866 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.3126606941223145  train_perf : 487.98675537109375 

Epoch: 000, Step:   450, Loss:  2.76385093, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 455/2771 [04:02<10:59:01, 17.07s/it]DLL 2022-02-17 19:00:53.338244 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.7638509273529053  train_perf : 488.18133544921875 

Epoch: 000, Step:   500, Loss:  2.91762543, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 531/2771 [04:07<7:26:55, 11.97s/it] DLL 2022-02-17 19:00:56.669015 - Training Epoch: 0 Training Iteration: 500  step_loss : 2.9176254272460938  train_perf : 488.30487060546875 

Epoch: 000, Step:   550, Loss:  3.13179088, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-17 19:00:59.999409 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.13179087638855  train_perf : 488.41265869140625 

Epoch: 000, Step:   600, Loss:  2.82760906, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 607/2771 [04:13<5:02:57,  8.40s/it]DLL 2022-02-17 19:01:03.329572 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.827609062194824  train_perf : 488.5129089355469 

Epoch: 000, Step:   650, Loss:  3.29234719, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 683/2771 [04:18<3:25:18,  5.90s/it]DLL 2022-02-17 19:01:06.656724 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.2923471927642822  train_perf : 488.6120910644531 

Epoch: 000, Step:   700, Loss:  2.96656561, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
DLL 2022-02-17 19:01:09.984916 - Training Epoch: 0 Training Iteration: 700  step_loss : 2.9665656089782715  train_perf : 488.69696044921875 

Epoch: 000, Step:   750, Loss:  2.71242476, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:751
Iteration:  27%|██▋       | 759/2771 [04:23<2:19:09,  4.15s/it]DLL 2022-02-17 19:01:13.320329 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.7124247550964355  train_perf : 488.7100830078125 

Epoch: 000, Step:   800, Loss:  2.97495294, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:801
2022-02-17 19:01:18.115749: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:01:18.237077: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:01:18.237096: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:01:18.237102: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:01:18.237105: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:01:18.237106: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:01:18.237108: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:  30%|███       | 832/2771 [04:28<1:34:32,  2.93s/it]DLL 2022-02-17 19:01:16.653059 - Training Epoch: 0 Training Iteration: 800  step_loss : 2.9749529361724854  train_perf : 488.74468994140625 

Epoch: 000, Step:   850, Loss:  3.38942099, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=28, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:850
DLL 2022-02-17 19:01:20.142772 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.389420986175537  train_perf : 488.3224792480469 

Epoch: 000, Step:   900, Loss:  2.49625301, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=78, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:900
Iteration:  33%|███▎      | 907/2771 [04:33<1:04:14,  2.07s/it]DLL 2022-02-17 19:01:23.480689 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.49625301361084  train_perf : 488.3270568847656 

Epoch: 000, Step:   950, Loss:  3.44685268, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=128, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:950
Iteration:  35%|███▌      | 983/2771 [04:38<43:43,  1.47s/it]  DLL 2022-02-17 19:01:26.811656 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.446852684020996  train_perf : 488.39080810546875 

Epoch: 000, Step:  1000, Loss:  3.04255986, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=178, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1000
DLL 2022-02-17 19:01:30.147493 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.042559862136841  train_perf : 488.4060363769531 

Epoch: 000, Step:  1050, Loss:  3.16772509, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=228, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1050
Iteration:  38%|███▊      | 1058/2771 [04:43<29:54,  1.05s/it]DLL 2022-02-17 19:01:33.482813 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.167725086212158  train_perf : 488.4310607910156 

Epoch: 000, Step:  1100, Loss:  3.68446374, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=278, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1100
Iteration:  41%|████      | 1133/2771 [04:48<20:33,  1.33it/s]DLL 2022-02-17 19:01:36.825668 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.6844637393951416  train_perf : 488.4046936035156 

Epoch: 000, Step:  1150, Loss:  3.69286108, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=328, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1150
DLL 2022-02-17 19:01:40.169330 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.6928610801696777  train_perf : 488.3768310546875 

Epoch: 000, Step:  1200, Loss:  3.07079649, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=378, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1200
Iteration:  44%|████▎     | 1208/2771 [04:53<14:15,  1.83it/s]DLL 2022-02-17 19:01:43.514957 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.070796489715576  train_perf : 488.3428955078125 

Epoch: 000, Step:  1250, Loss:  3.56490064, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=428, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1250
Iteration:  46%|████▋     | 1283/2771 [04:58<09:59,  2.48it/s]DLL 2022-02-17 19:01:46.853244 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.5649006366729736  train_perf : 488.3407287597656 

Epoch: 000, Step:  1300, Loss:  3.23658824, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=478, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1300
DLL 2022-02-17 19:01:50.199524 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.2365882396698  train_perf : 488.2875671386719 

Epoch: 000, Step:  1350, Loss:  2.97904062, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=528, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1350
Iteration:  49%|████▉     | 1358/2771 [05:03<07:07,  3.31it/s]DLL 2022-02-17 19:01:53.547228 - Training Epoch: 0 Training Iteration: 1350  step_loss : 2.9790406227111816  train_perf : 488.23419189453125 

Epoch: 000, Step:  1400, Loss:  3.19293427, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=578, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1400
Iteration:  52%|█████▏    | 1433/2771 [05:08<05:10,  4.32it/s]DLL 2022-02-17 19:01:56.896833 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.192934274673462  train_perf : 488.18780517578125 

Epoch: 000, Step:  1450, Loss:  2.97252846, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=628, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1450
DLL 2022-02-17 19:02:00.249357 - Training Epoch: 0 Training Iteration: 1450  step_loss : 2.9725284576416016  train_perf : 488.123046875 

Epoch: 000, Step:  1500, Loss:  3.51810861, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=678, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1500
Iteration:  54%|█████▍    | 1508/2771 [05:13<03:50,  5.49it/s]DLL 2022-02-17 19:02:03.602469 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.518108606338501  train_perf : 488.0672912597656 

Epoch: 000, Step:  1550, Loss:  3.11493278, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=728, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1550
Iteration:  57%|█████▋    | 1583/2771 [05:18<02:55,  6.77it/s]DLL 2022-02-17 19:02:06.954687 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.1149327754974365  train_perf : 488.00970458984375 

Epoch: 000, Step:  1600, Loss:  3.08210373, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=778, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1600
DLL 2022-02-17 19:02:10.310388 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.082103729248047  train_perf : 487.9415588378906 

Epoch: 000, Step:  1650, Loss:  3.21211195, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=828, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1650
Iteration:  60%|█████▉    | 1658/2771 [05:23<02:17,  8.10it/s]DLL 2022-02-17 19:02:13.658203 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.2121119499206543  train_perf : 487.9063720703125 

Epoch: 000, Step:  1700, Loss:  2.99617362, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=878, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1700
Iteration:  63%|██████▎   | 1733/2771 [05:28<01:50,  9.38it/s]DLL 2022-02-17 19:02:17.004644 - Training Epoch: 0 Training Iteration: 1700  step_loss : 2.996173620223999  train_perf : 487.8782958984375 

Epoch: 000, Step:  1750, Loss:  3.23450565, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=928, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1750
DLL 2022-02-17 19:02:20.361165 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.2345056533813477  train_perf : 487.8178405761719 

Epoch: 000, Step:  1800, Loss:  3.00259304, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=978, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1800
Iteration:  65%|██████▌   | 1808/2771 [05:33<01:31, 10.56it/s]DLL 2022-02-17 19:02:23.707933 - Training Epoch: 0 Training Iteration: 1800  step_loss : 3.0025930404663086  train_perf : 487.79534912109375 

Epoch: 000, Step:  1850, Loss:  2.95797205, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1028, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1850
Iteration:  68%|██████▊   | 1883/2771 [05:38<01:16, 11.58it/s]DLL 2022-02-17 19:02:27.057905 - Training Epoch: 0 Training Iteration: 1850  step_loss : 2.9579720497131348  train_perf : 487.7587585449219 

Epoch: 000, Step:  1900, Loss:  2.74201441, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1078, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1900
DLL 2022-02-17 19:02:30.410146 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.7420144081115723  train_perf : 487.7193603515625 

Epoch: 000, Step:  1950, Loss:  2.75693226, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1128, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1950
Iteration:  71%|███████   | 1958/2771 [05:43<01:05, 12.41it/s]DLL 2022-02-17 19:02:33.759330 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.756932258605957  train_perf : 487.6913757324219 

Epoch: 000, Step:  2000, Loss:  3.03243279, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1178, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2000
Iteration:  73%|███████▎  | 2033/2771 [05:48<00:56, 13.07it/s]DLL 2022-02-17 19:02:37.114128 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.032432794570923  train_perf : 487.64703369140625 

Epoch: 000, Step:  2050, Loss:  3.04370832, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1228, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2050
DLL 2022-02-17 19:02:40.461201 - Training Epoch: 0 Training Iteration: 2050  step_loss : 3.043708324432373  train_perf : 487.6247253417969 

Epoch: 000, Step:  2100, Loss:  3.39351296, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1278, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2100
Iteration:  76%|███████▌  | 2108/2771 [05:53<00:48, 13.57it/s]DLL 2022-02-17 19:02:43.815033 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.3935129642486572  train_perf : 487.58514404296875 

Epoch: 000, Step:  2150, Loss:  3.05353832, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1328, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2150
Iteration:  79%|███████▉  | 2183/2771 [05:58<00:42, 13.95it/s]DLL 2022-02-17 19:02:47.160618 - Training Epoch: 0 Training Iteration: 2150  step_loss : 3.0535383224487305  train_perf : 487.57666015625 

Epoch: 000, Step:  2200, Loss:  3.02574205, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1378, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-17 19:02:50.513619 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.0257420539855957  train_perf : 487.54571533203125 

Epoch: 000, Step:  2250, Loss:  2.86501312, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1428, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  81%|████████▏ | 2258/2771 [06:03<00:36, 14.23it/s]DLL 2022-02-17 19:02:53.863346 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.8650131225585938  train_perf : 487.5256652832031 

Epoch: 000, Step:  2300, Loss:  3.22640777, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1478, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  84%|████████▍ | 2333/2771 [06:08<00:30, 14.43it/s]DLL 2022-02-17 19:02:57.213004 - Training Epoch: 0 Training Iteration: 2300  step_loss : 3.226407766342163  train_perf : 487.5016174316406 

Epoch: 000, Step:  2350, Loss:  2.71254373, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1528, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-17 19:03:00.565114 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.7125437259674072  train_perf : 487.4775085449219 

Epoch: 000, Step:  2400, Loss:  2.85303545, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1578, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2408/2771 [06:13<00:24, 14.57it/s]DLL 2022-02-17 19:03:03.917991 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.8530354499816895  train_perf : 487.4490966796875 

Epoch: 000, Step:  2450, Loss:  3.34248018, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1628, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2483/2771 [06:18<00:19, 14.68it/s]DLL 2022-02-17 19:03:07.265598 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.342480182647705  train_perf : 487.4350280761719 

Epoch: 000, Step:  2500, Loss:  3.33497190, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1678, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-17 19:03:10.614581 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.3349719047546387  train_perf : 487.4176330566406 

Epoch: 000, Step:  2550, Loss:  2.96844578, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1728, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  92%|█████████▏| 2558/2771 [06:23<00:14, 14.75it/s]DLL 2022-02-17 19:03:13.968510 - Training Epoch: 0 Training Iteration: 2550  step_loss : 2.9684457778930664  train_perf : 487.3915100097656 

Epoch: 000, Step:  2600, Loss:  3.06315279, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1778, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2633/2771 [06:28<00:09, 14.80it/s]DLL 2022-02-17 19:03:17.315730 - Training Epoch: 0 Training Iteration: 2600  step_loss : 3.06315279006958  train_perf : 487.3761901855469 

Epoch: 000, Step:  2650, Loss:  3.08336234, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1828, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-17 19:03:20.665831 - Training Epoch: 0 Training Iteration: 2650  step_loss : 3.083362340927124  train_perf : 487.3565979003906 

Epoch: 000, Step:  2700, Loss:  2.95761681, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1878, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
Iteration:  98%|█████████▊| 2708/2771 [06:33<00:04, 14.83it/s]DLL 2022-02-17 19:03:24.022085 - Training Epoch: 0 Training Iteration: 2700  step_loss : 2.9576168060302734  train_perf : 487.3228759765625 

Epoch: 000, Step:  2750, Loss:  3.15258503, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1928, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:37<00:00,  6.96it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-17 19:03:27.373673 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.152585029602051  train_perf : 487.3019714355469 
DLL 2022-02-17 19:03:28.647148 -  e2e_train_time : 397.97758507728577  training_sequences_per_second : 487.29791259765625  final_loss : 3.2052276134490967 

Epoch: 001, Step:     0, Loss:  2.59345341, Perf:  456, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1948, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-17 19:03:32.788328 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.5934534072875977  train_perf : 456.04046630859375 

Epoch: 001, Step:    50, Loss:  2.56851482, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1998, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
2022-02-17 19:03:36.275273: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:03:36.392939: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:03:36.392959: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:03:36.392966: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:03:36.392968: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:03:36.392970: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:03:36.392971: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   2%|▏         | 68/2771 [00:05<03:19, 13.57it/s]DLL 2022-02-17 19:03:36.139867 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.568514823913574  train_perf : 485.5926513671875 

Epoch: 001, Step:   100, Loss:  2.49812865, Perf:  482, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=48, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 143/2771 [00:10<03:08, 13.94it/s]DLL 2022-02-17 19:03:39.655844 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.498128652572632  train_perf : 481.8295593261719 

Epoch: 001, Step:   150, Loss:  2.50680923, Perf:  483, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=98, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-17 19:03:43.011992 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.5068092346191406  train_perf : 483.0711975097656 

Epoch: 001, Step:   200, Loss:  2.39074516, Perf:  484, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=148, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 218/2771 [00:15<02:59, 14.22it/s]DLL 2022-02-17 19:03:46.366268 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.390745162963867  train_perf : 483.74639892578125 

Epoch: 001, Step:   250, Loss:  2.62058139, Perf:  484, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=198, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 293/2771 [00:20<02:51, 14.42it/s]DLL 2022-02-17 19:03:49.716756 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.6205813884735107  train_perf : 484.3013916015625 

Epoch: 001, Step:   300, Loss:  2.52997065, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=248, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-17 19:03:53.072679 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.529970645904541  train_perf : 484.52886962890625 

Epoch: 001, Step:   350, Loss:  2.48909163, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=298, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  13%|█▎        | 368/2771 [00:25<02:44, 14.56it/s]DLL 2022-02-17 19:03:56.422302 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.489091634750366  train_perf : 484.81219482421875 

Epoch: 001, Step:   400, Loss:  2.12463880, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=348, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 443/2771 [00:30<02:38, 14.67it/s]DLL 2022-02-17 19:03:59.776564 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.124638795852661  train_perf : 484.9660339355469 

Epoch: 001, Step:   450, Loss:  2.08840108, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=398, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-17 19:04:03.125257 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.0884010791778564  train_perf : 485.1485595703125 

Epoch: 001, Step:   500, Loss:  2.52040005, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=448, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▊        | 518/2771 [00:35<02:32, 14.74it/s]DLL 2022-02-17 19:04:06.477609 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.520400047302246  train_perf : 485.2214660644531 

Epoch: 001, Step:   550, Loss:  2.13218451, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=498, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
Iteration:  21%|██▏       | 593/2771 [00:40<02:27, 14.80it/s]DLL 2022-02-17 19:04:09.826831 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.1321845054626465  train_perf : 485.344482421875 

Epoch: 001, Step:   600, Loss:  2.05743909, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=548, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
DLL 2022-02-17 19:04:13.177663 - Training Epoch: 1 Training Iteration: 600  step_loss : 2.057439088821411  train_perf : 485.41815185546875 

Epoch: 001, Step:   650, Loss:  2.72748470, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=598, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 668/2771 [00:45<02:21, 14.84it/s]DLL 2022-02-17 19:04:16.523119 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.727484703063965  train_perf : 485.5465087890625 

Epoch: 001, Step:   700, Loss:  2.32406878, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=648, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
Iteration:  27%|██▋       | 743/2771 [00:50<02:16, 14.87it/s]DLL 2022-02-17 19:04:19.869625 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.324068784713745  train_perf : 485.64056396484375 

Epoch: 001, Step:   750, Loss:  2.07088470, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=698, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3520
DLL 2022-02-17 19:04:23.218788 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.0708847045898438  train_perf : 485.7019348144531 

Epoch: 001, Step:   800, Loss:  2.89325333, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=748, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3570
Iteration:  30%|██▉       | 818/2771 [00:55<02:11, 14.89it/s]DLL 2022-02-17 19:04:26.566849 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.8932533264160156  train_perf : 485.7526550292969 

Epoch: 001, Step:   850, Loss:  3.11824608, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=798, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3620
Iteration:  32%|███▏      | 893/2771 [01:00<02:06, 14.90it/s]DLL 2022-02-17 19:04:29.915971 - Training Epoch: 1 Training Iteration: 850  step_loss : 3.118246078491211  train_perf : 485.8036193847656 

Epoch: 001, Step:   900, Loss:  2.44981194, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=848, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3670
DLL 2022-02-17 19:04:33.261406 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.4498119354248047  train_perf : 485.8780212402344 

Epoch: 001, Step:   950, Loss:  2.78002906, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=898, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3720
Iteration:  35%|███▍      | 968/2771 [01:05<02:00, 14.91it/s]DLL 2022-02-17 19:04:36.611816 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.780029058456421  train_perf : 485.8982849121094 

Epoch: 001, Step:  1000, Loss:  2.72878456, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=948, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3770
Iteration:  38%|███▊      | 1043/2771 [01:10<01:55, 14.91it/s]DLL 2022-02-17 19:04:39.964757 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.7287845611572266  train_perf : 485.90771484375 

Epoch: 001, Step:  1050, Loss:  2.93682981, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=998, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3820
DLL 2022-02-17 19:04:43.315506 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.9368298053741455  train_perf : 485.91558837890625 

Epoch: 001, Step:  1100, Loss:  3.46661830, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1048, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3870
Iteration:  40%|████      | 1118/2771 [01:15<01:50, 14.92it/s]DLL 2022-02-17 19:04:46.661903 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.466618299484253  train_perf : 485.9559020996094 

Epoch: 001, Step:  1150, Loss:  2.97248220, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1098, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3920
Iteration:  43%|████▎     | 1193/2771 [01:20<01:45, 14.92it/s]DLL 2022-02-17 19:04:50.014124 - Training Epoch: 1 Training Iteration: 1150  step_loss : 2.972482204437256  train_perf : 485.9590148925781 

Epoch: 001, Step:  1200, Loss:  2.63011360, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1148, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3970
DLL 2022-02-17 19:04:53.367224 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.6301136016845703  train_perf : 485.9591369628906 

Epoch: 001, Step:  1250, Loss:  3.03406620, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1198, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4020
Iteration:  46%|████▌     | 1268/2771 [01:25<01:40, 14.92it/s]DLL 2022-02-17 19:04:56.715441 - Training Epoch: 1 Training Iteration: 1250  step_loss : 3.0340662002563477  train_perf : 485.97552490234375 

Epoch: 001, Step:  1300, Loss:  2.59607601, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1248, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4070
Iteration:  48%|████▊     | 1343/2771 [01:30<01:35, 14.92it/s]DLL 2022-02-17 19:05:00.066051 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.596076011657715  train_perf : 485.9896240234375 

Epoch: 001, Step:  1350, Loss:  2.44266653, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1298, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4120
DLL 2022-02-17 19:05:03.416981 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.442666530609131  train_perf : 486.00177001953125 

Epoch: 001, Step:  1400, Loss:  2.51030898, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1348, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4170
Iteration:  51%|█████     | 1418/2771 [01:35<01:30, 14.92it/s]DLL 2022-02-17 19:05:06.769173 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.5103089809417725  train_perf : 486.0022888183594 

Epoch: 001, Step:  1450, Loss:  2.31158471, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1398, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4220
Iteration:  54%|█████▍    | 1493/2771 [01:40<01:25, 14.91it/s]DLL 2022-02-17 19:05:10.121378 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.311584711074829  train_perf : 486.0063171386719 

Epoch: 001, Step:  1500, Loss:  3.49151206, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1448, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4270
DLL 2022-02-17 19:05:13.476438 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.4915120601654053  train_perf : 485.996337890625 

Epoch: 001, Step:  1550, Loss:  2.54931498, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1498, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4320
Iteration:  57%|█████▋    | 1568/2771 [01:45<01:20, 14.92it/s]DLL 2022-02-17 19:05:16.828041 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.5493149757385254  train_perf : 486.00152587890625 

Epoch: 001, Step:  1600, Loss:  2.62250757, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1548, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4370
Iteration:  59%|█████▉    | 1643/2771 [01:50<01:15, 14.92it/s]DLL 2022-02-17 19:05:20.178437 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.6225075721740723  train_perf : 486.0050048828125 

Epoch: 001, Step:  1650, Loss:  2.59454393, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1598, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4420
DLL 2022-02-17 19:05:23.530072 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.594543933868408  train_perf : 486.01263427734375 

Epoch: 001, Step:  1700, Loss:  2.58687544, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1648, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4470
Iteration:  62%|██████▏   | 1718/2771 [01:55<01:10, 14.92it/s]DLL 2022-02-17 19:05:26.881137 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.5868754386901855  train_perf : 486.0146179199219 

Epoch: 001, Step:  1750, Loss:  2.87971687, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1698, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4520
Iteration:  65%|██████▍   | 1793/2771 [02:00<01:05, 14.91it/s]DLL 2022-02-17 19:05:30.237122 - Training Epoch: 1 Training Iteration: 1750  step_loss : 2.8797168731689453  train_perf : 486.0031433105469 

Epoch: 001, Step:  1800, Loss:  2.48679543, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1748, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4570
DLL 2022-02-17 19:05:33.587850 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.486795425415039  train_perf : 486.00732421875 

Epoch: 001, Step:  1850, Loss:  2.66278696, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1798, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4620
Iteration:  67%|██████▋   | 1868/2771 [02:05<01:00, 14.92it/s]DLL 2022-02-17 19:05:36.942833 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.6627869606018066  train_perf : 485.9924011230469 

Epoch: 001, Step:  1900, Loss:  2.52619433, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1848, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4670
Iteration:  70%|███████   | 1943/2771 [02:10<00:55, 14.92it/s]DLL 2022-02-17 19:05:40.293519 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.5261943340301514  train_perf : 485.9981994628906 

Epoch: 001, Step:  1950, Loss:  2.42174053, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1898, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4720
DLL 2022-02-17 19:05:43.642719 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.4217405319213867  train_perf : 486.0081481933594 

Epoch: 001, Step:  2000, Loss:  2.51995111, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1948, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4770
Iteration:  73%|███████▎  | 2018/2771 [02:15<00:50, 14.92it/s]DLL 2022-02-17 19:05:46.995142 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.519951105117798  train_perf : 486.006103515625 

Epoch: 001, Step:  2050, Loss:  2.33092451, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1998, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4820
Iteration:  76%|███████▌  | 2093/2771 [02:20<00:45, 14.92it/s]DLL 2022-02-17 19:05:50.347287 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.3309245109558105  train_perf : 486.00213623046875 

Epoch: 001, Step:  2100, Loss:  2.97240019, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-17 19:05:53.693300 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.972400188446045  train_perf : 486.0205993652344 

Epoch: 001, Step:  2150, Loss:  2.63720179, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2168/2771 [02:25<00:40, 14.92it/s]DLL 2022-02-17 19:05:57.045512 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.6372017860412598  train_perf : 486.0174560546875 

Epoch: 001, Step:  2200, Loss:  2.54326200, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2243/2771 [02:30<00:35, 14.93it/s]DLL 2022-02-17 19:06:00.395195 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.543262004852295  train_perf : 486.020263671875 

Epoch: 001, Step:  2250, Loss:  2.47799969, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-17 19:06:03.742147 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.477999687194824  train_perf : 486.0342102050781 

Epoch: 001, Step:  2300, Loss:  2.55443621, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▎ | 2318/2771 [02:35<00:30, 14.93it/s]DLL 2022-02-17 19:06:07.085339 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.554436206817627  train_perf : 486.0591735839844 

Epoch: 001, Step:  2350, Loss:  2.35434532, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  86%|████████▋ | 2393/2771 [02:40<00:25, 14.93it/s]DLL 2022-02-17 19:06:10.438233 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.3543453216552734  train_perf : 486.0554504394531 

Epoch: 001, Step:  2400, Loss:  2.42534804, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-17 19:06:13.789280 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.4253480434417725  train_perf : 486.054931640625 

Epoch: 001, Step:  2450, Loss:  2.81392288, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2468/2771 [02:45<00:20, 14.93it/s]DLL 2022-02-17 19:06:17.136809 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.813922882080078  train_perf : 486.0636901855469 

Epoch: 001, Step:  2500, Loss:  2.80445647, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2543/2771 [02:50<00:15, 14.93it/s]DLL 2022-02-17 19:06:20.488869 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.8044564723968506  train_perf : 486.06201171875 

Epoch: 001, Step:  2550, Loss:  2.64154029, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-17 19:06:23.840127 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.641540288925171  train_perf : 486.06231689453125 

Epoch: 001, Step:  2600, Loss:  3.04052973, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  94%|█████████▍| 2618/2771 [02:55<00:10, 14.92it/s]DLL 2022-02-17 19:06:27.187647 - Training Epoch: 1 Training Iteration: 2600  step_loss : 3.040529727935791  train_perf : 486.0679931640625 

Epoch: 001, Step:  2650, Loss:  2.80245662, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2693/2771 [03:00<00:05, 14.92it/s]DLL 2022-02-17 19:06:30.543532 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.8024566173553467  train_perf : 486.060791015625 

Epoch: 001, Step:  2700, Loss:  2.75994515, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-17 19:06:33.894650 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.7599451541900635  train_perf : 486.0606689453125 

Epoch: 001, Step:  2750, Loss:  2.66231632, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2768/2771 [03:05<00:00, 14.92it/s]Iteration: 100%|█████████▉| 2770/2771 [03:06<00:00, 14.88it/s]DLL 2022-02-17 19:06:37.246811 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.66231632232666  train_perf : 486.0593566894531 
DLL 2022-02-17 19:06:38.519889 -  e2e_train_time : 584.071756362915  training_sequences_per_second : 486.061767578125  final_loss : 2.65012526512146 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-17 19:06:43.218671: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 19:08:08.289255: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:08:09.339747: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:08:09.339766: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:08:09.339772: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:08:09.339774: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:08:09.339776: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:08:09.339778: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:30<31:36, 90.31s/it]Iteration:  14%|█▎        | 3/22 [01:36<20:17, 64.08s/it]Iteration:  23%|██▎       | 5/22 [01:42<12:57, 45.75s/it]Iteration:  32%|███▏      | 7/22 [01:48<08:14, 32.94s/it]Iteration:  41%|████      | 9/22 [01:54<05:11, 23.99s/it]Iteration:  50%|█████     | 11/22 [02:00<03:14, 17.73s/it]Iteration:  59%|█████▉    | 13/22 [02:06<02:00, 13.35s/it]Iteration:  68%|██████▊   | 15/22 [02:13<01:12, 10.30s/it]Iteration:  77%|███████▋  | 17/22 [02:19<00:40,  8.17s/it]Iteration:  86%|████████▋ | 19/22 [02:26<00:20,  6.69s/it]Iteration:  95%|█████████▌| 21/22 [02:32<00:05,  5.68s/it]2022-02-17 19:09:15.637398: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.68s/it]2022-02-17 19:10:25.387009: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:10:26.440992: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:10:26.441011: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:10:26.441017: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:10:26.441020: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:10:26.441021: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:10:26.441023: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:45<00:00, 25.71s/it]Iteration: 100%|██████████| 22/22 [03:45<00:00, 10.23s/it]{"exact_match": 11.83538315988647, "f1": 16.069155686371868}

Epoch: 001 Results: {"exact_match": 11.83538315988647, "f1": 16.069155686371868}

**EVAL SUMMARY** - Epoch: 001,  EM: 11.835, F1: 16.069, Infer_Perf: 1051 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 443.159 ms, 90%: 442.587 ms, 95%: 442.886 ms, 99%: 442.886 ms
DLL 2022-02-17 19:10:44.277295 -  inference_sequences_per_second : 1050.9373779296875  e2e_inference_time : 239.6040906906128 
**RESULTS SUMMARY** - EM: 11.835, F1: 16.069, Train_Time:  584 s, Train_Perf:  486 seq/s, Infer_Perf: 1051 seq/s

DLL 2022-02-17 19:10:44.278114 -  exact_match : 11.83538315988647  F1 : 16.069155686371868 
====================================  END results/models/base/checkpoints/ckpt-2497  ====================================
==================================== START results/models/base/checkpoints/ckpt-2997 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/base/checkpoints/ckpt-2997 at step 2996
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/base/checkpoints/discriminator/config.json
Model weights saved in results/models/base/checkpoints/discriminator/tf_model.h5: {'electra', 'discriminator_predictions'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=test_results/models/base/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-17 19:10:50.342143: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-17 19:10:51.186780: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-17 19:10:51.203084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.203530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 19:10:51.203545: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 19:10:51.204942: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 19:10:51.205572: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 19:10:51.205709: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 19:10:51.207115: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 19:10:51.207444: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 19:10:51.207521: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 19:10:51.207562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.208000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.208411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 19:10:51.213430: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-17 19:10:51.213842: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f75c0000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-17 19:10:51.213854: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-17 19:10:51.358704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.359214: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f74c8000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-17 19:10:51.359227: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-17 19:10:51.359341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.359760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 19:10:51.359780: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 19:10:51.359807: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 19:10:51.359822: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 19:10:51.359837: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 19:10:51.359850: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 19:10:51.359864: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 19:10:51.359876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 19:10:51.359913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.360343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.360749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 19:10:51.360768: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 19:10:51.525560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-17 19:10:51.525587: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-17 19:10:51.525593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-17 19:10:51.525713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.526170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:10:51.526595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-17 19:10:51.186057 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: test_results/models/base/checkpoints/discriminator
loading configuration file test_results/models/base/checkpoints/discriminator/config.json
loading weights file test_results/models/base/checkpoints/discriminator/tf_model.h5
2022-02-17 19:10:51.591642: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-17 19:10:51.631737: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at test_results/models/base/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at test_results/models/base/checkpoints/discriminator and are newly initialized: ['start_logits', 'end_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 37%|███▋      | 164/442 [00:05<00:08, 32.68it/s] 69%|██████▊   | 303/442 [00:10<00:04, 31.04it/s]100%|██████████| 442/442 [00:15<00:00, 29.91it/s]100%|██████████| 442/442 [00:15<00:00, 29.35it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 27.00it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-17 19:11:49.147047: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-17 19:11:50.027338: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 19:11:50.053820: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 19:11:50.684220: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-17 19:11:50.972444: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-17 19:14:57.794118: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:01.019067: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:15:01.019087: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:15:01.019094: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:15:01.019096: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:15:01.019098: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:15:01.019100: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:15:01.032922: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-17 19:15:03.034110: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:03.178096: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:15:03.178115: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:15:03.178121: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:15:03.178123: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:15:03.178125: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:15:03.178127: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:15:03.236199: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:03.422713: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:15:03.422732: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:15:03.422739: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:15:03.422741: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:15:03.422743: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:15:03.422745: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:15:03.652102: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:04.423512: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:15:04.423530: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:15:04.423536: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:15:04.423538: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:15:04.423540: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:15:04.423542: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.27780342, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:24<156:59:52, 204.04s/it]2022-02-17 19:15:07.528672: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 19:15:08.848491: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:12.100271: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:15:12.100290: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:15:12.100297: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:15:12.100299: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:15:12.100301: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:15:12.100303: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:15:14.064146: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:14.151175: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:14.445826: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:15.468700: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:15.594518: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:15:15.594536: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:15:15.594557: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:15:15.594560: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:15:15.594561: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:15:15.594579: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:34<112:20:34, 146.06s/it]DLL 2022-02-17 19:15:04.866242 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.277803421020508  train_perf : 0.1570604294538498 

Epoch: 000, Step:    50, Loss:  5.06271124, Perf:  470, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:39<76:29:47, 102.26s/it]DLL 2022-02-17 19:15:18.901130 - Training Epoch: 0 Training Iteration: 50  step_loss : 5.062711238861084  train_perf : 470.1543884277344 

Epoch: 000, Step:   100, Loss:  4.21477509, Perf:  483, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-17 19:15:22.190620 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.214775085449219  train_perf : 482.8690185546875 

Epoch: 000, Step:   150, Loss:  3.55293536, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 154/2771 [03:44<52:03:02, 71.60s/it]DLL 2022-02-17 19:15:25.493549 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.5529353618621826  train_perf : 486.58734130859375 

Epoch: 000, Step:   200, Loss:  3.71851897, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 230/2771 [03:49<35:23:29, 50.14s/it]DLL 2022-02-17 19:15:28.789760 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.7185189723968506  train_perf : 488.6261291503906 

Epoch: 000, Step:   250, Loss:  3.32122231, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-17 19:15:33.853815: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:15:33.973391: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:15:33.973445: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:15:33.973454: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:15:33.973458: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:15:33.973462: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:15:33.973465: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 19:15:32.084539 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.3212223052978516  train_perf : 489.89501953125 

Epoch: 000, Step:   300, Loss:  3.18776298, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 304/2771 [03:54<24:03:59, 35.12s/it]DLL 2022-02-17 19:15:35.540380 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.187762975692749  train_perf : 489.40374755859375 

Epoch: 000, Step:   350, Loss:  3.15376949, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 380/2771 [03:59<16:20:26, 24.60s/it]DLL 2022-02-17 19:15:38.845736 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.1537694931030273  train_perf : 489.9814453125 

Epoch: 000, Step:   400, Loss:  3.22410107, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-17 19:15:42.161693 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.2241010665893555  train_perf : 490.2203063964844 

Epoch: 000, Step:   450, Loss:  2.62505317, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 456/2771 [04:04<11:05:15, 17.24s/it]DLL 2022-02-17 19:15:45.483772 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.6250531673431396  train_perf : 490.2942810058594 

Epoch: 000, Step:   500, Loss:  2.92078924, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 532/2771 [04:10<7:31:08, 12.09s/it] DLL 2022-02-17 19:15:48.795575 - Training Epoch: 0 Training Iteration: 500  step_loss : 2.9207892417907715  train_perf : 490.510986328125 

Epoch: 000, Step:   550, Loss:  3.19278622, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-17 19:15:52.117615 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.19278621673584  train_perf : 490.55377197265625 

Epoch: 000, Step:   600, Loss:  2.74256706, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 608/2771 [04:15<5:05:47,  8.48s/it]DLL 2022-02-17 19:15:55.436182 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.7425670623779297  train_perf : 490.6354064941406 

Epoch: 000, Step:   650, Loss:  3.24580908, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 684/2771 [04:20<3:27:13,  5.96s/it]DLL 2022-02-17 19:15:58.756198 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.2458090782165527  train_perf : 490.67193603515625 

Epoch: 000, Step:   700, Loss:  3.02990818, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
DLL 2022-02-17 19:16:02.075273 - Training Epoch: 0 Training Iteration: 700  step_loss : 3.0299081802368164  train_perf : 490.71380615234375 

Epoch: 000, Step:   750, Loss:  2.77744579, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:751
Iteration:  27%|██▋       | 760/2771 [04:25<2:20:26,  4.19s/it]DLL 2022-02-17 19:16:05.394850 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.7774457931518555  train_perf : 490.73992919921875 

Epoch: 000, Step:   800, Loss:  2.93735838, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:801
Iteration:  30%|███       | 836/2771 [04:30<1:35:14,  2.95s/it]DLL 2022-02-17 19:16:08.712867 - Training Epoch: 0 Training Iteration: 800  step_loss : 2.9373583793640137  train_perf : 490.780517578125 

Epoch: 000, Step:   850, Loss:  3.54871249, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:851
DLL 2022-02-17 19:16:12.031873 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.5487124919891357  train_perf : 490.8191833496094 

Epoch: 000, Step:   900, Loss:  2.54256582, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:901
Iteration:  33%|███▎      | 912/2771 [04:35<1:04:39,  2.09s/it]DLL 2022-02-17 19:16:15.356345 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.5425658226013184  train_perf : 490.7998962402344 

Epoch: 000, Step:   950, Loss:  3.35216999, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:951
Iteration:  36%|███▌      | 988/2771 [04:40<44:00,  1.48s/it]  DLL 2022-02-17 19:16:18.683527 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.352169990539551  train_perf : 490.7645568847656 

Epoch: 000, Step:  1000, Loss:  3.19619441, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1001, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1001
DLL 2022-02-17 19:16:22.006432 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.1961944103240967  train_perf : 490.7654724121094 

Epoch: 000, Step:  1050, Loss:  3.05833983, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1051, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1051
Iteration:  38%|███▊      | 1064/2771 [04:45<30:03,  1.06s/it]DLL 2022-02-17 19:16:25.330011 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.058339834213257  train_perf : 490.7554016113281 

Epoch: 000, Step:  1100, Loss:  3.55793023, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1101
Iteration:  41%|████      | 1139/2771 [04:50<20:39,  1.32it/s]DLL 2022-02-17 19:16:28.663401 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.5579302310943604  train_perf : 490.70599365234375 

Epoch: 000, Step:  1150, Loss:  3.47775412, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1151
DLL 2022-02-17 19:16:32.003374 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.4777541160583496  train_perf : 490.5855712890625 

Epoch: 000, Step:  1200, Loss:  3.18042088, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1201
Iteration:  44%|████▍     | 1214/2771 [04:55<14:19,  1.81it/s]DLL 2022-02-17 19:16:35.337432 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.1804208755493164  train_perf : 490.510498046875 

Epoch: 000, Step:  1250, Loss:  3.51851034, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1251
Iteration:  47%|████▋     | 1289/2771 [05:00<10:02,  2.46it/s]DLL 2022-02-17 19:16:38.669993 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.518510341644287  train_perf : 490.4559326171875 

Epoch: 000, Step:  1300, Loss:  3.15754986, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1301
DLL 2022-02-17 19:16:42.004693 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.1575498580932617  train_perf : 490.40087890625 

Epoch: 000, Step:  1350, Loss:  2.93189573, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1351
Iteration:  49%|████▉     | 1365/2771 [05:05<07:07,  3.29it/s]DLL 2022-02-17 19:16:45.336136 - Training Epoch: 0 Training Iteration: 1350  step_loss : 2.9318957328796387  train_perf : 490.3617248535156 

Epoch: 000, Step:  1400, Loss:  3.16951513, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1401
Iteration:  52%|█████▏    | 1440/2771 [05:10<05:10,  4.29it/s]DLL 2022-02-17 19:16:48.669557 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.1695151329040527  train_perf : 490.31494140625 

Epoch: 000, Step:  1450, Loss:  3.09213686, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1451
DLL 2022-02-17 19:16:52.007622 - Training Epoch: 0 Training Iteration: 1450  step_loss : 3.092136859893799  train_perf : 490.25714111328125 

Epoch: 000, Step:  1500, Loss:  3.42293334, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1501
Iteration:  55%|█████▍    | 1515/2771 [05:15<03:50,  5.46it/s]DLL 2022-02-17 19:16:55.341759 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.422933340072632  train_perf : 490.218017578125 

Epoch: 000, Step:  1550, Loss:  3.30077958, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1551
Iteration:  57%|█████▋    | 1590/2771 [05:20<02:55,  6.75it/s]DLL 2022-02-17 19:16:58.681244 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.3007795810699463  train_perf : 490.1468811035156 

Epoch: 000, Step:  1600, Loss:  3.12677884, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1601
DLL 2022-02-17 19:17:02.018661 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.1267788410186768  train_perf : 490.0935363769531 

Epoch: 000, Step:  1650, Loss:  3.10123110, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1651
Iteration:  60%|██████    | 1665/2771 [05:25<02:16,  8.08it/s]DLL 2022-02-17 19:17:05.357444 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.101231098175049  train_perf : 490.0376892089844 

Epoch: 000, Step:  1700, Loss:  3.07972574, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1701
Iteration:  63%|██████▎   | 1740/2771 [05:30<01:50,  9.37it/s]DLL 2022-02-17 19:17:08.699074 - Training Epoch: 0 Training Iteration: 1700  step_loss : 3.079725742340088  train_perf : 489.9726257324219 

Epoch: 000, Step:  1750, Loss:  3.17650843, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1751
DLL 2022-02-17 19:17:12.039664 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.1765084266662598  train_perf : 489.9184875488281 

Epoch: 000, Step:  1800, Loss:  2.98621750, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1801
Iteration:  65%|██████▌   | 1815/2771 [05:35<01:30, 10.56it/s]DLL 2022-02-17 19:17:15.376124 - Training Epoch: 0 Training Iteration: 1800  step_loss : 2.986217498779297  train_perf : 489.880126953125 

Epoch: 000, Step:  1850, Loss:  3.06342673, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1851
Iteration:  68%|██████▊   | 1890/2771 [05:40<01:16, 11.58it/s]DLL 2022-02-17 19:17:18.714792 - Training Epoch: 0 Training Iteration: 1850  step_loss : 3.0634267330169678  train_perf : 489.83251953125 

Epoch: 000, Step:  1900, Loss:  2.74336338, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1901
DLL 2022-02-17 19:17:22.049656 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.743363380432129  train_perf : 489.8045349121094 

Epoch: 000, Step:  1950, Loss:  2.81075764, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1951
Iteration:  71%|███████   | 1965/2771 [05:45<01:04, 12.43it/s]2022-02-17 19:17:28.666186: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:17:28.783642: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:17:28.783662: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:17:28.783668: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:17:28.783670: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:17:28.783672: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:17:28.783674: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 19:17:25.390836 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.810757637023926  train_perf : 489.7603454589844 

Epoch: 000, Step:  2000, Loss:  3.12792873, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2001
2022-02-17 19:17:30.560366: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:17:30.675533: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:17:30.675551: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:17:30.675556: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:17:30.675558: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:17:30.675560: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:17:30.675562: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:  74%|███████▎  | 2040/2771 [05:50<00:56, 12.88it/s]DLL 2022-02-17 19:17:28.885449 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.1279287338256836  train_perf : 489.5379638671875 

Epoch: 000, Step:  2050, Loss:  2.89278698, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=25, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2050
DLL 2022-02-17 19:17:32.386662 - Training Epoch: 0 Training Iteration: 2050  step_loss : 2.892786979675293  train_perf : 489.28277587890625 

Epoch: 000, Step:  2100, Loss:  3.17207241, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=75, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2100
Iteration:  76%|███████▋  | 2115/2771 [05:55<00:48, 13.44it/s]DLL 2022-02-17 19:17:35.733081 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.172072410583496  train_perf : 489.2325744628906 

Epoch: 000, Step:  2150, Loss:  2.97102404, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=125, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2150
Iteration:  79%|███████▉  | 2190/2771 [06:00<00:41, 13.87it/s]DLL 2022-02-17 19:17:39.070963 - Training Epoch: 0 Training Iteration: 2150  step_loss : 2.9710240364074707  train_perf : 489.2201843261719 

Epoch: 000, Step:  2200, Loss:  3.01602077, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=175, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-17 19:17:42.413496 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.0160207748413086  train_perf : 489.1881408691406 

Epoch: 000, Step:  2250, Loss:  2.80229950, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=225, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  82%|████████▏ | 2265/2771 [06:05<00:35, 14.18it/s]DLL 2022-02-17 19:17:45.757259 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.8022994995117188  train_perf : 489.14825439453125 

Epoch: 000, Step:  2300, Loss:  3.12714839, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=275, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  84%|████████▍ | 2340/2771 [06:10<00:29, 14.41it/s]DLL 2022-02-17 19:17:49.097390 - Training Epoch: 0 Training Iteration: 2300  step_loss : 3.127148389816284  train_perf : 489.1232604980469 

Epoch: 000, Step:  2350, Loss:  2.64295387, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=325, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-17 19:17:52.434796 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.642953872680664  train_perf : 489.10601806640625 

Epoch: 000, Step:  2400, Loss:  2.90077734, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=375, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2415/2771 [06:15<00:24, 14.58it/s]DLL 2022-02-17 19:17:55.770595 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.9007773399353027  train_perf : 489.0948181152344 

Epoch: 000, Step:  2450, Loss:  3.29814339, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=425, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2490/2771 [06:20<00:19, 14.70it/s]DLL 2022-02-17 19:17:59.103887 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.2981433868408203  train_perf : 489.0927734375 

Epoch: 000, Step:  2500, Loss:  3.24416566, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=475, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-17 19:18:02.443500 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.2441656589508057  train_perf : 489.069580078125 

Epoch: 000, Step:  2550, Loss:  2.99611211, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=525, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  93%|█████████▎| 2565/2771 [06:25<00:13, 14.77it/s]DLL 2022-02-17 19:18:05.789530 - Training Epoch: 0 Training Iteration: 2550  step_loss : 2.996112108230591  train_perf : 489.0332336425781 

Epoch: 000, Step:  2600, Loss:  2.98489070, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=575, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2640/2771 [06:30<00:08, 14.83it/s]DLL 2022-02-17 19:18:09.126243 - Training Epoch: 0 Training Iteration: 2600  step_loss : 2.9848906993865967  train_perf : 489.01837158203125 

Epoch: 000, Step:  2650, Loss:  3.00047445, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=625, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-17 19:18:12.470741 - Training Epoch: 0 Training Iteration: 2650  step_loss : 3.000474452972412  train_perf : 488.9878845214844 

Epoch: 000, Step:  2700, Loss:  3.04290581, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=675, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
Iteration:  98%|█████████▊| 2715/2771 [06:35<00:03, 14.87it/s]DLL 2022-02-17 19:18:15.809290 - Training Epoch: 0 Training Iteration: 2700  step_loss : 3.042905807495117  train_perf : 488.975341796875 

Epoch: 000, Step:  2750, Loss:  3.10370159, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=725, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:39<00:00,  6.93it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-17 19:18:19.157384 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.103701591491699  train_perf : 488.93939208984375 
DLL 2022-02-17 19:18:20.427684 -  e2e_train_time : 399.6013698577881  training_sequences_per_second : 488.9324951171875  final_loss : 3.203296422958374 

Epoch: 001, Step:     0, Loss:  2.60932636, Perf:  460, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=745, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-17 19:18:24.259657 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.6093263626098633  train_perf : 459.7741394042969 

Epoch: 001, Step:    50, Loss:  2.47383237, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=795, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
Iteration:   3%|▎         | 71/2771 [00:05<03:11, 14.10it/s]DLL 2022-02-17 19:18:27.596816 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.473832368850708  train_perf : 487.531494140625 

Epoch: 001, Step:   100, Loss:  2.44107819, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=845, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 146/2771 [00:10<03:02, 14.36it/s]DLL 2022-02-17 19:18:30.932010 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.4410781860351562  train_perf : 487.9784851074219 

Epoch: 001, Step:   150, Loss:  2.58297920, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=895, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-17 19:18:34.267690 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.582979202270508  train_perf : 488.13885498046875 

Epoch: 001, Step:   200, Loss:  2.47302151, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=945, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 221/2771 [00:15<02:55, 14.54it/s]DLL 2022-02-17 19:18:37.605712 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.4730215072631836  train_perf : 488.18206787109375 

Epoch: 001, Step:   250, Loss:  2.49012184, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=995, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 297/2771 [00:20<02:48, 14.68it/s]DLL 2022-02-17 19:18:40.935195 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.490121841430664  train_perf : 488.4384460449219 

Epoch: 001, Step:   300, Loss:  2.54851389, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1045, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-17 19:18:44.267180 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.548513889312744  train_perf : 488.5158996582031 

Epoch: 001, Step:   350, Loss:  2.47297740, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1095, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  13%|█▎        | 372/2771 [00:25<02:42, 14.77it/s]DLL 2022-02-17 19:18:47.602005 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.47297739982605  train_perf : 488.5250549316406 

Epoch: 001, Step:   400, Loss:  2.11945653, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1145, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 448/2771 [00:30<02:36, 14.84it/s]DLL 2022-02-17 19:18:50.936960 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.1194565296173096  train_perf : 488.53369140625 

Epoch: 001, Step:   450, Loss:  2.14219570, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1195, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-17 19:18:54.267085 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.142195701599121  train_perf : 488.6098327636719 

Epoch: 001, Step:   500, Loss:  2.57539606, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1245, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▉        | 523/2771 [00:35<02:31, 14.89it/s]DLL 2022-02-17 19:18:57.604160 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.5753960609436035  train_perf : 488.5928649902344 

Epoch: 001, Step:   550, Loss:  2.30433011, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1295, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
Iteration:  22%|██▏       | 599/2771 [00:40<02:25, 14.92it/s]DLL 2022-02-17 19:19:00.932959 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.3043301105499268  train_perf : 488.69482421875 

Epoch: 001, Step:   600, Loss:  2.13865542, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1345, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
DLL 2022-02-17 19:19:04.267350 - Training Epoch: 1 Training Iteration: 600  step_loss : 2.138655424118042  train_perf : 488.7177734375 

Epoch: 001, Step:   650, Loss:  2.72916818, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1395, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 674/2771 [00:45<02:20, 14.94it/s]DLL 2022-02-17 19:19:07.602507 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.729168176651001  train_perf : 488.7271423339844 

Epoch: 001, Step:   700, Loss:  2.44937253, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1445, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
Iteration:  27%|██▋       | 749/2771 [00:50<02:15, 14.95it/s]DLL 2022-02-17 19:19:10.948746 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.4493725299835205  train_perf : 488.60498046875 

Epoch: 001, Step:   750, Loss:  2.19672918, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1495, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3520
DLL 2022-02-17 19:19:14.290471 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.1967291831970215  train_perf : 488.5409851074219 

Epoch: 001, Step:   800, Loss:  2.91794705, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1545, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3570
Iteration:  30%|██▉       | 824/2771 [00:55<02:10, 14.95it/s]DLL 2022-02-17 19:19:17.630092 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.9179470539093018  train_perf : 488.51116943359375 

Epoch: 001, Step:   850, Loss:  3.10081291, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1595, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3620
Iteration:  32%|███▏      | 899/2771 [01:00<02:05, 14.95it/s]DLL 2022-02-17 19:19:20.973150 - Training Epoch: 1 Training Iteration: 850  step_loss : 3.1008129119873047  train_perf : 488.45166015625 

Epoch: 001, Step:   900, Loss:  2.67457008, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1645, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3670
DLL 2022-02-17 19:19:24.313261 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.674570083618164  train_perf : 488.4278869628906 

Epoch: 001, Step:   950, Loss:  2.69512916, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1695, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3720
Iteration:  35%|███▌      | 974/2771 [01:05<02:00, 14.95it/s]DLL 2022-02-17 19:19:27.657685 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.695129156112671  train_perf : 488.3620300292969 

Epoch: 001, Step:  1000, Loss:  2.79910564, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1745, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3770
Iteration:  38%|███▊      | 1049/2771 [01:10<01:55, 14.95it/s]DLL 2022-02-17 19:19:30.998025 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.799105644226074  train_perf : 488.3382568359375 

Epoch: 001, Step:  1050, Loss:  2.99828839, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1795, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3820
DLL 2022-02-17 19:19:34.343899 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.99828839302063  train_perf : 488.2792053222656 

Epoch: 001, Step:  1100, Loss:  3.54958344, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1845, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3870
Iteration:  41%|████      | 1124/2771 [01:15<01:50, 14.95it/s]DLL 2022-02-17 19:19:37.687074 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.5495834350585938  train_perf : 488.23876953125 

Epoch: 001, Step:  1150, Loss:  3.05270290, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1895, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3920
Iteration:  43%|████▎     | 1199/2771 [01:20<01:45, 14.96it/s]DLL 2022-02-17 19:19:41.028592 - Training Epoch: 1 Training Iteration: 1150  step_loss : 3.0527029037475586  train_perf : 488.21270751953125 

Epoch: 001, Step:  1200, Loss:  2.76175904, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1945, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3970
DLL 2022-02-17 19:19:44.366138 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.761759042739868  train_perf : 488.21319580078125 

Epoch: 001, Step:  1250, Loss:  3.04296827, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1995, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4020
Iteration:  46%|████▌     | 1274/2771 [01:25<01:39, 14.97it/s]DLL 2022-02-17 19:19:47.704220 - Training Epoch: 1 Training Iteration: 1250  step_loss : 3.042968273162842  train_perf : 488.2207336425781 

Epoch: 001, Step:  1300, Loss:  2.68888903, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=45, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4070
Iteration:  49%|████▊     | 1349/2771 [01:30<01:34, 14.97it/s]DLL 2022-02-17 19:19:51.038673 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.6888890266418457  train_perf : 488.2454833984375 

Epoch: 001, Step:  1350, Loss:  2.64800692, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=46, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4119
DLL 2022-02-17 19:19:54.379422 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.6480069160461426  train_perf : 488.23541259765625 

Epoch: 001, Step:  1400, Loss:  2.68646646, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=96, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4169
Iteration:  51%|█████▏    | 1424/2771 [01:35<01:29, 14.97it/s]DLL 2022-02-17 19:19:57.718493 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.6864664554595947  train_perf : 488.2184143066406 

Epoch: 001, Step:  1450, Loss:  2.38550949, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=146, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4219
Iteration:  54%|█████▍    | 1499/2771 [01:40<01:25, 14.96it/s]DLL 2022-02-17 19:20:01.065542 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.385509490966797  train_perf : 488.1679382324219 

Epoch: 001, Step:  1500, Loss:  3.54566002, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=196, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4269
DLL 2022-02-17 19:20:04.405591 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.5456600189208984  train_perf : 488.1565246582031 

Epoch: 001, Step:  1550, Loss:  2.65482116, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=246, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4319
Iteration:  57%|█████▋    | 1574/2771 [01:45<01:19, 14.96it/s]DLL 2022-02-17 19:20:07.747815 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.6548211574554443  train_perf : 488.1341857910156 

Epoch: 001, Step:  1600, Loss:  2.69102764, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=296, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4369
Iteration:  60%|█████▉    | 1649/2771 [01:50<01:14, 14.97it/s]DLL 2022-02-17 19:20:11.088316 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.6910276412963867  train_perf : 488.1217956542969 

Epoch: 001, Step:  1650, Loss:  2.63610601, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=346, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4419
DLL 2022-02-17 19:20:14.428418 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.636106014251709  train_perf : 488.11529541015625 

Epoch: 001, Step:  1700, Loss:  2.66164804, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=396, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4469
Iteration:  62%|██████▏   | 1724/2771 [01:55<01:09, 14.97it/s]DLL 2022-02-17 19:20:17.764037 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.6616480350494385  train_perf : 488.1226501464844 

Epoch: 001, Step:  1750, Loss:  3.12503552, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=446, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4519
Iteration:  65%|██████▍   | 1799/2771 [02:00<01:04, 14.97it/s]DLL 2022-02-17 19:20:21.102910 - Training Epoch: 1 Training Iteration: 1750  step_loss : 3.125035524368286  train_perf : 488.1223449707031 

Epoch: 001, Step:  1800, Loss:  2.42836666, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=496, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4569
DLL 2022-02-17 19:20:24.439554 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.4283666610717773  train_perf : 488.1293029785156 

Epoch: 001, Step:  1850, Loss:  2.79774785, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=546, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4619
Iteration:  68%|██████▊   | 1874/2771 [02:05<00:59, 14.97it/s]DLL 2022-02-17 19:20:27.779636 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.797747850418091  train_perf : 488.1241455078125 

Epoch: 001, Step:  1900, Loss:  2.55158424, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=596, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4669
Iteration:  70%|███████   | 1949/2771 [02:10<00:54, 14.98it/s]DLL 2022-02-17 19:20:31.119863 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.551584243774414  train_perf : 488.1170959472656 

Epoch: 001, Step:  1950, Loss:  2.54055762, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=646, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4719
DLL 2022-02-17 19:20:34.455176 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.540557622909546  train_perf : 488.1201171875 

Epoch: 001, Step:  2000, Loss:  2.68072009, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=696, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4769
Iteration:  73%|███████▎  | 2024/2771 [02:15<00:49, 14.97it/s]DLL 2022-02-17 19:20:37.796888 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.680720090866089  train_perf : 488.10577392578125 

Epoch: 001, Step:  2050, Loss:  2.39062452, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=746, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
Iteration:  76%|███████▌  | 2099/2771 [02:20<00:44, 14.97it/s]DLL 2022-02-17 19:20:41.135476 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.390624523162842  train_perf : 488.1051330566406 

Epoch: 001, Step:  2100, Loss:  3.04768205, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=796, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-17 19:20:44.480178 - Training Epoch: 1 Training Iteration: 2100  step_loss : 3.047682046890259  train_perf : 488.0841369628906 

Epoch: 001, Step:  2150, Loss:  2.57882571, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=846, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2174/2771 [02:25<00:39, 14.97it/s]DLL 2022-02-17 19:20:47.818797 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.5788257122039795  train_perf : 488.0820007324219 

Epoch: 001, Step:  2200, Loss:  2.58201122, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=896, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2249/2771 [02:30<00:34, 14.97it/s]DLL 2022-02-17 19:20:51.162430 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.5820112228393555  train_perf : 488.0668029785156 

Epoch: 001, Step:  2250, Loss:  2.54018354, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=946, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-17 19:20:54.498608 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.5401835441589355  train_perf : 488.07135009765625 

Epoch: 001, Step:  2300, Loss:  2.51952553, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=996, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▍ | 2324/2771 [02:35<00:29, 14.98it/s]DLL 2022-02-17 19:20:57.834323 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.5195255279541016  train_perf : 488.0762634277344 

Epoch: 001, Step:  2350, Loss:  2.34829950, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1046, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  87%|████████▋ | 2399/2771 [02:40<00:24, 14.98it/s]DLL 2022-02-17 19:21:01.170564 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.348299503326416  train_perf : 488.0809326171875 

Epoch: 001, Step:  2400, Loss:  2.54650569, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1096, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-17 19:21:04.507765 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.5465056896209717  train_perf : 488.0815734863281 

Epoch: 001, Step:  2450, Loss:  2.87255955, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1146, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2474/2771 [02:45<00:19, 14.98it/s]DLL 2022-02-17 19:21:07.846495 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.8725595474243164  train_perf : 488.0796203613281 

Epoch: 001, Step:  2500, Loss:  2.88691616, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1196, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2549/2771 [02:50<00:14, 14.97it/s]DLL 2022-02-17 19:21:11.185779 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.886916160583496  train_perf : 488.0762634277344 

Epoch: 001, Step:  2550, Loss:  2.73101759, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1246, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-17 19:21:14.529642 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.731017589569092  train_perf : 488.0606689453125 

Epoch: 001, Step:  2600, Loss:  2.95245504, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1296, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▍| 2624/2771 [02:55<00:09, 14.97it/s]DLL 2022-02-17 19:21:17.866300 - Training Epoch: 1 Training Iteration: 2600  step_loss : 2.9524550437927246  train_perf : 488.0658264160156 

Epoch: 001, Step:  2650, Loss:  2.80649114, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1346, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2699/2771 [03:00<00:04, 14.97it/s]DLL 2022-02-17 19:21:21.209735 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.8064911365509033  train_perf : 488.0514831542969 

Epoch: 001, Step:  2700, Loss:  2.75167894, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1396, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-17 19:21:24.547038 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.751678943634033  train_perf : 488.05181884765625 

Epoch: 001, Step:  2750, Loss:  2.64558053, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1446, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.95it/s]DLL 2022-02-17 19:21:27.891871 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.645580530166626  train_perf : 488.0328063964844 
DLL 2022-02-17 19:21:29.161206 -  e2e_train_time : 584.865110874176  training_sequences_per_second : 488.03289794921875  final_loss : 2.7031259536743164 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-17 19:21:33.557080: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 19:23:00.077967: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:23:01.135944: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:23:01.135975: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:23:01.135981: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:23:01.135983: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:23:01.135985: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:23:01.135988: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:31<32:07, 91.76s/it]Iteration:  14%|█▎        | 3/22 [01:37<20:36, 65.08s/it]Iteration:  23%|██▎       | 5/22 [01:43<13:09, 46.44s/it]Iteration:  32%|███▏      | 7/22 [01:49<08:21, 33.41s/it]Iteration:  41%|████      | 9/22 [01:55<05:16, 24.32s/it]Iteration:  50%|█████     | 11/22 [02:01<03:17, 17.96s/it]Iteration:  59%|█████▉    | 13/22 [02:08<02:01, 13.52s/it]Iteration:  68%|██████▊   | 15/22 [02:14<01:13, 10.43s/it]Iteration:  77%|███████▋  | 17/22 [02:21<00:41,  8.28s/it]Iteration:  86%|████████▋ | 19/22 [02:27<00:20,  6.79s/it]Iteration:  95%|█████████▌| 21/22 [02:34<00:05,  5.76s/it]2022-02-17 19:24:07.613912: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.76s/it]2022-02-17 19:25:17.507548: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:25:18.559036: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:25:18.559055: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:25:18.559061: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:25:18.559063: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:25:18.559065: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:25:18.559067: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:46<00:00, 25.80s/it]Iteration: 100%|██████████| 22/22 [03:46<00:00, 10.31s/it]{"exact_match": 8.789025543992432, "f1": 11.876547870407299}

Epoch: 001 Results: {"exact_match": 8.789025543992432, "f1": 11.876547870407299}

**EVAL SUMMARY** - Epoch: 001,  EM:  8.789, F1: 11.877, Infer_Perf: 1053 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 442.471 ms, 90%: 441.880 ms, 95%: 442.164 ms, 99%: 442.164 ms
DLL 2022-02-17 19:25:35.946465 -  inference_sequences_per_second : 1052.5660400390625  e2e_inference_time : 241.07465982437134 
**RESULTS SUMMARY** - EM:  8.789, F1: 11.877, Train_Time:  585 s, Train_Perf:  488 seq/s, Infer_Perf: 1053 seq/s

DLL 2022-02-17 19:25:35.947116 -  exact_match : 8.789025543992432  F1 : 11.876547870407299 
====================================  END results/models/base/checkpoints/ckpt-2997  ====================================
==================================== START results/models/base/checkpoints/ckpt-3497 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/base/checkpoints/ckpt-3497 at step 3496
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/base/checkpoints/discriminator/config.json
Model weights saved in results/models/base/checkpoints/discriminator/tf_model.h5: {'electra', 'discriminator_predictions'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=test_results/models/base/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-17 19:25:41.932036: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-17 19:25:42.768747: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-17 19:25:42.784582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:42.785044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 19:25:42.785077: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 19:25:42.786541: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 19:25:42.787165: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 19:25:42.787326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 19:25:42.788732: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 19:25:42.789086: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 19:25:42.789179: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 19:25:42.789222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:42.789718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:42.790142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 19:25:42.795179: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-17 19:25:42.795583: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe834000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-17 19:25:42.795595: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-17 19:25:42.935417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:42.935923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe738000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-17 19:25:42.935936: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-17 19:25:42.936049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:42.936466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-17 19:25:42.936484: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 19:25:42.936509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-17 19:25:42.936520: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-17 19:25:42.936529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-17 19:25:42.936538: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-17 19:25:42.936548: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-17 19:25:42.936558: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 19:25:42.936587: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:42.937007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:42.937406: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-17 19:25:42.937422: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-17 19:25:43.103175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-17 19:25:43.103201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-17 19:25:43.103205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-17 19:25:43.103320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:43.103771: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-17 19:25:43.104191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-17 19:25:42.768165 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: test_results/models/base/checkpoints/discriminator
loading configuration file test_results/models/base/checkpoints/discriminator/config.json
loading weights file test_results/models/base/checkpoints/discriminator/tf_model.h5
2022-02-17 19:25:43.171459: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-17 19:25:43.210928: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at test_results/models/base/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at test_results/models/base/checkpoints/discriminator and are newly initialized: ['end_logits', 'start_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 36%|███▋      | 161/442 [00:05<00:08, 32.14it/s] 67%|██████▋   | 296/442 [00:10<00:04, 30.39it/s] 98%|█████████▊| 434/442 [00:15<00:00, 29.48it/s]100%|██████████| 442/442 [00:15<00:00, 28.89it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 26.52it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-17 19:26:40.201306: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-17 19:26:41.069571: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 19:26:41.095302: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-17 19:26:41.716886: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-17 19:26:42.005598: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-17 19:29:50.133544: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:29:53.475501: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:29:53.475520: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:29:53.475526: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:29:53.475528: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:29:53.475530: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:29:53.475532: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:29:53.489076: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-17 19:29:55.460173: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:29:55.602165: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:29:55.602183: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:29:55.602188: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:29:55.602190: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:29:55.602192: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:29:55.602193: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:29:55.662543: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:29:55.846448: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:29:55.846466: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:29:55.846473: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:29:55.846475: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:29:55.846477: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:29:55.846478: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:29:56.065264: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:29:56.833666: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:29:56.833684: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:29:56.833691: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:29:56.833693: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:29:56.833694: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:29:56.833696: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.27780342, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:25<157:59:02, 205.32s/it]2022-02-17 19:29:59.899946: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 19:30:01.221511: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:30:04.429761: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:30:04.429780: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:30:04.429787: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:30:04.429789: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:30:04.429791: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:30:04.429792: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-17 19:30:06.414281: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:30:06.503383: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:30:06.796024: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:30:07.819168: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:30:07.948383: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:30:07.948416: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:30:07.948422: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:30:07.948424: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:30:07.948426: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:30:07.948428: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:36<113:01:20, 146.94s/it]DLL 2022-02-17 19:29:57.266812 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.277803421020508  train_perf : 0.15607835352420807 

Epoch: 000, Step:    50, Loss:  5.06269550, Perf:  472, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:41<76:57:32, 102.88s/it]DLL 2022-02-17 19:30:11.240918 - Training Epoch: 0 Training Iteration: 50  step_loss : 5.062695503234863  train_perf : 471.9567565917969 

Epoch: 000, Step:   100, Loss:  4.21476269, Perf:  483, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-17 19:30:14.538417 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.2147626876831055  train_perf : 483.1595764160156 

Epoch: 000, Step:   150, Loss:  3.56781888, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 154/2771 [03:46<52:21:55, 72.03s/it]DLL 2022-02-17 19:30:17.845020 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.5678188800811768  train_perf : 486.4784240722656 

Epoch: 000, Step:   200, Loss:  3.75370860, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 230/2771 [03:51<35:36:19, 50.44s/it]DLL 2022-02-17 19:30:21.159209 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.753708600997925  train_perf : 487.8291015625 

Epoch: 000, Step:   250, Loss:  3.14759111, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-17 19:30:26.270418: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:30:26.387461: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:30:26.387484: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:30:26.387491: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:30:26.387494: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:30:26.387495: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:30:26.387498: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 19:30:24.478042 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.1475911140441895  train_perf : 488.5708312988281 

Epoch: 000, Step:   300, Loss:  3.23609877, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 304/2771 [03:56<24:12:43, 35.33s/it]DLL 2022-02-17 19:30:27.949512 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.2360987663269043  train_perf : 487.9140930175781 

Epoch: 000, Step:   350, Loss:  3.06517315, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 380/2771 [04:01<16:26:22, 24.75s/it]DLL 2022-02-17 19:30:31.269721 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.0651731491088867  train_perf : 488.4234313964844 

Epoch: 000, Step:   400, Loss:  3.33222222, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-17 19:30:34.587662 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.3322222232818604  train_perf : 488.7811279296875 

Epoch: 000, Step:   450, Loss:  2.74042487, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 456/2771 [04:06<11:09:16, 17.35s/it]DLL 2022-02-17 19:30:37.910893 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.740424871444702  train_perf : 488.99737548828125 

Epoch: 000, Step:   500, Loss:  3.08321238, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 532/2771 [04:11<7:33:51, 12.16s/it] DLL 2022-02-17 19:30:41.245984 - Training Epoch: 0 Training Iteration: 500  step_loss : 3.083212375640869  train_perf : 489.0166320800781 

Epoch: 000, Step:   550, Loss:  3.42923284, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-17 19:30:44.577531 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.4292328357696533  train_perf : 489.0683288574219 

Epoch: 000, Step:   600, Loss:  2.88598657, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 608/2771 [04:16<5:07:38,  8.53s/it]DLL 2022-02-17 19:30:47.909939 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.885986566543579  train_perf : 489.1020812988281 

Epoch: 000, Step:   650, Loss:  3.11545968, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 684/2771 [04:21<3:28:28,  5.99s/it]DLL 2022-02-17 19:30:51.243282 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.115459680557251  train_perf : 489.1124572753906 

Epoch: 000, Step:   700, Loss:  2.90495253, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
DLL 2022-02-17 19:30:54.574378 - Training Epoch: 0 Training Iteration: 700  step_loss : 2.9049525260925293  train_perf : 489.137451171875 

Epoch: 000, Step:   750, Loss:  2.76344323, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:751
Iteration:  27%|██▋       | 759/2771 [04:26<2:21:21,  4.22s/it]DLL 2022-02-17 19:30:57.907590 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.7634432315826416  train_perf : 489.1400146484375 

Epoch: 000, Step:   800, Loss:  2.99952698, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:801
Iteration:  30%|███       | 835/2771 [04:31<1:35:51,  2.97s/it]DLL 2022-02-17 19:31:01.235722 - Training Epoch: 0 Training Iteration: 800  step_loss : 2.9995269775390625  train_perf : 489.1842346191406 

Epoch: 000, Step:   850, Loss:  3.50629139, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:851
DLL 2022-02-17 19:31:04.571161 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.506291389465332  train_perf : 489.16546630859375 

Epoch: 000, Step:   900, Loss:  2.47865438, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:901
Iteration:  33%|███▎      | 910/2771 [04:36<1:05:07,  2.10s/it]DLL 2022-02-17 19:31:07.903827 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.478654384613037  train_perf : 489.15557861328125 

Epoch: 000, Step:   950, Loss:  3.54330826, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:951
Iteration:  36%|███▌      | 985/2771 [04:41<44:20,  1.49s/it]  DLL 2022-02-17 19:31:11.241958 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.5433082580566406  train_perf : 489.1088562011719 

Epoch: 000, Step:  1000, Loss:  3.13918662, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1001, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1001
DLL 2022-02-17 19:31:14.577766 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.1391866207122803  train_perf : 489.0970153808594 

Epoch: 000, Step:  1050, Loss:  3.23291373, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1051, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1051
Iteration:  38%|███▊      | 1060/2771 [04:46<30:18,  1.06s/it]DLL 2022-02-17 19:31:17.916178 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.2329137325286865  train_perf : 489.0549621582031 

Epoch: 000, Step:  1100, Loss:  3.39756060, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1101
Iteration:  41%|████      | 1135/2771 [04:51<20:49,  1.31it/s]DLL 2022-02-17 19:31:21.257225 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.3975605964660645  train_perf : 489.0142517089844 

Epoch: 000, Step:  1150, Loss:  3.53173423, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1151
DLL 2022-02-17 19:31:24.597049 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.5317342281341553  train_perf : 488.9703369140625 

Epoch: 000, Step:  1200, Loss:  3.19181824, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1201
Iteration:  44%|████▎     | 1210/2771 [04:56<14:26,  1.80it/s]DLL 2022-02-17 19:31:27.936407 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.1918182373046875  train_perf : 488.93359375 

Epoch: 000, Step:  1250, Loss:  3.35059118, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1251
Iteration:  46%|████▋     | 1285/2771 [05:01<10:06,  2.45it/s]DLL 2022-02-17 19:31:31.284173 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.3505911827087402  train_perf : 488.85784912109375 

Epoch: 000, Step:  1300, Loss:  3.13250351, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1301
DLL 2022-02-17 19:31:34.625504 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.1325035095214844  train_perf : 488.81884765625 

Epoch: 000, Step:  1350, Loss:  2.97278738, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1351
Iteration:  49%|████▉     | 1360/2771 [05:06<07:11,  3.27it/s]DLL 2022-02-17 19:31:37.965895 - Training Epoch: 0 Training Iteration: 1350  step_loss : 2.972787380218506  train_perf : 488.796630859375 

Epoch: 000, Step:  1400, Loss:  3.19582152, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1401
Iteration:  52%|█████▏    | 1435/2771 [05:11<05:12,  4.27it/s]DLL 2022-02-17 19:31:41.314597 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.195821523666382  train_perf : 488.7359619140625 

Epoch: 000, Step:  1450, Loss:  2.91351700, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1451
DLL 2022-02-17 19:31:44.658740 - Training Epoch: 0 Training Iteration: 1450  step_loss : 2.9135169982910156  train_perf : 488.6941833496094 

Epoch: 000, Step:  1500, Loss:  3.47153449, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1501
Iteration:  54%|█████▍    | 1510/2771 [05:16<03:52,  5.43it/s]DLL 2022-02-17 19:31:48.002346 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.471534490585327  train_perf : 488.6536560058594 

Epoch: 000, Step:  1550, Loss:  3.20430088, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1551
Iteration:  57%|█████▋    | 1585/2771 [05:21<02:56,  6.72it/s]DLL 2022-02-17 19:31:51.342934 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.204300880432129  train_perf : 488.6259765625 

Epoch: 000, Step:  1600, Loss:  3.17911673, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1601
DLL 2022-02-17 19:31:54.682089 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.179116725921631  train_perf : 488.6070556640625 

Epoch: 000, Step:  1650, Loss:  3.17379785, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1651
Iteration:  60%|█████▉    | 1660/2771 [05:26<02:18,  8.05it/s]DLL 2022-02-17 19:31:58.020178 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.173797845840454  train_perf : 488.5885009765625 

Epoch: 000, Step:  1700, Loss:  3.02259064, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1701
Iteration:  63%|██████▎   | 1735/2771 [05:31<01:50,  9.35it/s]DLL 2022-02-17 19:32:01.360308 - Training Epoch: 0 Training Iteration: 1700  step_loss : 3.0225906372070312  train_perf : 488.56488037109375 

Epoch: 000, Step:  1750, Loss:  3.29272771, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1751
DLL 2022-02-17 19:32:04.700679 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.2927277088165283  train_perf : 488.5496826171875 

Epoch: 000, Step:  1800, Loss:  2.92385077, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1801
Iteration:  65%|██████▌   | 1810/2771 [05:36<01:31, 10.53it/s]DLL 2022-02-17 19:32:08.038877 - Training Epoch: 0 Training Iteration: 1800  step_loss : 2.9238507747650146  train_perf : 488.54095458984375 

Epoch: 000, Step:  1850, Loss:  2.95231843, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1851
Iteration:  68%|██████▊   | 1885/2771 [05:41<01:16, 11.56it/s]DLL 2022-02-17 19:32:11.379514 - Training Epoch: 0 Training Iteration: 1850  step_loss : 2.9523184299468994  train_perf : 488.5270690917969 

Epoch: 000, Step:  1900, Loss:  2.73108983, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1901
DLL 2022-02-17 19:32:14.724606 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.7310898303985596  train_perf : 488.4939270019531 

Epoch: 000, Step:  1950, Loss:  2.72629571, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1951
Iteration:  71%|███████   | 1960/2771 [05:46<01:05, 12.40it/s]2022-02-17 19:32:21.353211: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:32:21.470809: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:32:21.470827: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:32:21.470833: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:32:21.470835: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:32:21.470837: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:32:21.470839: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-17 19:32:18.072503 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.7262957096099854  train_perf : 488.44598388671875 

Epoch: 000, Step:  2000, Loss:  3.02026963, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2001
2022-02-17 19:32:22.108221: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:32:22.228000: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:32:22.228019: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:32:22.228025: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:32:22.228027: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:32:22.228029: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:32:22.228031: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:  73%|███████▎  | 2035/2771 [05:52<00:57, 12.86it/s]DLL 2022-02-17 19:32:21.572756 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.0202696323394775  train_perf : 488.2426452636719 

Epoch: 000, Step:  2050, Loss:  2.95048594, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=42, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2050
DLL 2022-02-17 19:32:25.078078 - Training Epoch: 0 Training Iteration: 2050  step_loss : 2.950485944747925  train_perf : 488.03033447265625 

Epoch: 000, Step:  2100, Loss:  3.31588173, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=92, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2100
Iteration:  76%|███████▌  | 2110/2771 [05:57<00:49, 13.42it/s]DLL 2022-02-17 19:32:28.425772 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.3158817291259766  train_perf : 488.0050048828125 

Epoch: 000, Step:  2150, Loss:  3.01835060, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=142, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2150
Iteration:  79%|███████▉  | 2185/2771 [06:02<00:42, 13.84it/s]DLL 2022-02-17 19:32:31.770008 - Training Epoch: 0 Training Iteration: 2150  step_loss : 3.018350601196289  train_perf : 487.98638916015625 

Epoch: 000, Step:  2200, Loss:  3.11439371, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=192, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-17 19:32:35.118619 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.114393711090088  train_perf : 487.9602355957031 

Epoch: 000, Step:  2250, Loss:  2.81570244, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=242, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  82%|████████▏ | 2260/2771 [06:07<00:36, 14.15it/s]DLL 2022-02-17 19:32:38.466549 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.815702438354492  train_perf : 487.93597412109375 

Epoch: 000, Step:  2300, Loss:  3.10195780, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=292, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  84%|████████▍ | 2335/2771 [06:12<00:30, 14.38it/s]DLL 2022-02-17 19:32:41.812102 - Training Epoch: 0 Training Iteration: 2300  step_loss : 3.1019577980041504  train_perf : 487.9206237792969 

Epoch: 000, Step:  2350, Loss:  2.67906761, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=342, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-17 19:32:45.156188 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.679067611694336  train_perf : 487.9077453613281 

Epoch: 000, Step:  2400, Loss:  2.84171295, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=392, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2410/2771 [06:17<00:24, 14.54it/s]DLL 2022-02-17 19:32:48.502099 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.8417129516601562  train_perf : 487.8924865722656 

Epoch: 000, Step:  2450, Loss:  3.31111360, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=442, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2485/2771 [06:22<00:19, 14.66it/s]DLL 2022-02-17 19:32:51.846072 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.3111135959625244  train_perf : 487.88092041015625 

Epoch: 000, Step:  2500, Loss:  3.30471849, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=492, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-17 19:32:55.194879 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.304718494415283  train_perf : 487.8592529296875 

Epoch: 000, Step:  2550, Loss:  2.88020372, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=542, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  92%|█████████▏| 2560/2771 [06:27<00:14, 14.74it/s]DLL 2022-02-17 19:32:58.545260 - Training Epoch: 0 Training Iteration: 2550  step_loss : 2.8802037239074707  train_perf : 487.8316955566406 

Epoch: 000, Step:  2600, Loss:  3.09423375, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=592, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2635/2771 [06:32<00:09, 14.79it/s]DLL 2022-02-17 19:33:01.902737 - Training Epoch: 0 Training Iteration: 2600  step_loss : 3.094233751296997  train_perf : 487.7907409667969 

Epoch: 000, Step:  2650, Loss:  3.09378219, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=642, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-17 19:33:05.255039 - Training Epoch: 0 Training Iteration: 2650  step_loss : 3.0937821865081787  train_perf : 487.7586364746094 

Epoch: 000, Step:  2700, Loss:  3.03833723, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=692, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
Iteration:  98%|█████████▊| 2710/2771 [06:37<00:04, 14.82it/s]DLL 2022-02-17 19:33:08.612756 - Training Epoch: 0 Training Iteration: 2700  step_loss : 3.038337230682373  train_perf : 487.7141418457031 

Epoch: 000, Step:  2750, Loss:  3.07441902, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=742, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:41<00:00,  6.90it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-17 19:33:11.965667 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.0744190216064453  train_perf : 487.68060302734375 
DLL 2022-02-17 19:33:13.239718 -  e2e_train_time : 401.29448890686035  training_sequences_per_second : 487.6709289550781  final_loss : 3.1991209983825684 

Epoch: 001, Step:     0, Loss:  2.47617531, Perf:  459, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=762, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-17 19:33:17.083806 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.476175308227539  train_perf : 459.23297119140625 

Epoch: 001, Step:    50, Loss:  2.59503937, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=812, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
Iteration:   3%|▎         | 71/2771 [00:05<03:11, 14.08it/s]DLL 2022-02-17 19:33:20.425216 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.5950393676757812  train_perf : 487.0528259277344 

Epoch: 001, Step:   100, Loss:  2.60890293, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=862, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 146/2771 [00:10<03:03, 14.33it/s]DLL 2022-02-17 19:33:23.768990 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.608902931213379  train_perf : 487.14117431640625 

Epoch: 001, Step:   150, Loss:  2.40854025, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=912, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-17 19:33:27.111459 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.4085402488708496  train_perf : 487.2528991699219 

Epoch: 001, Step:   200, Loss:  2.35573101, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=962, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 221/2771 [00:15<02:55, 14.52it/s]DLL 2022-02-17 19:33:30.453447 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.3557310104370117  train_perf : 487.349853515625 

Epoch: 001, Step:   250, Loss:  2.47285199, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1012, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 297/2771 [00:20<02:48, 14.66it/s]DLL 2022-02-17 19:33:33.783771 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.4728519916534424  train_perf : 487.72216796875 

Epoch: 001, Step:   300, Loss:  2.40058708, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1062, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-17 19:33:37.118659 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.4005870819091797  train_perf : 487.8612976074219 

Epoch: 001, Step:   350, Loss:  2.36102676, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1112, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  13%|█▎        | 372/2771 [00:25<02:42, 14.76it/s]DLL 2022-02-17 19:33:40.449120 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.3610267639160156  train_perf : 488.06378173828125 

Epoch: 001, Step:   400, Loss:  2.05373335, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1162, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 447/2771 [00:30<02:36, 14.82it/s]DLL 2022-02-17 19:33:43.794630 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.0537333488464355  train_perf : 487.9306945800781 

Epoch: 001, Step:   450, Loss:  2.16296268, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1212, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-17 19:33:47.138887 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.1629626750946045  train_perf : 487.847412109375 

Epoch: 001, Step:   500, Loss:  2.59845567, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1262, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▉        | 522/2771 [00:35<02:31, 14.86it/s]DLL 2022-02-17 19:33:50.484637 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.5984556674957275  train_perf : 487.7730407714844 

Epoch: 001, Step:   550, Loss:  2.20149875, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1312, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
Iteration:  22%|██▏       | 597/2771 [00:40<02:26, 14.88it/s]DLL 2022-02-17 19:33:53.830153 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.2014987468719482  train_perf : 487.6971435546875 

Epoch: 001, Step:   600, Loss:  2.02393818, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1362, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
DLL 2022-02-17 19:33:57.178200 - Training Epoch: 1 Training Iteration: 600  step_loss : 2.0239381790161133  train_perf : 487.6278076171875 

Epoch: 001, Step:   650, Loss:  2.76115036, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1412, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 672/2771 [00:45<02:20, 14.90it/s]DLL 2022-02-17 19:34:00.522097 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.761150360107422  train_perf : 487.5918273925781 

Epoch: 001, Step:   700, Loss:  2.33088303, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1462, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
Iteration:  27%|██▋       | 747/2771 [00:50<02:15, 14.91it/s]DLL 2022-02-17 19:34:03.871826 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.330883026123047  train_perf : 487.5155944824219 

Epoch: 001, Step:   750, Loss:  2.05071020, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1512, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3520
DLL 2022-02-17 19:34:07.222426 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.0507102012634277  train_perf : 487.4236145019531 

Epoch: 001, Step:   800, Loss:  2.85797405, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1562, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3570
Iteration:  30%|██▉       | 822/2771 [00:55<02:10, 14.91it/s]DLL 2022-02-17 19:34:10.571988 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.857974052429199  train_perf : 487.3534240722656 

Epoch: 001, Step:   850, Loss:  2.88502789, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1612, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3620
Iteration:  32%|███▏      | 897/2771 [01:00<02:05, 14.91it/s]DLL 2022-02-17 19:34:13.928785 - Training Epoch: 1 Training Iteration: 850  step_loss : 2.8850278854370117  train_perf : 487.23614501953125 

Epoch: 001, Step:   900, Loss:  2.57822275, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1662, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3670
DLL 2022-02-17 19:34:17.281318 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.5782227516174316  train_perf : 487.16357421875 

Epoch: 001, Step:   950, Loss:  2.63094807, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1712, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3720
Iteration:  35%|███▌      | 972/2771 [01:05<02:00, 14.91it/s]DLL 2022-02-17 19:34:20.631148 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.630948066711426  train_perf : 487.1147155761719 

Epoch: 001, Step:  1000, Loss:  2.78619766, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1762, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3770
Iteration:  38%|███▊      | 1047/2771 [01:10<01:55, 14.91it/s]DLL 2022-02-17 19:34:23.985509 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.7861976623535156  train_perf : 487.04632568359375 

Epoch: 001, Step:  1050, Loss:  2.86455750, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1812, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3820
DLL 2022-02-17 19:34:27.339381 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.8645575046539307  train_perf : 486.9798889160156 

Epoch: 001, Step:  1100, Loss:  3.55399871, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1862, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3870
Iteration:  40%|████      | 1122/2771 [01:15<01:50, 14.91it/s]DLL 2022-02-17 19:34:30.692563 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.5539987087249756  train_perf : 486.92626953125 

Epoch: 001, Step:  1150, Loss:  3.02083206, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1912, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3920
Iteration:  43%|████▎     | 1197/2771 [01:20<01:45, 14.91it/s]DLL 2022-02-17 19:34:34.046890 - Training Epoch: 1 Training Iteration: 1150  step_loss : 3.020832061767578  train_perf : 486.8791809082031 

Epoch: 001, Step:  1200, Loss:  2.63432717, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1962, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3970
DLL 2022-02-17 19:34:37.405643 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.6343271732330322  train_perf : 486.80511474609375 

Epoch: 001, Step:  1250, Loss:  3.05317903, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4019
Iteration:  46%|████▌     | 1272/2771 [01:25<01:40, 14.90it/s]DLL 2022-02-17 19:34:40.762334 - Training Epoch: 1 Training Iteration: 1250  step_loss : 3.0531790256500244  train_perf : 486.75860595703125 

Epoch: 001, Step:  1300, Loss:  2.57646084, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=50, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4069
Iteration:  49%|████▊     | 1347/2771 [01:30<01:35, 14.91it/s]DLL 2022-02-17 19:34:44.115540 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.576460838317871  train_perf : 486.7236633300781 

Epoch: 001, Step:  1350, Loss:  2.43739200, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=100, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4119
DLL 2022-02-17 19:34:47.467116 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.437391996383667  train_perf : 486.70391845703125 

Epoch: 001, Step:  1400, Loss:  2.53052235, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=150, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4169
Iteration:  51%|█████▏    | 1422/2771 [01:35<01:30, 14.91it/s]DLL 2022-02-17 19:34:50.817721 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.530522346496582  train_perf : 486.68682861328125 

Epoch: 001, Step:  1450, Loss:  2.25050902, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=200, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4219
Iteration:  54%|█████▍    | 1497/2771 [01:40<01:25, 14.91it/s]DLL 2022-02-17 19:34:54.171076 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.250509023666382  train_perf : 486.6524353027344 

Epoch: 001, Step:  1500, Loss:  3.44181514, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=250, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4269
DLL 2022-02-17 19:34:57.521902 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.441815137863159  train_perf : 486.63262939453125 

Epoch: 001, Step:  1550, Loss:  2.53702593, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=300, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4319
Iteration:  57%|█████▋    | 1572/2771 [01:45<01:20, 14.91it/s]DLL 2022-02-17 19:35:00.876643 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.5370259284973145  train_perf : 486.6039733886719 

Epoch: 001, Step:  1600, Loss:  2.56399655, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=350, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4369
Iteration:  59%|█████▉    | 1647/2771 [01:50<01:15, 14.91it/s]DLL 2022-02-17 19:35:04.232084 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.5639965534210205  train_perf : 486.5751037597656 

Epoch: 001, Step:  1650, Loss:  2.56724286, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=400, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4419
DLL 2022-02-17 19:35:07.590664 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.5672428607940674  train_perf : 486.53546142578125 

Epoch: 001, Step:  1700, Loss:  2.62888813, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=450, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4469
Iteration:  62%|██████▏   | 1722/2771 [01:55<01:10, 14.90it/s]DLL 2022-02-17 19:35:10.946726 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.6288881301879883  train_perf : 486.5025634765625 

Epoch: 001, Step:  1750, Loss:  2.81618309, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=500, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4519
Iteration:  65%|██████▍   | 1797/2771 [02:00<01:05, 14.90it/s]DLL 2022-02-17 19:35:14.303018 - Training Epoch: 1 Training Iteration: 1750  step_loss : 2.816183090209961  train_perf : 486.468994140625 

Epoch: 001, Step:  1800, Loss:  2.46346545, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=550, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4569
DLL 2022-02-17 19:35:17.654399 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.463465452194214  train_perf : 486.4599304199219 

Epoch: 001, Step:  1850, Loss:  2.73630118, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=600, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4619
Iteration:  68%|██████▊   | 1872/2771 [02:05<01:00, 14.90it/s]DLL 2022-02-17 19:35:21.010374 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.7363011837005615  train_perf : 486.4277648925781 

Epoch: 001, Step:  1900, Loss:  2.60227633, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=650, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4669
Iteration:  70%|███████   | 1947/2771 [02:10<00:55, 14.91it/s]DLL 2022-02-17 19:35:24.366114 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.60227632522583  train_perf : 486.4014892578125 

Epoch: 001, Step:  1950, Loss:  2.52067161, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=700, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4719
DLL 2022-02-17 19:35:27.720464 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.5206716060638428  train_perf : 486.38250732421875 

Epoch: 001, Step:  2000, Loss:  2.65824485, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=750, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4769
Iteration:  73%|███████▎  | 2022/2771 [02:15<00:50, 14.90it/s]DLL 2022-02-17 19:35:31.074545 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.6582448482513428  train_perf : 486.36566162109375 

Epoch: 001, Step:  2050, Loss:  2.29327893, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=800, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
Iteration:  76%|███████▌  | 2097/2771 [02:20<00:45, 14.91it/s]DLL 2022-02-17 19:35:34.427908 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.293278932571411  train_perf : 486.35137939453125 

Epoch: 001, Step:  2100, Loss:  2.95102262, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=850, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-17 19:35:37.782525 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.9510226249694824  train_perf : 486.33319091796875 

Epoch: 001, Step:  2150, Loss:  2.64708853, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=900, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2172/2771 [02:25<00:40, 14.90it/s]DLL 2022-02-17 19:35:41.140247 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.6470885276794434  train_perf : 486.30621337890625 

Epoch: 001, Step:  2200, Loss:  2.56329060, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=950, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2247/2771 [02:30<00:35, 14.90it/s]DLL 2022-02-17 19:35:44.499988 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.563290596008301  train_perf : 486.2763977050781 

Epoch: 001, Step:  2250, Loss:  2.54535484, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1000, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-17 19:35:47.855080 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.5453548431396484  train_perf : 486.25848388671875 

Epoch: 001, Step:  2300, Loss:  2.42907858, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1050, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▍ | 2322/2771 [02:35<00:30, 14.90it/s]DLL 2022-02-17 19:35:51.214556 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.4290785789489746  train_perf : 486.23077392578125 

Epoch: 001, Step:  2350, Loss:  2.34733129, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1100, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  87%|████████▋ | 2397/2771 [02:40<00:25, 14.90it/s]DLL 2022-02-17 19:35:54.569660 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.3473312854766846  train_perf : 486.2176208496094 

Epoch: 001, Step:  2400, Loss:  2.37135673, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1150, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-17 19:35:57.929370 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.371356725692749  train_perf : 486.1910095214844 

Epoch: 001, Step:  2450, Loss:  2.80812097, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1200, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2472/2771 [02:45<00:20, 14.90it/s]DLL 2022-02-17 19:36:01.284361 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.8081209659576416  train_perf : 486.17706298828125 

Epoch: 001, Step:  2500, Loss:  2.72387266, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1250, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2547/2771 [02:50<00:15, 14.91it/s]DLL 2022-02-17 19:36:04.633262 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.723872661590576  train_perf : 486.17657470703125 

Epoch: 001, Step:  2550, Loss:  2.72294617, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1300, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-17 19:36:07.986204 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.7229461669921875  train_perf : 486.169189453125 

Epoch: 001, Step:  2600, Loss:  3.04010725, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1350, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▍| 2622/2771 [02:56<00:09, 14.90it/s]DLL 2022-02-17 19:36:11.343020 - Training Epoch: 1 Training Iteration: 2600  step_loss : 3.040107250213623  train_perf : 486.1512451171875 

Epoch: 001, Step:  2650, Loss:  2.85148191, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1400, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2697/2771 [03:01<00:04, 14.90it/s]DLL 2022-02-17 19:36:14.701821 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.8514819145202637  train_perf : 486.1309509277344 

Epoch: 001, Step:  2700, Loss:  2.76014709, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1450, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-17 19:36:18.054011 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.7601470947265625  train_perf : 486.1275939941406 

Epoch: 001, Step:  2750, Loss:  2.74884105, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1500, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.89it/s]DLL 2022-02-17 19:36:21.413646 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.7488410472869873  train_perf : 486.1078796386719 
DLL 2022-02-17 19:36:22.689445 -  e2e_train_time : 587.2635140419006  training_sequences_per_second : 486.1051025390625  final_loss : 2.6493921279907227 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-17 19:36:27.580549: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-17 19:37:53.692239: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:37:54.765611: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:37:54.765630: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:37:54.765636: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:37:54.765638: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:37:54.765640: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:37:54.765642: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:31<32:00, 91.43s/it]Iteration:  14%|█▎        | 3/22 [01:37<20:32, 64.86s/it]Iteration:  23%|██▎       | 5/22 [01:42<13:06, 46.28s/it]Iteration:  32%|███▏      | 7/22 [01:48<08:19, 33.29s/it]Iteration:  41%|████      | 9/22 [01:55<05:14, 24.21s/it]Iteration:  50%|█████     | 11/22 [02:01<03:16, 17.87s/it]Iteration:  59%|█████▉    | 13/22 [02:07<02:01, 13.46s/it]Iteration:  68%|██████▊   | 15/22 [02:13<01:12, 10.38s/it]Iteration:  77%|███████▋  | 17/22 [02:20<00:41,  8.24s/it]Iteration:  86%|████████▋ | 19/22 [02:26<00:20,  6.75s/it]Iteration:  95%|█████████▌| 21/22 [02:33<00:05,  5.71s/it]2022-02-17 19:39:00.718585: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.71s/it]2022-02-17 19:40:10.802850: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-17 19:40:11.853724: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-17 19:40:11.853746: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-17 19:40:11.853754: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-17 19:40:11.853758: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-17 19:40:11.853761: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-17 19:40:11.853764: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:46<00:00, 25.83s/it]Iteration: 100%|██████████| 22/22 [03:46<00:00, 10.28s/it]{"exact_match": 11.759697256385998, "f1": 15.915691175959505}

Epoch: 001 Results: {"exact_match": 11.759697256385998, "f1": 15.915691175959505}

**EVAL SUMMARY** - Epoch: 001,  EM: 11.760, F1: 15.916, Infer_Perf: 1049 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 443.923 ms, 90%: 443.496 ms, 95%: 443.719 ms, 99%: 443.719 ms
DLL 2022-02-17 19:40:29.371256 -  inference_sequences_per_second : 1049.1429443359375  e2e_inference_time : 240.55154013633728 
**RESULTS SUMMARY** - EM: 11.760, F1: 15.916, Train_Time:  587 s, Train_Perf:  486 seq/s, Infer_Perf: 1049 seq/s

DLL 2022-02-17 19:40:29.371979 -  exact_match : 11.759697256385998  F1 : 15.915691175959505 
====================================  END results/models/base/checkpoints/ckpt-3497  ====================================
