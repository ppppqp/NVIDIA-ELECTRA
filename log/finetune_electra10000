==================================== START results/models/test/checkpoints/ckpt-10000 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/test/checkpoints/ckpt-10000 at step 10000
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/test/checkpoints/discriminator/config.json
Model weights saved in results/models/test/checkpoints/discriminator/tf_model.h5: {'electra', 'discriminator_predictions'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=results/models/test/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-22 17:18:26.897092: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-22 17:18:27.743851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-22 17:18:27.760718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:27.761226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 17:18:27.761259: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:18:27.762712: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 17:18:27.763315: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 17:18:27.763485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 17:18:27.764925: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 17:18:27.765281: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 17:18:27.765399: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:18:27.765450: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:27.765919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:27.766361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 17:18:27.771387: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-22 17:18:27.772014: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f2ee8000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-22 17:18:27.772026: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-22 17:18:27.916030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:27.916551: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f2de4000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-22 17:18:27.916564: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-22 17:18:27.916675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:27.917107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 17:18:27.917123: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:18:27.917145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 17:18:27.917153: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 17:18:27.917162: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 17:18:27.917171: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 17:18:27.917180: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 17:18:27.917188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:18:27.917221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:27.917669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:27.918082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 17:18:27.918098: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:18:28.082375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-22 17:18:28.082399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-22 17:18:28.082404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-22 17:18:28.082521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:28.082971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:18:28.083393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-22 17:18:27.743223 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: results/models/test/checkpoints/discriminator
loading configuration file results/models/test/checkpoints/discriminator/config.json
loading weights file results/models/test/checkpoints/discriminator/tf_model.h5
2022-02-22 17:18:28.148148: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-22 17:18:28.192091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at results/models/test/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at results/models/test/checkpoints/discriminator and are newly initialized: ['start_logits', 'end_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 37%|███▋      | 163/442 [00:05<00:08, 32.50it/s] 68%|██████▊   | 300/442 [00:10<00:04, 30.75it/s]100%|█████████▉| 440/442 [00:15<00:00, 29.86it/s]100%|██████████| 442/442 [00:15<00:00, 29.19it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 26.86it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-22 17:19:22.449847: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-22 17:19:23.351217: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 17:19:23.378104: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:19:23.978912: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-22 17:19:24.245554: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-22 17:22:19.977985: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:23.287791: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:22:23.287810: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:22:23.287817: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:22:23.287819: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:22:23.287820: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:22:23.287822: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:22:23.301617: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-22 17:22:25.290883: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:25.432261: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:22:25.432284: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:22:25.432294: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:22:25.432297: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:22:25.432300: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:22:25.432303: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:22:25.491622: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:25.664290: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:22:25.664308: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:22:25.664315: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:22:25.664317: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:22:25.664319: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:22:25.664321: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:22:25.896241: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:26.656424: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:22:26.656443: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:22:26.656467: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:22:26.656469: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:22:26.656471: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:22:26.656474: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.23925877, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:12<148:26:46, 192.93s/it]2022-02-22 17:22:29.799831: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 17:22:31.137790: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:34.437885: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:22:34.437903: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:22:34.437910: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:22:34.437912: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:22:34.437914: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:22:34.437916: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:22:36.380384: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:36.474488: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:36.781664: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:37.796026: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:37.908457: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:22:37.908489: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:22:37.908495: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:22:37.908498: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:22:37.908499: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:22:37.908501: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:23<106:22:42, 138.30s/it]DLL 2022-02-22 17:22:27.096136 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.239258766174316  train_perf : 0.16612057387828827 

Epoch: 000, Step:    50, Loss:  4.95592690, Perf:  469, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:28<72:26:09, 96.83s/it] DLL 2022-02-22 17:22:41.225396 - Training Epoch: 0 Training Iteration: 50  step_loss : 4.955926895141602  train_perf : 468.88592529296875 

Epoch: 000, Step:   100, Loss:  4.09810400, Perf:  482, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-22 17:22:44.518622 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.098104000091553  train_perf : 482.0975341796875 

Epoch: 000, Step:   150, Loss:  3.45440173, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 155/2771 [03:33<49:16:11, 67.80s/it]DLL 2022-02-22 17:22:47.799324 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.454401731491089  train_perf : 487.1636962890625 

Epoch: 000, Step:   200, Loss:  3.82253265, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 231/2771 [03:38<33:30:02, 47.48s/it]DLL 2022-02-22 17:22:51.089374 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.8225326538085938  train_perf : 489.3767395019531 

Epoch: 000, Step:   250, Loss:  3.27566695, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-22 17:22:56.171268: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:22:56.278232: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:22:56.278250: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:22:56.278257: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:22:56.278260: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:22:56.278262: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:22:56.278264: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 17:22:54.393946 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.2756669521331787  train_perf : 490.2467956542969 

Epoch: 000, Step:   300, Loss:  3.12895608, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 305/2771 [03:43<22:46:52, 33.26s/it]DLL 2022-02-22 17:22:57.841697 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.1289560794830322  train_perf : 489.6921081542969 

Epoch: 000, Step:   350, Loss:  3.13591051, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 381/2771 [03:48<15:28:07, 23.30s/it]DLL 2022-02-22 17:23:01.154310 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.1359105110168457  train_perf : 490.1183166503906 

Epoch: 000, Step:   400, Loss:  3.31431913, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-22 17:23:04.470707 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.314319133758545  train_perf : 490.343017578125 

Epoch: 000, Step:   450, Loss:  2.80159068, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 457/2771 [03:54<10:29:47, 16.33s/it]DLL 2022-02-22 17:23:07.783504 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.80159068107605  train_perf : 490.5899353027344 

Epoch: 000, Step:   500, Loss:  3.14043331, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 533/2771 [03:59<7:07:07, 11.45s/it] DLL 2022-02-22 17:23:11.097838 - Training Epoch: 0 Training Iteration: 500  step_loss : 3.1404333114624023  train_perf : 490.7638854980469 

Epoch: 000, Step:   550, Loss:  3.19520736, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-22 17:23:14.408535 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.195207357406616  train_perf : 490.9396057128906 

Epoch: 000, Step:   600, Loss:  2.87185264, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 609/2771 [04:04<4:49:32,  8.04s/it]DLL 2022-02-22 17:23:17.726902 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.8718526363372803  train_perf : 490.9911193847656 

Epoch: 000, Step:   650, Loss:  3.20011902, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 685/2771 [04:09<3:16:14,  5.64s/it]DLL 2022-02-22 17:23:21.045502 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.2001190185546875  train_perf : 491.0289306640625 

Epoch: 000, Step:   700, Loss:  3.07955289, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
DLL 2022-02-22 17:23:24.361489 - Training Epoch: 0 Training Iteration: 700  step_loss : 3.0795528888702393  train_perf : 491.09918212890625 

Epoch: 000, Step:   750, Loss:  2.86027336, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:751
Iteration:  27%|██▋       | 761/2771 [04:14<2:13:02,  3.97s/it]DLL 2022-02-22 17:23:27.677776 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.8602733612060547  train_perf : 491.1426086425781 

Epoch: 000, Step:   800, Loss:  3.02964640, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:801
Iteration:  30%|███       | 837/2771 [04:19<1:30:14,  2.80s/it]DLL 2022-02-22 17:23:30.995744 - Training Epoch: 0 Training Iteration: 800  step_loss : 3.029646396636963  train_perf : 491.1717834472656 

Epoch: 000, Step:   850, Loss:  3.41830349, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:851
DLL 2022-02-22 17:23:34.319555 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.4183034896850586  train_perf : 491.125244140625 

Epoch: 000, Step:   900, Loss:  2.64914346, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:901
Iteration:  33%|███▎      | 913/2771 [04:24<1:01:18,  1.98s/it]DLL 2022-02-22 17:23:37.638242 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.6491434574127197  train_perf : 491.1353454589844 

Epoch: 000, Step:   950, Loss:  3.29829335, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:951
Iteration:  36%|███▌      | 989/2771 [04:29<41:45,  1.41s/it]  DLL 2022-02-22 17:23:40.960887 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.298293352127075  train_perf : 491.1148986816406 

Epoch: 000, Step:  1000, Loss:  3.02140999, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1001, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1001
DLL 2022-02-22 17:23:44.293877 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.0214099884033203  train_perf : 491.0361633300781 

Epoch: 000, Step:  1050, Loss:  3.18078899, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1051, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1051
Iteration:  38%|███▊      | 1064/2771 [04:34<28:33,  1.00s/it]DLL 2022-02-22 17:23:47.629922 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.180788993835449  train_perf : 490.94134521484375 

Epoch: 000, Step:  1100, Loss:  3.60034466, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1101
Iteration:  41%|████      | 1139/2771 [04:39<19:39,  1.38it/s]DLL 2022-02-22 17:23:50.967168 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.600344657897949  train_perf : 490.8363342285156 

Epoch: 000, Step:  1150, Loss:  3.75240803, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1151
DLL 2022-02-22 17:23:54.304096 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.752408027648926  train_perf : 490.74810791015625 

Epoch: 000, Step:  1200, Loss:  3.04168844, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1201
Iteration:  44%|████▍     | 1214/2771 [04:44<13:39,  1.90it/s]DLL 2022-02-22 17:23:57.643430 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.0416884422302246  train_perf : 490.6545104980469 

Epoch: 000, Step:  1250, Loss:  3.54095268, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1251
Iteration:  47%|████▋     | 1289/2771 [04:49<09:35,  2.58it/s]DLL 2022-02-22 17:24:00.981732 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.540952682495117  train_perf : 490.5747375488281 

Epoch: 000, Step:  1300, Loss:  3.32358742, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1301
DLL 2022-02-22 17:24:04.315905 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.323587417602539  train_perf : 490.52008056640625 

Epoch: 000, Step:  1350, Loss:  3.08938789, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1351
Iteration:  49%|████▉     | 1364/2771 [04:54<06:50,  3.43it/s]DLL 2022-02-22 17:24:07.649715 - Training Epoch: 0 Training Iteration: 1350  step_loss : 3.089387893676758  train_perf : 490.46942138671875 

Epoch: 000, Step:  1400, Loss:  3.22571015, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1401
Iteration:  52%|█████▏    | 1440/2771 [04:59<04:58,  4.46it/s]DLL 2022-02-22 17:24:10.982498 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.225710153579712  train_perf : 490.4303283691406 

Epoch: 000, Step:  1450, Loss:  3.17075753, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1451
DLL 2022-02-22 17:24:14.316416 - Training Epoch: 0 Training Iteration: 1450  step_loss : 3.170757532119751  train_perf : 490.39501953125 

Epoch: 000, Step:  1500, Loss:  3.48550105, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1501
Iteration:  55%|█████▍    | 1515/2771 [05:04<03:42,  5.65it/s]DLL 2022-02-22 17:24:17.651420 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.4855010509490967  train_perf : 490.3437194824219 

Epoch: 000, Step:  1550, Loss:  3.32751346, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1551
Iteration:  57%|█████▋    | 1590/2771 [05:09<02:49,  6.95it/s]DLL 2022-02-22 17:24:20.982142 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.3275134563446045  train_perf : 490.323486328125 

Epoch: 000, Step:  1600, Loss:  3.11457753, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1601
DLL 2022-02-22 17:24:24.315829 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.114577531814575  train_perf : 490.28985595703125 

Epoch: 000, Step:  1650, Loss:  3.34336329, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1651
Iteration:  60%|██████    | 1666/2771 [05:14<02:13,  8.28it/s]DLL 2022-02-22 17:24:27.644558 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.3433632850646973  train_perf : 490.27142333984375 

Epoch: 000, Step:  1700, Loss:  2.91091013, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1701
Iteration:  63%|██████▎   | 1742/2771 [05:19<01:47,  9.57it/s]DLL 2022-02-22 17:24:30.982515 - Training Epoch: 0 Training Iteration: 1700  step_loss : 2.910910129547119  train_perf : 490.22412109375 

Epoch: 000, Step:  1750, Loss:  3.37082005, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1751
DLL 2022-02-22 17:24:34.315431 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.3708200454711914  train_perf : 490.18963623046875 

Epoch: 000, Step:  1800, Loss:  2.98602247, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1801
Iteration:  66%|██████▌   | 1817/2771 [05:24<01:28, 10.74it/s]DLL 2022-02-22 17:24:37.649237 - Training Epoch: 0 Training Iteration: 1800  step_loss : 2.986022472381592  train_perf : 490.1619567871094 

Epoch: 000, Step:  1850, Loss:  2.96684456, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1851
Iteration:  68%|██████▊   | 1892/2771 [05:29<01:14, 11.74it/s]DLL 2022-02-22 17:24:40.982371 - Training Epoch: 0 Training Iteration: 1850  step_loss : 2.9668445587158203  train_perf : 490.1351318359375 

Epoch: 000, Step:  1900, Loss:  2.89996672, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1901
DLL 2022-02-22 17:24:44.315940 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.8999667167663574  train_perf : 490.1092834472656 

Epoch: 000, Step:  1950, Loss:  2.75039339, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1951
Iteration:  71%|███████   | 1967/2771 [05:34<01:04, 12.55it/s]2022-02-22 17:24:50.920697: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:24:51.027229: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:24:51.027263: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:24:51.027270: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:24:51.027273: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:24:51.027274: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:24:51.027276: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 17:24:47.648872 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.7503933906555176  train_perf : 490.08807373046875 

Epoch: 000, Step:  2000, Loss:  3.17275667, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2001
Iteration:  74%|███████▎  | 2042/2771 [05:39<00:55, 13.09it/s]DLL 2022-02-22 17:24:51.129201 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.1727566719055176  train_perf : 489.8846435546875 

Epoch: 000, Step:  2050, Loss:  3.00483513, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2051
2022-02-22 17:24:55.542337: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:24:55.647400: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:24:55.647417: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:24:55.647424: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:24:55.647426: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:24:55.647428: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:24:55.647430: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 17:24:54.475559 - Training Epoch: 0 Training Iteration: 2050  step_loss : 3.0048351287841797  train_perf : 489.8216247558594 

Epoch: 000, Step:  2100, Loss:  3.40400767, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=34, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2100
Iteration:  76%|███████▋  | 2117/2771 [05:44<00:48, 13.50it/s]DLL 2022-02-22 17:24:57.957677 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.40400767326355  train_perf : 489.6015319824219 

Epoch: 000, Step:  2150, Loss:  3.06822896, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=84, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2150
Iteration:  79%|███████▉  | 2192/2771 [05:49<00:41, 13.91it/s]DLL 2022-02-22 17:25:01.299398 - Training Epoch: 0 Training Iteration: 2150  step_loss : 3.0682289600372314  train_perf : 489.5674133300781 

Epoch: 000, Step:  2200, Loss:  3.15039587, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=134, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-22 17:25:04.640445 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.1503958702087402  train_perf : 489.5388488769531 

Epoch: 000, Step:  2250, Loss:  2.78548622, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=184, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  82%|████████▏ | 2267/2771 [05:54<00:35, 14.21it/s]DLL 2022-02-22 17:25:07.978759 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.7854862213134766  train_perf : 489.5137023925781 

Epoch: 000, Step:  2300, Loss:  2.96926808, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=234, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  85%|████████▍ | 2342/2771 [05:59<00:29, 14.44it/s]DLL 2022-02-22 17:25:11.313006 - Training Epoch: 0 Training Iteration: 2300  step_loss : 2.9692680835723877  train_perf : 489.5059814453125 

Epoch: 000, Step:  2350, Loss:  2.81178737, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=284, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-22 17:25:14.653305 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.8117873668670654  train_perf : 489.4814453125 

Epoch: 000, Step:  2400, Loss:  2.83968115, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=334, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2417/2771 [06:04<00:24, 14.60it/s]DLL 2022-02-22 17:25:17.989243 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.8396811485290527  train_perf : 489.4679260253906 

Epoch: 000, Step:  2450, Loss:  3.31823349, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=384, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2492/2771 [06:09<00:18, 14.71it/s]DLL 2022-02-22 17:25:21.324967 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.3182334899902344  train_perf : 489.4523010253906 

Epoch: 000, Step:  2500, Loss:  3.30091643, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=434, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-22 17:25:24.668182 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.3009164333343506  train_perf : 489.4190673828125 

Epoch: 000, Step:  2550, Loss:  3.00137901, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=484, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  93%|█████████▎| 2567/2771 [06:14<00:13, 14.79it/s]DLL 2022-02-22 17:25:28.000402 - Training Epoch: 0 Training Iteration: 2550  step_loss : 3.0013790130615234  train_perf : 489.415283203125 

Epoch: 000, Step:  2600, Loss:  3.06367779, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=534, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2642/2771 [06:19<00:08, 14.84it/s]DLL 2022-02-22 17:25:31.338063 - Training Epoch: 0 Training Iteration: 2600  step_loss : 3.0636777877807617  train_perf : 489.3979797363281 

Epoch: 000, Step:  2650, Loss:  3.00090575, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=584, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-22 17:25:34.679827 - Training Epoch: 0 Training Iteration: 2650  step_loss : 3.000905752182007  train_perf : 489.3731689453125 

Epoch: 000, Step:  2700, Loss:  3.13058901, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=634, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
Iteration:  98%|█████████▊| 2717/2771 [06:24<00:03, 14.89it/s]DLL 2022-02-22 17:25:38.013642 - Training Epoch: 0 Training Iteration: 2700  step_loss : 3.130589008331299  train_perf : 489.3689880371094 

Epoch: 000, Step:  2750, Loss:  3.17401695, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=684, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:28<00:00,  7.13it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-22 17:25:41.351332 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.1740169525146484  train_perf : 489.35205078125 
DLL 2022-02-22 17:25:42.616201 -  e2e_train_time : 388.4456322193146  training_sequences_per_second : 489.35791015625  final_loss : 3.2345738410949707 

Epoch: 001, Step:     0, Loss:  2.57953620, Perf:  459, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=704, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-22 17:25:49.676303 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.579536199569702  train_perf : 458.52227783203125 

Epoch: 001, Step:    50, Loss:  2.56860995, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=754, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
Iteration:   3%|▎         | 71/2771 [00:05<03:11, 14.08it/s]DLL 2022-02-22 17:25:53.011767 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.5686099529266357  train_perf : 488.1770935058594 

Epoch: 001, Step:   100, Loss:  2.58691478, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=804, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 147/2771 [00:10<03:02, 14.35it/s]DLL 2022-02-22 17:25:56.344116 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.5869147777557373  train_perf : 488.8723449707031 

Epoch: 001, Step:   150, Loss:  2.32981396, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=854, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-22 17:25:59.675755 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.3298139572143555  train_perf : 488.90582275390625 

Epoch: 001, Step:   200, Loss:  2.48518920, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=904, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 222/2771 [00:15<02:55, 14.54it/s]DLL 2022-02-22 17:26:03.013230 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.485189199447632  train_perf : 488.83197021484375 

Epoch: 001, Step:   250, Loss:  2.47045302, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=954, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 297/2771 [00:20<02:48, 14.67it/s]DLL 2022-02-22 17:26:06.344448 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.4704530239105225  train_perf : 488.91827392578125 

Epoch: 001, Step:   300, Loss:  2.53395486, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1004, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-22 17:26:09.678529 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.5339548587799072  train_perf : 488.95733642578125 

Epoch: 001, Step:   350, Loss:  2.39122343, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1054, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  13%|█▎        | 373/2771 [00:25<02:42, 14.77it/s]DLL 2022-02-22 17:26:13.012767 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.391223430633545  train_perf : 488.9533386230469 

Epoch: 001, Step:   400, Loss:  2.07599545, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1104, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 449/2771 [00:30<02:36, 14.84it/s]DLL 2022-02-22 17:26:16.341963 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.075995445251465  train_perf : 489.02410888671875 

Epoch: 001, Step:   450, Loss:  2.16099024, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1154, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-22 17:26:19.676623 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.1609902381896973  train_perf : 489.00579833984375 

Epoch: 001, Step:   500, Loss:  2.55841732, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1204, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▉        | 525/2771 [00:35<02:30, 14.89it/s]DLL 2022-02-22 17:26:23.011388 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.558417320251465  train_perf : 488.9874267578125 

Epoch: 001, Step:   550, Loss:  2.23828554, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1254, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
DLL 2022-02-22 17:26:26.347985 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.238285541534424  train_perf : 488.96075439453125 

Epoch: 001, Step:   600, Loss:  2.03182745, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1304, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
Iteration:  22%|██▏       | 601/2771 [00:40<02:25, 14.92it/s]DLL 2022-02-22 17:26:29.677001 - Training Epoch: 1 Training Iteration: 600  step_loss : 2.031827449798584  train_perf : 489.0262451171875 

Epoch: 001, Step:   650, Loss:  2.84487271, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1354, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 677/2771 [00:45<02:20, 14.95it/s]DLL 2022-02-22 17:26:33.010353 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.8448727130889893  train_perf : 489.0304870605469 

Epoch: 001, Step:   700, Loss:  2.27995038, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1404, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
DLL 2022-02-22 17:26:36.342586 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.2799503803253174  train_perf : 489.04345703125 

Epoch: 001, Step:   750, Loss:  2.06763411, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=24, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3519
Iteration:  27%|██▋       | 752/2771 [00:50<02:14, 14.96it/s]DLL 2022-02-22 17:26:39.676669 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.067634105682373  train_perf : 489.0384826660156 

Epoch: 001, Step:   800, Loss:  2.70392656, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=74, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3569
Iteration:  30%|██▉       | 828/2771 [00:55<02:09, 14.98it/s]DLL 2022-02-22 17:26:43.006762 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.7039265632629395  train_perf : 489.0654602050781 

Epoch: 001, Step:   850, Loss:  2.79769659, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=124, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3619
DLL 2022-02-22 17:26:46.340829 - Training Epoch: 1 Training Iteration: 850  step_loss : 2.797696590423584  train_perf : 489.049072265625 

Epoch: 001, Step:   900, Loss:  2.57690167, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=174, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3669
Iteration:  33%|███▎      | 903/2771 [01:00<02:04, 14.98it/s]DLL 2022-02-22 17:26:49.678316 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.57690167427063  train_perf : 489.00347900390625 

Epoch: 001, Step:   950, Loss:  2.67267966, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=224, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3719
Iteration:  35%|███▌      | 978/2771 [01:05<01:59, 14.98it/s]DLL 2022-02-22 17:26:53.015429 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.6726796627044678  train_perf : 488.9725341796875 

Epoch: 001, Step:  1000, Loss:  2.69657040, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=274, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3769
DLL 2022-02-22 17:26:56.346663 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.69657039642334  train_perf : 488.9937438964844 

Epoch: 001, Step:  1050, Loss:  2.80706811, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=324, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3819
Iteration:  38%|███▊      | 1054/2771 [01:10<01:54, 14.99it/s]DLL 2022-02-22 17:26:59.677174 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.807068109512329  train_perf : 489.0025939941406 

Epoch: 001, Step:  1100, Loss:  3.38632035, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=374, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3869
Iteration:  41%|████      | 1130/2771 [01:15<01:49, 14.99it/s]DLL 2022-02-22 17:27:03.011272 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.3863203525543213  train_perf : 488.9891052246094 

Epoch: 001, Step:  1150, Loss:  2.99043107, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=424, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3919
DLL 2022-02-22 17:27:06.346669 - Training Epoch: 1 Training Iteration: 1150  step_loss : 2.990431070327759  train_perf : 488.9717712402344 

Epoch: 001, Step:  1200, Loss:  2.51627636, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=474, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3969
Iteration:  43%|████▎     | 1205/2771 [01:20<01:44, 14.99it/s]DLL 2022-02-22 17:27:09.679515 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.5162763595581055  train_perf : 488.9739685058594 

Epoch: 001, Step:  1250, Loss:  3.07904053, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=524, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4019
Iteration:  46%|████▌     | 1280/2771 [01:25<01:39, 14.98it/s]DLL 2022-02-22 17:27:13.019930 - Training Epoch: 1 Training Iteration: 1250  step_loss : 3.07904052734375  train_perf : 488.9355163574219 

Epoch: 001, Step:  1300, Loss:  2.40889192, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=574, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4069
DLL 2022-02-22 17:27:16.361842 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.4088919162750244  train_perf : 488.88360595703125 

Epoch: 001, Step:  1350, Loss:  2.56169891, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=624, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4119
Iteration:  49%|████▉     | 1355/2771 [01:30<01:34, 14.99it/s]DLL 2022-02-22 17:27:19.695588 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.5616989135742188  train_perf : 488.8774108886719 

Epoch: 001, Step:  1400, Loss:  2.50505590, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=674, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4169
Iteration:  52%|█████▏    | 1430/2771 [01:35<01:29, 14.98it/s]DLL 2022-02-22 17:27:23.032722 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.5050559043884277  train_perf : 488.8608703613281 

Epoch: 001, Step:  1450, Loss:  2.37531781, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=724, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4219
DLL 2022-02-22 17:27:26.371680 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.3753178119659424  train_perf : 488.839599609375 

Epoch: 001, Step:  1500, Loss:  3.47800398, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=774, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4269
Iteration:  54%|█████▍    | 1505/2771 [01:40<01:24, 14.98it/s]DLL 2022-02-22 17:27:29.707953 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.478003978729248  train_perf : 488.8280334472656 

Epoch: 001, Step:  1550, Loss:  2.52855062, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=824, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4319
Iteration:  57%|█████▋    | 1580/2771 [01:45<01:19, 14.98it/s]DLL 2022-02-22 17:27:33.044594 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.528550624847412  train_perf : 488.8222961425781 

Epoch: 001, Step:  1600, Loss:  2.67472029, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=874, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4369
DLL 2022-02-22 17:27:36.374313 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.674720287322998  train_perf : 488.8448486328125 

Epoch: 001, Step:  1650, Loss:  2.59826612, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=924, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4419
Iteration:  60%|█████▉    | 1655/2771 [01:50<01:14, 14.99it/s]DLL 2022-02-22 17:27:39.714428 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.598266124725342  train_perf : 488.8221130371094 

Epoch: 001, Step:  1700, Loss:  2.67962575, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=974, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4469
Iteration:  62%|██████▏   | 1731/2771 [01:55<01:09, 14.99it/s]DLL 2022-02-22 17:27:43.047387 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.6796257495880127  train_perf : 488.8327331542969 

Epoch: 001, Step:  1750, Loss:  2.87102795, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1024, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4519
DLL 2022-02-22 17:27:46.381694 - Training Epoch: 1 Training Iteration: 1750  step_loss : 2.871027946472168  train_perf : 488.83251953125 

Epoch: 001, Step:  1800, Loss:  2.43967748, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1074, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4569
Iteration:  65%|██████▌   | 1806/2771 [02:00<01:04, 15.00it/s]DLL 2022-02-22 17:27:49.712439 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.4396774768829346  train_perf : 488.8468933105469 

Epoch: 001, Step:  1850, Loss:  2.60195613, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1124, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4619
Iteration:  68%|██████▊   | 1882/2771 [02:05<00:59, 15.00it/s]DLL 2022-02-22 17:27:53.043189 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.6019561290740967  train_perf : 488.8588562011719 

Epoch: 001, Step:  1900, Loss:  2.64218354, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1174, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4669
DLL 2022-02-22 17:27:56.374953 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.642183542251587  train_perf : 488.867431640625 

Epoch: 001, Step:  1950, Loss:  2.53726006, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1224, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4719
Iteration:  71%|███████   | 1957/2771 [02:10<00:54, 15.00it/s]DLL 2022-02-22 17:27:59.709810 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.537260055541992  train_perf : 488.86590576171875 

Epoch: 001, Step:  2000, Loss:  2.63622212, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1274, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4769
Iteration:  73%|███████▎  | 2032/2771 [02:15<00:49, 15.00it/s]DLL 2022-02-22 17:28:03.043438 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.6362221240997314  train_perf : 488.8606872558594 

Epoch: 001, Step:  2050, Loss:  2.40585184, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1324, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
DLL 2022-02-22 17:28:06.379058 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.4058518409729004  train_perf : 488.8554382324219 

Epoch: 001, Step:  2100, Loss:  2.89696932, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1374, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
Iteration:  76%|███████▌  | 2107/2771 [02:20<00:44, 15.00it/s]DLL 2022-02-22 17:28:09.713878 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.8969693183898926  train_perf : 488.8481140136719 

Epoch: 001, Step:  2150, Loss:  2.70682192, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1424, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  79%|███████▊  | 2182/2771 [02:25<00:39, 15.00it/s]DLL 2022-02-22 17:28:13.044524 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.706821918487549  train_perf : 488.85919189453125 

Epoch: 001, Step:  2200, Loss:  2.51072288, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1474, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
DLL 2022-02-22 17:28:16.382934 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.5107228755950928  train_perf : 488.8410949707031 

Epoch: 001, Step:  2250, Loss:  2.42725658, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1524, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
Iteration:  81%|████████▏ | 2257/2771 [02:30<00:34, 14.99it/s]DLL 2022-02-22 17:28:19.724552 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.4272565841674805  train_perf : 488.8129577636719 

Epoch: 001, Step:  2300, Loss:  2.57287931, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1574, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▍ | 2332/2771 [02:35<00:29, 14.98it/s]DLL 2022-02-22 17:28:23.061662 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.5728793144226074  train_perf : 488.8026428222656 

Epoch: 001, Step:  2350, Loss:  2.22595072, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1624, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
DLL 2022-02-22 17:28:26.407999 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.2259507179260254  train_perf : 488.76611328125 

Epoch: 001, Step:  2400, Loss:  2.29476523, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1674, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
Iteration:  87%|████████▋ | 2407/2771 [02:40<00:24, 14.98it/s]DLL 2022-02-22 17:28:29.745994 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.2947652339935303  train_perf : 488.7583312988281 

Epoch: 001, Step:  2450, Loss:  2.79724002, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1724, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  90%|████████▉ | 2483/2771 [02:45<00:19, 14.98it/s]DLL 2022-02-22 17:28:33.079545 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.7972400188446045  train_perf : 488.76177978515625 

Epoch: 001, Step:  2500, Loss:  2.63780403, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1774, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
DLL 2022-02-22 17:28:36.411030 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.6378040313720703  train_perf : 488.770751953125 

Epoch: 001, Step:  2550, Loss:  2.51568437, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1824, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
Iteration:  92%|█████████▏| 2559/2771 [02:50<00:14, 14.99it/s]DLL 2022-02-22 17:28:39.741782 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.5156843662261963  train_perf : 488.7796325683594 

Epoch: 001, Step:  2600, Loss:  2.95411325, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1874, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▌| 2634/2771 [02:55<00:09, 14.99it/s]DLL 2022-02-22 17:28:43.075310 - Training Epoch: 1 Training Iteration: 2600  step_loss : 2.954113245010376  train_perf : 488.78094482421875 

Epoch: 001, Step:  2650, Loss:  2.69886780, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1924, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
DLL 2022-02-22 17:28:46.412628 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.6988677978515625  train_perf : 488.7762756347656 

Epoch: 001, Step:  2700, Loss:  2.76392221, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1974, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
Iteration:  98%|█████████▊| 2709/2771 [03:00<00:04, 14.99it/s]DLL 2022-02-22 17:28:49.745057 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.7639222145080566  train_perf : 488.7786865234375 

Epoch: 001, Step:  2750, Loss:  2.64695787, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=24, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.97it/s]DLL 2022-02-22 17:28:53.079229 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.6469578742980957  train_perf : 488.7790222167969 
DLL 2022-02-22 17:28:54.350948 -  e2e_train_time : 573.4904639720917  training_sequences_per_second : 488.7696838378906  final_loss : 2.6448941230773926 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-22 17:29:02.087993: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 17:30:23.902298: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:30:24.971510: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:30:24.971529: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:30:24.971536: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:30:24.971538: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:30:24.971540: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:30:24.971542: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:27<30:32, 87.24s/it]Iteration:  14%|█▎        | 3/22 [01:33<19:36, 61.94s/it]Iteration:  23%|██▎       | 5/22 [01:39<12:32, 44.27s/it]Iteration:  32%|███▏      | 7/22 [01:45<07:58, 31.90s/it]Iteration:  41%|████      | 9/22 [01:51<05:02, 23.27s/it]Iteration:  50%|█████     | 11/22 [01:57<03:09, 17.23s/it]Iteration:  59%|█████▉    | 13/22 [02:04<01:57, 13.02s/it]Iteration:  68%|██████▊   | 15/22 [02:10<01:10, 10.08s/it]Iteration:  77%|███████▋  | 17/22 [02:17<00:40,  8.04s/it]Iteration:  86%|████████▋ | 19/22 [02:23<00:19,  6.61s/it]Iteration:  95%|█████████▌| 21/22 [02:30<00:05,  5.63s/it]2022-02-22 17:31:32.080904: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.63s/it]2022-02-22 17:32:39.097500: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:32:40.158702: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:32:40.158721: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:32:40.158727: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:32:40.158730: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:32:40.158731: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:32:40.158733: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:40<00:00, 24.86s/it]Iteration: 100%|██████████| 22/22 [03:40<00:00, 10.00s/it]{"exact_match": 12.70577105014191, "f1": 17.237015477495127}

Epoch: 001 Results: {"exact_match": 12.70577105014191, "f1": 17.237015477495127}

**EVAL SUMMARY** - Epoch: 001,  EM: 12.706, F1: 17.237, Infer_Perf: 1048 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 444.235 ms, 90%: 443.845 ms, 95%: 444.049 ms, 99%: 444.049 ms
DLL 2022-02-22 17:32:58.225358 -  inference_sequences_per_second : 1048.41162109375  e2e_inference_time : 234.44021773338318 
**RESULTS SUMMARY** - EM: 12.706, F1: 17.237, Train_Time:  573 s, Train_Perf:  489 seq/s, Infer_Perf: 1048 seq/s

DLL 2022-02-22 17:32:58.226165 -  exact_match : 12.70577105014191  F1 : 17.237015477495127 
====================================  END results/models/test/checkpoints/ckpt-10000  ====================================
==================================== START results/models/test/checkpoints/ckpt-8494 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/test/checkpoints/ckpt-8494 at step 8493
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/test/checkpoints/discriminator/config.json
Model weights saved in results/models/test/checkpoints/discriminator/tf_model.h5: {'electra', 'discriminator_predictions'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=results/models/test/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-22 17:33:05.650137: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-22 17:33:06.505189: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-22 17:33:06.521274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.521743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 17:33:06.521760: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:33:06.523187: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 17:33:06.523774: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 17:33:06.523910: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 17:33:06.525373: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 17:33:06.525722: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 17:33:06.525805: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:33:06.525857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.526315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.526734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 17:33:06.531610: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-22 17:33:06.532021: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4a00000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-22 17:33:06.532031: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-22 17:33:06.675025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.675530: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f4914000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-22 17:33:06.675542: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-22 17:33:06.675656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.676081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 17:33:06.676097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:33:06.676120: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 17:33:06.676131: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 17:33:06.676139: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 17:33:06.676148: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 17:33:06.676158: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 17:33:06.676168: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:33:06.676200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.676635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.677048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 17:33:06.677064: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:33:06.845846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-22 17:33:06.845873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-22 17:33:06.845878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-22 17:33:06.845998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.846455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:33:06.846880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-22 17:33:06.504605 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: results/models/test/checkpoints/discriminator
loading configuration file results/models/test/checkpoints/discriminator/config.json
loading weights file results/models/test/checkpoints/discriminator/tf_model.h5
2022-02-22 17:33:06.913672: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-22 17:33:06.953855: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at results/models/test/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at results/models/test/checkpoints/discriminator and are newly initialized: ['end_logits', 'start_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 37%|███▋      | 164/442 [00:05<00:08, 32.78it/s] 69%|██████▉   | 304/442 [00:10<00:04, 31.14it/s]100%|██████████| 442/442 [00:14<00:00, 29.52it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 27.19it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-22 17:34:03.534774: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-22 17:34:04.424123: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 17:34:04.450926: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:34:05.055973: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-22 17:34:05.330378: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-22 17:37:10.511551: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:13.811752: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:37:13.811770: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:37:13.811776: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:37:13.811778: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:37:13.811780: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:37:13.811782: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:37:13.826134: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-22 17:37:15.822665: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:15.962237: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:37:15.962271: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:37:15.962279: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:37:15.962282: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:37:15.962286: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:37:15.962288: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:37:16.022704: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:16.205857: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:37:16.205879: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:37:16.205888: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:37:16.205891: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:37:16.205894: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:37:16.205898: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:37:16.427253: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:17.188105: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:37:17.188128: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:37:17.188135: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:37:17.188139: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:37:17.188142: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:37:17.188145: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.19953632, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:22<155:45:58, 202.44s/it]2022-02-22 17:37:20.291734: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 17:37:21.635024: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:24.941746: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:37:24.941764: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:37:24.941770: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:37:24.941773: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:37:24.941774: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:37:24.941776: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:37:26.908386: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:26.997554: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:27.289402: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:28.302072: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:28.419764: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:37:28.419783: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:37:28.419790: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:37:28.419792: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:37:28.419794: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:37:28.419796: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:33<111:29:39, 144.95s/it]DLL 2022-02-22 17:37:17.635396 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.199536323547363  train_perf : 0.15830190479755402 

Epoch: 000, Step:    50, Loss:  4.90986633, Perf:  468, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 77/2771 [03:38<75:56:49, 101.49s/it]DLL 2022-02-22 17:37:31.739919 - Training Epoch: 0 Training Iteration: 50  step_loss : 4.9098663330078125  train_perf : 468.108154296875 

Epoch: 000, Step:   100, Loss:  4.17315674, Perf:  479, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-22 17:37:35.064787 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.17315673828125  train_perf : 479.2964172363281 

Epoch: 000, Step:   150, Loss:  3.55785656, Perf:  483, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 153/2771 [03:43<51:40:39, 71.06s/it]DLL 2022-02-22 17:37:38.382488 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.557856559753418  train_perf : 483.43109130859375 

Epoch: 000, Step:   200, Loss:  3.62154245, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 229/2771 [03:48<35:08:17, 49.76s/it]DLL 2022-02-22 17:37:41.699194 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.621542453765869  train_perf : 485.45867919921875 

Epoch: 000, Step:   250, Loss:  3.35797334, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-22 17:37:46.805617: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:37:46.918384: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:37:46.918402: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:37:46.918408: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:37:46.918410: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:37:46.918412: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:37:46.918414: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 17:37:45.015225 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.357973337173462  train_perf : 486.7359619140625 

Epoch: 000, Step:   300, Loss:  3.04061556, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 303/2771 [03:53<23:53:41, 34.85s/it]DLL 2022-02-22 17:37:48.487798 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.0406155586242676  train_perf : 486.17633056640625 

Epoch: 000, Step:   350, Loss:  3.06644797, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 379/2771 [03:58<16:13:28, 24.42s/it]DLL 2022-02-22 17:37:51.820476 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.0664479732513428  train_perf : 486.6327819824219 

Epoch: 000, Step:   400, Loss:  3.12562990, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-22 17:37:55.151031 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.1256299018859863  train_perf : 487.0065002441406 

Epoch: 000, Step:   450, Loss:  2.66435242, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 455/2771 [04:03<11:00:33, 17.11s/it]DLL 2022-02-22 17:37:58.474107 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.6643524169921875  train_perf : 487.4171142578125 

Epoch: 000, Step:   500, Loss:  3.01296902, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 531/2771 [04:08<7:27:57, 12.00s/it] DLL 2022-02-22 17:38:01.794956 - Training Epoch: 0 Training Iteration: 500  step_loss : 3.0129690170288086  train_perf : 487.7632141113281 

Epoch: 000, Step:   550, Loss:  3.24820805, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-22 17:38:05.125628 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.2482080459594727  train_perf : 487.94403076171875 

Epoch: 000, Step:   600, Loss:  2.67436242, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 606/2771 [04:13<5:03:47,  8.42s/it]DLL 2022-02-22 17:38:08.465738 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.6743624210357666  train_perf : 487.9790954589844 

Epoch: 000, Step:   650, Loss:  3.13670444, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 682/2771 [04:18<3:25:53,  5.91s/it]DLL 2022-02-22 17:38:11.795190 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.136704444885254  train_perf : 488.1088562011719 

Epoch: 000, Step:   700, Loss:  3.01350498, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
DLL 2022-02-22 17:38:15.132551 - Training Epoch: 0 Training Iteration: 700  step_loss : 3.013504981994629  train_perf : 488.15679931640625 

Epoch: 000, Step:   750, Loss:  2.87760186, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:751
Iteration:  27%|██▋       | 758/2771 [04:23<2:19:32,  4.16s/it]DLL 2022-02-22 17:38:18.463206 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.8776018619537354  train_perf : 488.2472839355469 

Epoch: 000, Step:   800, Loss:  2.98136258, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:801
Iteration:  30%|███       | 833/2771 [04:28<1:34:41,  2.93s/it]DLL 2022-02-22 17:38:21.795967 - Training Epoch: 0 Training Iteration: 800  step_loss : 2.9813625812530518  train_perf : 488.3216247558594 

Epoch: 000, Step:   850, Loss:  3.61559916, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:851
DLL 2022-02-22 17:38:25.131330 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.6155991554260254  train_perf : 488.3600769042969 

Epoch: 000, Step:   900, Loss:  2.38654637, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:901
Iteration:  33%|███▎      | 909/2771 [04:33<1:04:18,  2.07s/it]DLL 2022-02-22 17:38:28.461611 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.3865463733673096  train_perf : 488.41607666015625 

Epoch: 000, Step:   950, Loss:  3.36487651, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:951
Iteration:  36%|███▌      | 984/2771 [04:38<43:47,  1.47s/it]  DLL 2022-02-22 17:38:31.802436 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.3648765087127686  train_perf : 488.38494873046875 

Epoch: 000, Step:  1000, Loss:  3.11580467, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1001, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1001
DLL 2022-02-22 17:38:35.152494 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.115804672241211  train_perf : 488.30072021484375 

Epoch: 000, Step:  1050, Loss:  3.03916311, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1051, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1051
Iteration:  38%|███▊      | 1059/2771 [04:43<29:56,  1.05s/it]DLL 2022-02-22 17:38:38.499471 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.039163112640381  train_perf : 488.2550354003906 

Epoch: 000, Step:  1100, Loss:  3.50958562, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1101
Iteration:  41%|████      | 1134/2771 [04:48<20:35,  1.33it/s]DLL 2022-02-22 17:38:41.839541 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.5095856189727783  train_perf : 488.2472229003906 

Epoch: 000, Step:  1150, Loss:  3.53566504, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1151
DLL 2022-02-22 17:38:45.189967 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.5356650352478027  train_perf : 488.17718505859375 

Epoch: 000, Step:  1200, Loss:  3.17416716, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1201
Iteration:  44%|████▎     | 1209/2771 [04:53<14:16,  1.82it/s]DLL 2022-02-22 17:38:48.537701 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.1741671562194824  train_perf : 488.12774658203125 

Epoch: 000, Step:  1250, Loss:  3.41741848, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1251
Iteration:  46%|████▋     | 1284/2771 [04:58<10:00,  2.48it/s]DLL 2022-02-22 17:38:51.883757 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.4174184799194336  train_perf : 488.0948181152344 

Epoch: 000, Step:  1300, Loss:  3.18387032, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1301
DLL 2022-02-22 17:38:55.228750 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.183870315551758  train_perf : 488.06793212890625 

Epoch: 000, Step:  1350, Loss:  3.04465532, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1351
Iteration:  49%|████▉     | 1359/2771 [05:03<07:07,  3.30it/s]DLL 2022-02-22 17:38:58.575354 - Training Epoch: 0 Training Iteration: 1350  step_loss : 3.0446553230285645  train_perf : 488.0323791503906 

Epoch: 000, Step:  1400, Loss:  3.19092178, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1401
Iteration:  52%|█████▏    | 1434/2771 [05:08<05:10,  4.31it/s]DLL 2022-02-22 17:39:01.921948 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.1909217834472656  train_perf : 487.99737548828125 

Epoch: 000, Step:  1450, Loss:  3.00991035, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1451
DLL 2022-02-22 17:39:05.271674 - Training Epoch: 0 Training Iteration: 1450  step_loss : 3.0099103450775146  train_perf : 487.9548645019531 

Epoch: 000, Step:  1500, Loss:  3.55274439, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1501
Iteration:  54%|█████▍    | 1509/2771 [05:13<03:50,  5.48it/s]DLL 2022-02-22 17:39:08.618573 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.5527443885803223  train_perf : 487.9294128417969 

Epoch: 000, Step:  1550, Loss:  3.17583513, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1551
Iteration:  57%|█████▋    | 1584/2771 [05:18<02:55,  6.76it/s]DLL 2022-02-22 17:39:11.966693 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.175835132598877  train_perf : 487.9048156738281 

Epoch: 000, Step:  1600, Loss:  3.12390804, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1601
DLL 2022-02-22 17:39:15.314891 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.123908042907715  train_perf : 487.87652587890625 

Epoch: 000, Step:  1650, Loss:  3.09647942, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1651
Iteration:  60%|█████▉    | 1659/2771 [05:24<02:17,  8.09it/s]DLL 2022-02-22 17:39:18.664170 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.0964794158935547  train_perf : 487.8419494628906 

Epoch: 000, Step:  1700, Loss:  2.99921179, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1701
Iteration:  63%|██████▎   | 1734/2771 [05:29<01:50,  9.38it/s]DLL 2022-02-22 17:39:22.014616 - Training Epoch: 0 Training Iteration: 1700  step_loss : 2.9992117881774902  train_perf : 487.8050537109375 

Epoch: 000, Step:  1750, Loss:  3.18553162, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1751
DLL 2022-02-22 17:39:25.366715 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.1855316162109375  train_perf : 487.7601013183594 

Epoch: 000, Step:  1800, Loss:  2.92425251, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1801
Iteration:  65%|██████▌   | 1809/2771 [05:34<01:31, 10.56it/s]DLL 2022-02-22 17:39:28.716149 - Training Epoch: 0 Training Iteration: 1800  step_loss : 2.924252510070801  train_perf : 487.7292785644531 

Epoch: 000, Step:  1850, Loss:  2.86953497, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1851
Iteration:  68%|██████▊   | 1884/2771 [05:39<01:16, 11.57it/s]DLL 2022-02-22 17:39:32.064541 - Training Epoch: 0 Training Iteration: 1850  step_loss : 2.869534969329834  train_perf : 487.70172119140625 

Epoch: 000, Step:  1900, Loss:  2.78123021, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1901
DLL 2022-02-22 17:39:35.414549 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.7812302112579346  train_perf : 487.677734375 

Epoch: 000, Step:  1950, Loss:  2.57426524, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1951
Iteration:  71%|███████   | 1959/2771 [05:44<01:05, 12.41it/s]2022-02-22 17:39:42.040708: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:39:42.155795: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:39:42.155832: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:39:42.155853: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:39:42.155855: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:39:42.155857: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:39:42.155859: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 17:39:38.763203 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.574265241622925  train_perf : 487.6661682128906 

Epoch: 000, Step:  2000, Loss:  3.12508416, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2001
Iteration:  73%|███████▎  | 2034/2771 [05:49<00:56, 12.97it/s]DLL 2022-02-22 17:39:42.257519 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.125084161758423  train_perf : 487.4938049316406 

Epoch: 000, Step:  2050, Loss:  2.95665979, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2051
2022-02-22 17:39:45.873071: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:39:45.986533: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:39:45.986552: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:39:45.986558: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:39:45.986560: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:39:45.986563: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:39:45.986565: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 17:39:45.607644 - Training Epoch: 0 Training Iteration: 2050  step_loss : 2.9566597938537598  train_perf : 487.4712219238281 

Epoch: 000, Step:  2100, Loss:  3.18614078, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=46, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2100
Iteration:  76%|███████▌  | 2109/2771 [05:54<00:49, 13.40it/s]DLL 2022-02-22 17:39:49.105717 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.186140775680542  train_perf : 487.2736511230469 

Epoch: 000, Step:  2150, Loss:  2.96413970, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=96, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2150
Iteration:  79%|███████▉  | 2184/2771 [05:59<00:42, 13.82it/s]DLL 2022-02-22 17:39:52.455290 - Training Epoch: 0 Training Iteration: 2150  step_loss : 2.964139699935913  train_perf : 487.2610778808594 

Epoch: 000, Step:  2200, Loss:  3.05611324, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=146, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-22 17:39:55.802669 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.0561132431030273  train_perf : 487.2527770996094 

Epoch: 000, Step:  2250, Loss:  2.93039989, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=196, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  82%|████████▏ | 2259/2771 [06:04<00:36, 14.13it/s]DLL 2022-02-22 17:39:59.156370 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.9303998947143555  train_perf : 487.2259521484375 

Epoch: 000, Step:  2300, Loss:  3.11433148, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=246, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  84%|████████▍ | 2334/2771 [06:09<00:30, 14.36it/s]DLL 2022-02-22 17:40:02.507970 - Training Epoch: 0 Training Iteration: 2300  step_loss : 3.1143314838409424  train_perf : 487.208984375 

Epoch: 000, Step:  2350, Loss:  2.65596008, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=296, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-22 17:40:05.853683 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.6559600830078125  train_perf : 487.20758056640625 

Epoch: 000, Step:  2400, Loss:  2.82865071, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=346, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2409/2771 [06:14<00:24, 14.53it/s]DLL 2022-02-22 17:40:09.200968 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.828650712966919  train_perf : 487.2000732421875 

Epoch: 000, Step:  2450, Loss:  3.19790721, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=396, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2484/2771 [06:19<00:19, 14.64it/s]DLL 2022-02-22 17:40:12.554230 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.1979072093963623  train_perf : 487.1783447265625 

Epoch: 000, Step:  2500, Loss:  3.26774859, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=446, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-22 17:40:15.906064 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.2677485942840576  train_perf : 487.16632080078125 

Epoch: 000, Step:  2550, Loss:  3.08425879, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=496, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  92%|█████████▏| 2559/2771 [06:24<00:14, 14.72it/s]DLL 2022-02-22 17:40:19.259950 - Training Epoch: 0 Training Iteration: 2550  step_loss : 3.084258794784546  train_perf : 487.1433410644531 

Epoch: 000, Step:  2600, Loss:  3.04739571, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=546, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2634/2771 [06:29<00:09, 14.78it/s]DLL 2022-02-22 17:40:22.611125 - Training Epoch: 0 Training Iteration: 2600  step_loss : 3.047395706176758  train_perf : 487.1283264160156 

Epoch: 000, Step:  2650, Loss:  3.02287102, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=596, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-22 17:40:25.962807 - Training Epoch: 0 Training Iteration: 2650  step_loss : 3.0228710174560547  train_perf : 487.1102294921875 

Epoch: 000, Step:  2700, Loss:  3.00900745, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=646, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
Iteration:  98%|█████████▊| 2709/2771 [06:34<00:04, 14.83it/s]DLL 2022-02-22 17:40:29.309091 - Training Epoch: 0 Training Iteration: 2700  step_loss : 3.009007453918457  train_perf : 487.10986328125 

Epoch: 000, Step:  2750, Loss:  3.05000091, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=696, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:38<00:00,  6.95it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-22 17:40:32.657208 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.0500009059906006  train_perf : 487.1045837402344 
DLL 2022-02-22 17:40:33.931741 -  e2e_train_time : 398.7354505062103  training_sequences_per_second : 487.10235595703125  final_loss : 3.1773574352264404 

Epoch: 001, Step:     0, Loss:  2.47739220, Perf:  458, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=716, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-22 17:40:42.270075 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.4773921966552734  train_perf : 457.72479248046875 

Epoch: 001, Step:    50, Loss:  2.48596358, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=766, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
Iteration:   3%|▎         | 71/2771 [00:05<03:11, 14.10it/s]DLL 2022-02-22 17:40:45.613910 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.4859635829925537  train_perf : 487.1449279785156 

Epoch: 001, Step:   100, Loss:  2.42544842, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=816, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 146/2771 [00:10<03:03, 14.34it/s]DLL 2022-02-22 17:40:48.958018 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.425448417663574  train_perf : 487.225830078125 

Epoch: 001, Step:   150, Loss:  2.45026660, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=866, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-22 17:40:52.300454 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.4502665996551514  train_perf : 487.34552001953125 

Epoch: 001, Step:   200, Loss:  2.35413337, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=916, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 221/2771 [00:15<02:55, 14.52it/s]DLL 2022-02-22 17:40:55.647766 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.354133367538452  train_perf : 487.223388671875 

Epoch: 001, Step:   250, Loss:  2.49242902, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=966, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 296/2771 [00:20<02:48, 14.65it/s]DLL 2022-02-22 17:40:58.993628 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.49242901802063  train_perf : 487.2178039550781 

Epoch: 001, Step:   300, Loss:  2.37156200, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1016, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-22 17:41:02.336064 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.3715620040893555  train_perf : 487.2650146484375 

Epoch: 001, Step:   350, Loss:  2.45099020, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1066, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  13%|█▎        | 371/2771 [00:25<02:42, 14.74it/s]DLL 2022-02-22 17:41:05.680005 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.4509902000427246  train_perf : 487.2923889160156 

Epoch: 001, Step:   400, Loss:  2.02966189, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1116, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 446/2771 [00:30<02:37, 14.80it/s]DLL 2022-02-22 17:41:09.026870 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.0296618938446045  train_perf : 487.2524108886719 

Epoch: 001, Step:   450, Loss:  2.23768425, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1166, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-22 17:41:12.368090 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.2376842498779297  train_perf : 487.30419921875 

Epoch: 001, Step:   500, Loss:  2.36535406, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1216, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▉        | 521/2771 [00:35<02:31, 14.84it/s]DLL 2022-02-22 17:41:15.714190 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.365354061126709  train_perf : 487.2866516113281 

Epoch: 001, Step:   550, Loss:  2.03641987, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1266, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
Iteration:  22%|██▏       | 596/2771 [00:40<02:26, 14.88it/s]DLL 2022-02-22 17:41:19.055870 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.0364198684692383  train_perf : 487.3197021484375 

Epoch: 001, Step:   600, Loss:  1.96000051, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1316, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
DLL 2022-02-22 17:41:22.397222 - Training Epoch: 1 Training Iteration: 600  step_loss : 1.9600005149841309  train_perf : 487.3468322753906 

Epoch: 001, Step:   650, Loss:  2.78028917, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1366, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 671/2771 [00:45<02:20, 14.90it/s]DLL 2022-02-22 17:41:25.742279 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.7802891731262207  train_perf : 487.337646484375 

Epoch: 001, Step:   700, Loss:  2.41466379, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1416, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
Iteration:  27%|██▋       | 746/2771 [00:50<02:15, 14.91it/s]DLL 2022-02-22 17:41:29.087367 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.414663791656494  train_perf : 487.3113708496094 

Epoch: 001, Step:   750, Loss:  2.08809948, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1466, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3520
DLL 2022-02-22 17:41:32.431744 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.088099479675293  train_perf : 487.3044128417969 

Epoch: 001, Step:   800, Loss:  2.75951838, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1516, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3570
Iteration:  30%|██▉       | 821/2771 [00:55<02:10, 14.93it/s]DLL 2022-02-22 17:41:35.773169 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.7595183849334717  train_perf : 487.32330322265625 

Epoch: 001, Step:   850, Loss:  2.83859015, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1566, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3620
Iteration:  32%|███▏      | 896/2771 [01:00<02:05, 14.93it/s]DLL 2022-02-22 17:41:39.119522 - Training Epoch: 1 Training Iteration: 850  step_loss : 2.838590145111084  train_perf : 487.2962951660156 

Epoch: 001, Step:   900, Loss:  2.43180656, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1616, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3670
DLL 2022-02-22 17:41:42.469635 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.4318065643310547  train_perf : 487.2483825683594 

Epoch: 001, Step:   950, Loss:  2.67511010, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1666, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3720
Iteration:  35%|███▌      | 971/2771 [01:05<02:00, 14.92it/s]DLL 2022-02-22 17:41:45.825272 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.675110101699829  train_perf : 487.1669921875 

Epoch: 001, Step:  1000, Loss:  2.74328279, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1716, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3770
Iteration:  38%|███▊      | 1046/2771 [01:10<01:55, 14.92it/s]DLL 2022-02-22 17:41:49.173926 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.7432827949523926  train_perf : 487.1336364746094 

Epoch: 001, Step:  1050, Loss:  2.88480210, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1766, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3820
DLL 2022-02-22 17:41:52.525311 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.8848021030426025  train_perf : 487.08489990234375 

Epoch: 001, Step:  1100, Loss:  3.22923994, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1816, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3870
Iteration:  40%|████      | 1121/2771 [01:15<01:50, 14.92it/s]DLL 2022-02-22 17:41:55.878554 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.2292399406433105  train_perf : 487.03564453125 

Epoch: 001, Step:  1150, Loss:  2.88350177, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1866, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3920
Iteration:  43%|████▎     | 1196/2771 [01:20<01:45, 14.92it/s]DLL 2022-02-22 17:41:59.226826 - Training Epoch: 1 Training Iteration: 1150  step_loss : 2.8835017681121826  train_perf : 487.0090637207031 

Epoch: 001, Step:  1200, Loss:  2.52540708, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1916, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3970
DLL 2022-02-22 17:42:02.577548 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.525407075881958  train_perf : 486.9786071777344 

Epoch: 001, Step:  1250, Loss:  3.01126766, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1966, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4020
Iteration:  46%|████▌     | 1271/2771 [01:25<01:40, 14.92it/s]DLL 2022-02-22 17:42:05.930112 - Training Epoch: 1 Training Iteration: 1250  step_loss : 3.01126766204834  train_perf : 486.9434814453125 

Epoch: 001, Step:  1300, Loss:  2.39043689, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=16, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4070
Iteration:  49%|████▊     | 1346/2771 [01:30<01:35, 14.92it/s]DLL 2022-02-22 17:42:09.278716 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.390436887741089  train_perf : 486.9239196777344 

Epoch: 001, Step:  1350, Loss:  2.49860859, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=66, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4120
DLL 2022-02-22 17:42:12.629517 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.4986085891723633  train_perf : 486.90191650390625 

Epoch: 001, Step:  1400, Loss:  2.38299346, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=116, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4170
Iteration:  51%|█████▏    | 1421/2771 [01:35<01:30, 14.93it/s]DLL 2022-02-22 17:42:15.974746 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.382993459701538  train_perf : 486.9064025878906 

Epoch: 001, Step:  1450, Loss:  2.26708412, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=7, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4219
Iteration:  54%|█████▍    | 1496/2771 [01:40<01:25, 14.93it/s]DLL 2022-02-22 17:42:19.322503 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.2670841217041016  train_perf : 486.8990783691406 

Epoch: 001, Step:  1500, Loss:  3.46812010, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=57, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4269
DLL 2022-02-22 17:42:22.672688 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.4681200981140137  train_perf : 486.87994384765625 

Epoch: 001, Step:  1550, Loss:  2.53718948, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=107, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4319
Iteration:  57%|█████▋    | 1571/2771 [01:45<01:20, 14.93it/s]DLL 2022-02-22 17:42:26.023811 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.537189483642578  train_perf : 486.8606872558594 

Epoch: 001, Step:  1600, Loss:  2.46850848, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=157, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4369
Iteration:  59%|█████▉    | 1646/2771 [01:50<01:15, 14.93it/s]DLL 2022-02-22 17:42:29.374638 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.46850848197937  train_perf : 486.8453063964844 

Epoch: 001, Step:  1650, Loss:  2.65077066, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=207, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4419
DLL 2022-02-22 17:42:32.727284 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.650770664215088  train_perf : 486.8219299316406 

Epoch: 001, Step:  1700, Loss:  2.65205765, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=257, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4469
Iteration:  62%|██████▏   | 1721/2771 [01:55<01:10, 14.92it/s]DLL 2022-02-22 17:42:36.081065 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.652057647705078  train_perf : 486.7937316894531 

Epoch: 001, Step:  1750, Loss:  2.82461834, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=307, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4519
Iteration:  65%|██████▍   | 1796/2771 [02:00<01:05, 14.92it/s]DLL 2022-02-22 17:42:39.433375 - Training Epoch: 1 Training Iteration: 1750  step_loss : 2.824618339538574  train_perf : 486.7661437988281 

Epoch: 001, Step:  1800, Loss:  2.41386747, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=357, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4569
DLL 2022-02-22 17:42:42.784245 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.413867473602295  train_perf : 486.74957275390625 

Epoch: 001, Step:  1850, Loss:  2.66109180, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=407, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4619
Iteration:  68%|██████▊   | 1871/2771 [02:05<01:00, 14.92it/s]DLL 2022-02-22 17:42:46.135047 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.6610918045043945  train_perf : 486.73486328125 

Epoch: 001, Step:  1900, Loss:  2.64798474, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=457, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4669
Iteration:  70%|███████   | 1946/2771 [02:10<00:55, 14.91it/s]DLL 2022-02-22 17:42:49.492700 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.647984743118286  train_perf : 486.70440673828125 

Epoch: 001, Step:  1950, Loss:  2.36087584, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=507, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4719
DLL 2022-02-22 17:42:52.843044 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.3608758449554443  train_perf : 486.6962890625 

Epoch: 001, Step:  2000, Loss:  2.62508965, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=557, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4769
Iteration:  73%|███████▎  | 2021/2771 [02:15<00:50, 14.92it/s]DLL 2022-02-22 17:42:56.194898 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.625089645385742  train_perf : 486.68743896484375 

Epoch: 001, Step:  2050, Loss:  2.34423971, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=607, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
Iteration:  76%|███████▌  | 2096/2771 [02:20<00:45, 14.91it/s]DLL 2022-02-22 17:42:59.543469 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.3442397117614746  train_perf : 486.6839599609375 

Epoch: 001, Step:  2100, Loss:  2.95305014, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=657, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-22 17:43:02.900080 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.953050136566162  train_perf : 486.655517578125 

Epoch: 001, Step:  2150, Loss:  2.80087090, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=707, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2171/2771 [02:25<00:40, 14.91it/s]DLL 2022-02-22 17:43:06.251554 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.800870895385742  train_perf : 486.6446533203125 

Epoch: 001, Step:  2200, Loss:  2.49677968, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=757, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2246/2771 [02:30<00:35, 14.91it/s]DLL 2022-02-22 17:43:09.608829 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.496779680252075  train_perf : 486.6129455566406 

Epoch: 001, Step:  2250, Loss:  2.45274210, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=807, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-22 17:43:12.960472 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.452742099761963  train_perf : 486.60101318359375 

Epoch: 001, Step:  2300, Loss:  2.47924948, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=857, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▍ | 2321/2771 [02:35<00:30, 14.92it/s]DLL 2022-02-22 17:43:16.305435 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.4792494773864746  train_perf : 486.60833740234375 

Epoch: 001, Step:  2350, Loss:  2.19810796, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=907, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  86%|████████▋ | 2396/2771 [02:40<00:25, 14.92it/s]DLL 2022-02-22 17:43:19.655606 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.198107957839966  train_perf : 486.5978698730469 

Epoch: 001, Step:  2400, Loss:  2.24130368, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=957, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-22 17:43:23.001430 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.2413036823272705  train_perf : 486.6039733886719 

Epoch: 001, Step:  2450, Loss:  2.83427167, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1007, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2471/2771 [02:45<00:20, 14.92it/s]DLL 2022-02-22 17:43:26.354059 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.8342716693878174  train_perf : 486.590576171875 

Epoch: 001, Step:  2500, Loss:  2.75956368, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1057, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2546/2771 [02:50<00:15, 14.92it/s]DLL 2022-02-22 17:43:29.703625 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.759563684463501  train_perf : 486.5875244140625 

Epoch: 001, Step:  2550, Loss:  2.68496180, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1107, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-22 17:43:33.056117 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.6849617958068848  train_perf : 486.57208251953125 

Epoch: 001, Step:  2600, Loss:  2.91409397, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1157, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▍| 2621/2771 [02:55<00:10, 14.92it/s]DLL 2022-02-22 17:43:36.409629 - Training Epoch: 1 Training Iteration: 2600  step_loss : 2.9140939712524414  train_perf : 486.56036376953125 

Epoch: 001, Step:  2650, Loss:  2.77571392, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1207, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2696/2771 [03:00<00:05, 14.92it/s]DLL 2022-02-22 17:43:39.760076 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.7757139205932617  train_perf : 486.5553894042969 

Epoch: 001, Step:  2700, Loss:  2.71554494, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1257, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-22 17:43:43.112675 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.7155449390411377  train_perf : 486.547119140625 

Epoch: 001, Step:  2750, Loss:  2.63571882, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1307, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.91it/s]DLL 2022-02-22 17:43:46.462198 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.635718822479248  train_perf : 486.5450439453125 
DLL 2022-02-22 17:43:47.733640 -  e2e_train_time : 584.5556075572968  training_sequences_per_second : 486.5485534667969  final_loss : 2.614384651184082 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-22 17:43:54.237309: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 17:45:20.123905: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:45:21.186603: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:45:21.186622: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:45:21.186629: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:45:21.186631: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:45:21.186633: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:45:21.186635: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:31<31:54, 91.18s/it]Iteration:  14%|█▎        | 3/22 [01:37<20:29, 64.70s/it]Iteration:  23%|██▎       | 5/22 [01:42<13:05, 46.19s/it]Iteration:  32%|███▏      | 7/22 [01:49<08:18, 33.25s/it]Iteration:  41%|████      | 9/22 [01:55<05:14, 24.20s/it]Iteration:  50%|█████     | 11/22 [02:01<03:16, 17.88s/it]Iteration:  59%|█████▉    | 13/22 [02:07<02:01, 13.47s/it]Iteration:  68%|██████▊   | 15/22 [02:14<01:12, 10.39s/it]Iteration:  77%|███████▋  | 17/22 [02:20<00:41,  8.25s/it]Iteration:  86%|████████▋ | 19/22 [02:27<00:20,  6.75s/it]Iteration:  95%|█████████▌| 21/22 [02:33<00:05,  5.73s/it]2022-02-22 17:46:27.938494: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.73s/it]2022-02-22 17:47:39.199125: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:47:40.278554: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:47:40.278573: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:47:40.278579: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:47:40.278582: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:47:40.278583: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:47:40.278585: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:47<00:00, 26.21s/it]Iteration: 100%|██████████| 22/22 [03:47<00:00, 10.36s/it]{"exact_match": 12.71523178807947, "f1": 17.203398238494966}

Epoch: 001 Results: {"exact_match": 12.71523178807947, "f1": 17.203398238494966}

**EVAL SUMMARY** - Epoch: 001,  EM: 12.715, F1: 17.203, Infer_Perf: 1050 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 443.420 ms, 90%: 442.979 ms, 95%: 443.211 ms, 99%: 443.211 ms
DLL 2022-02-22 17:47:58.003110 -  inference_sequences_per_second : 1050.3109130859375  e2e_inference_time : 242.49227595329285 
**RESULTS SUMMARY** - EM: 12.715, F1: 17.203, Train_Time:  585 s, Train_Perf:  487 seq/s, Infer_Perf: 1050 seq/s

DLL 2022-02-22 17:47:58.003924 -  exact_match : 12.71523178807947  F1 : 17.203398238494966 
====================================  END results/models/test/checkpoints/ckpt-8494  ====================================
==================================== START results/models/test/checkpoints/ckpt-8994 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/test/checkpoints/ckpt-8994 at step 8993
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/test/checkpoints/discriminator/config.json
Model weights saved in results/models/test/checkpoints/discriminator/tf_model.h5: {'electra', 'discriminator_predictions'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=results/models/test/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-22 17:48:04.784535: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-22 17:48:05.645659: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-22 17:48:05.661836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.662355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 17:48:05.662388: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:48:05.663849: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 17:48:05.664500: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 17:48:05.664664: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 17:48:05.666252: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 17:48:05.666620: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 17:48:05.666715: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:48:05.666760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.667245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.667690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 17:48:05.672725: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-22 17:48:05.673238: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0d64000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-22 17:48:05.673248: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-22 17:48:05.816857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.817371: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0c78000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-22 17:48:05.817384: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-22 17:48:05.817566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.818012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 17:48:05.818028: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:48:05.818050: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 17:48:05.818058: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 17:48:05.818083: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 17:48:05.818093: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 17:48:05.818102: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 17:48:05.818110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:48:05.818172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.818639: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.819069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 17:48:05.819084: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 17:48:05.985865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-22 17:48:05.985888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-22 17:48:05.985892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-22 17:48:05.986010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.986464: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 17:48:05.986883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-22 17:48:05.645018 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: results/models/test/checkpoints/discriminator
loading configuration file results/models/test/checkpoints/discriminator/config.json
loading weights file results/models/test/checkpoints/discriminator/tf_model.h5
2022-02-22 17:48:06.054381: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-22 17:48:06.093863: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at results/models/test/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at results/models/test/checkpoints/discriminator and are newly initialized: ['end_logits', 'start_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 36%|███▋      | 161/442 [00:05<00:08, 32.18it/s] 67%|██████▋   | 296/442 [00:10<00:04, 30.42it/s] 98%|█████████▊| 434/442 [00:15<00:00, 29.50it/s]100%|██████████| 442/442 [00:15<00:00, 28.91it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 26.55it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-22 17:49:01.244339: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-22 17:49:02.111751: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 17:49:02.137942: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 17:49:02.752283: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-22 17:49:03.036874: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-22 17:52:11.523649: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:14.808414: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:52:14.808433: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:52:14.808439: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:52:14.808442: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:52:14.808444: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:52:14.808445: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:52:14.822793: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-22 17:52:16.952942: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:17.098706: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:52:17.098724: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:52:17.098729: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:52:17.098732: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:52:17.098734: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:52:17.098735: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:52:17.157626: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:17.345886: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:52:17.345904: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:52:17.345910: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:52:17.345912: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:52:17.345914: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:52:17.345916: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:52:17.582350: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:18.379717: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:52:18.379736: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:52:18.379743: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:52:18.379745: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:52:18.379747: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:52:18.379749: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.28492641, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:25<158:27:47, 205.94s/it]2022-02-22 17:52:21.545439: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 17:52:22.896017: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:26.282302: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:52:26.282320: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:52:26.282326: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:52:26.282328: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:52:26.282330: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:52:26.282332: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 17:52:28.283290: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:28.370600: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:28.668815: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:29.703643: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:29.826364: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:52:29.826381: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:52:29.826386: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:52:29.826388: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:52:29.826390: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:52:29.826392: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:36<113:25:47, 147.47s/it]DLL 2022-02-22 17:52:18.831241 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.284926414489746  train_perf : 0.15560726821422577 

Epoch: 000, Step:    50, Loss:  4.99146748, Perf:  471, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:42<77:14:11, 103.25s/it]DLL 2022-02-22 17:52:33.129009 - Training Epoch: 0 Training Iteration: 50  step_loss : 4.991467475891113  train_perf : 470.919921875 

Epoch: 000, Step:   100, Loss:  4.14232588, Perf:  483, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-22 17:52:36.427221 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.1423258781433105  train_perf : 482.7984313964844 

Epoch: 000, Step:   150, Loss:  3.61217666, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 154/2771 [03:47<52:33:15, 72.29s/it]DLL 2022-02-22 17:52:39.729174 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.6121766567230225  train_perf : 486.600830078125 

Epoch: 000, Step:   200, Loss:  3.68854833, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 230/2771 [03:52<35:44:00, 50.63s/it]DLL 2022-02-22 17:52:43.025628 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.6885483264923096  train_perf : 488.7204284667969 

Epoch: 000, Step:   250, Loss:  3.35993004, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-22 17:52:48.108185: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:52:48.242608: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:52:48.242626: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:52:48.242633: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:52:48.242635: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:52:48.242636: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:52:48.242638: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 17:52:46.328577 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.3599300384521484  train_perf : 489.7816162109375 

Epoch: 000, Step:   300, Loss:  3.12130189, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 303/2771 [03:57<24:18:32, 35.46s/it]DLL 2022-02-22 17:52:49.807400 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.1213018894195557  train_perf : 489.11883544921875 

Epoch: 000, Step:   350, Loss:  3.03838253, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 379/2771 [04:02<16:30:19, 24.84s/it]DLL 2022-02-22 17:52:53.121765 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.0383825302124023  train_perf : 489.5935363769531 

Epoch: 000, Step:   400, Loss:  3.12586188, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-22 17:52:56.433888 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.125861883163452  train_perf : 489.9636535644531 

Epoch: 000, Step:   450, Loss:  2.82059121, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 455/2771 [04:07<11:11:58, 17.41s/it]DLL 2022-02-22 17:52:59.753239 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.8205912113189697  train_perf : 490.1470031738281 

Epoch: 000, Step:   500, Loss:  2.93875217, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 531/2771 [04:12<7:35:41, 12.21s/it] DLL 2022-02-22 17:53:03.069102 - Training Epoch: 0 Training Iteration: 500  step_loss : 2.9387521743774414  train_perf : 490.33624267578125 

Epoch: 000, Step:   550, Loss:  3.13769555, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-22 17:53:06.397281 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.137695550918579  train_perf : 490.34527587890625 

Epoch: 000, Step:   600, Loss:  2.86440063, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 607/2771 [04:17<5:08:52,  8.56s/it]DLL 2022-02-22 17:53:09.710754 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.864400625228882  train_perf : 490.50067138671875 

Epoch: 000, Step:   650, Loss:  3.45901394, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 683/2771 [04:22<3:29:18,  6.01s/it]DLL 2022-02-22 17:53:13.031765 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.4590139389038086  train_perf : 490.5641174316406 

Epoch: 000, Step:   700, Loss:  3.02975821, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
DLL 2022-02-22 17:53:16.352197 - Training Epoch: 0 Training Iteration: 700  step_loss : 3.0297582149505615  train_perf : 490.6248779296875 

Epoch: 000, Step:   750, Loss:  2.67453694, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:751
Iteration:  27%|██▋       | 759/2771 [04:27<2:21:51,  4.23s/it]DLL 2022-02-22 17:53:19.675476 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.674536943435669  train_perf : 490.62554931640625 

Epoch: 000, Step:   800, Loss:  2.87209058, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:801
Iteration:  30%|███       | 835/2771 [04:32<1:36:11,  2.98s/it]DLL 2022-02-22 17:53:22.994476 - Training Epoch: 0 Training Iteration: 800  step_loss : 2.8720905780792236  train_perf : 490.67822265625 

Epoch: 000, Step:   850, Loss:  3.49829578, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:851
DLL 2022-02-22 17:53:26.323157 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.498295783996582  train_perf : 490.64483642578125 

Epoch: 000, Step:   900, Loss:  2.64317131, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:901
Iteration:  33%|███▎      | 911/2771 [04:37<1:05:18,  2.11s/it]DLL 2022-02-22 17:53:29.643767 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.6431713104248047  train_perf : 490.67193603515625 

Epoch: 000, Step:   950, Loss:  3.32264733, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:951
Iteration:  36%|███▌      | 987/2771 [04:42<44:26,  1.49s/it]  DLL 2022-02-22 17:53:32.972697 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.3226473331451416  train_perf : 490.6199951171875 

Epoch: 000, Step:  1000, Loss:  3.10276222, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1001, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1001
DLL 2022-02-22 17:53:36.296501 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.102762222290039  train_perf : 490.62628173828125 

Epoch: 000, Step:  1050, Loss:  3.22293687, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1051, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1051
Iteration:  38%|███▊      | 1063/2771 [04:47<30:21,  1.07s/it]DLL 2022-02-22 17:53:39.617592 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.2229368686676025  train_perf : 490.6407165527344 

Epoch: 000, Step:  1100, Loss:  3.40380096, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1101
Iteration:  41%|████      | 1139/2771 [04:52<20:50,  1.30it/s]DLL 2022-02-22 17:53:42.943598 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.4038009643554688  train_perf : 490.61669921875 

Epoch: 000, Step:  1150, Loss:  3.52605534, Perf:  491, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1151
DLL 2022-02-22 17:53:46.275907 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.526055335998535  train_perf : 490.5709228515625 

Epoch: 000, Step:  1200, Loss:  2.91832590, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1201
Iteration:  44%|████▍     | 1214/2771 [04:57<14:26,  1.80it/s]DLL 2022-02-22 17:53:49.616383 - Training Epoch: 0 Training Iteration: 1200  step_loss : 2.918325901031494  train_perf : 490.4813232421875 

Epoch: 000, Step:  1250, Loss:  3.39893389, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1251
Iteration:  47%|████▋     | 1289/2771 [05:02<10:06,  2.44it/s]DLL 2022-02-22 17:53:52.955046 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.3989338874816895  train_perf : 490.39862060546875 

Epoch: 000, Step:  1300, Loss:  3.16876912, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1301
DLL 2022-02-22 17:53:56.294673 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.168769121170044  train_perf : 490.3245544433594 

Epoch: 000, Step:  1350, Loss:  3.01106095, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1351
Iteration:  49%|████▉     | 1364/2771 [05:07<07:11,  3.26it/s]DLL 2022-02-22 17:53:59.637424 - Training Epoch: 0 Training Iteration: 1350  step_loss : 3.011060953140259  train_perf : 490.2284851074219 

Epoch: 000, Step:  1400, Loss:  3.22053862, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1401
Iteration:  52%|█████▏    | 1439/2771 [05:12<05:12,  4.26it/s]DLL 2022-02-22 17:54:02.981415 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.22053861618042  train_perf : 490.13629150390625 

Epoch: 000, Step:  1450, Loss:  2.86833239, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1451
DLL 2022-02-22 17:54:06.328525 - Training Epoch: 0 Training Iteration: 1450  step_loss : 2.8683323860168457  train_perf : 490.0460510253906 

Epoch: 000, Step:  1500, Loss:  3.57054996, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1501
Iteration:  55%|█████▍    | 1514/2771 [05:17<03:51,  5.42it/s]DLL 2022-02-22 17:54:09.669358 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.570549964904785  train_perf : 489.9849548339844 

Epoch: 000, Step:  1550, Loss:  3.16020441, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1551
Iteration:  57%|█████▋    | 1589/2771 [05:22<02:56,  6.71it/s]DLL 2022-02-22 17:54:13.009717 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.1602044105529785  train_perf : 489.9266052246094 

Epoch: 000, Step:  1600, Loss:  3.10179377, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1601
DLL 2022-02-22 17:54:16.350669 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.1017937660217285  train_perf : 489.869140625 

Epoch: 000, Step:  1650, Loss:  3.17399359, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1651
Iteration:  60%|██████    | 1664/2771 [05:27<02:17,  8.03it/s]DLL 2022-02-22 17:54:19.697773 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.1739935874938965  train_perf : 489.78662109375 

Epoch: 000, Step:  1700, Loss:  2.92504978, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1701
Iteration:  63%|██████▎   | 1739/2771 [05:32<01:50,  9.33it/s]DLL 2022-02-22 17:54:23.039464 - Training Epoch: 0 Training Iteration: 1700  step_loss : 2.9250497817993164  train_perf : 489.7374267578125 

Epoch: 000, Step:  1750, Loss:  3.22493982, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1751, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1751
DLL 2022-02-22 17:54:26.384777 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.2249398231506348  train_perf : 489.6792907714844 

Epoch: 000, Step:  1800, Loss:  2.94582152, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1801, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1801
Iteration:  65%|██████▌   | 1814/2771 [05:37<01:31, 10.51it/s]DLL 2022-02-22 17:54:29.734684 - Training Epoch: 0 Training Iteration: 1800  step_loss : 2.945821523666382  train_perf : 489.60369873046875 

Epoch: 000, Step:  1850, Loss:  2.82689190, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1851, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1851
Iteration:  68%|██████▊   | 1889/2771 [05:42<01:16, 11.54it/s]DLL 2022-02-22 17:54:33.079636 - Training Epoch: 0 Training Iteration: 1850  step_loss : 2.8268918991088867  train_perf : 489.5503234863281 

Epoch: 000, Step:  1900, Loss:  2.81954908, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1901, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1901
DLL 2022-02-22 17:54:36.417842 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.819549083709717  train_perf : 489.5262451171875 

Epoch: 000, Step:  1950, Loss:  2.68386006, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1951, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1951
Iteration:  71%|███████   | 1964/2771 [05:47<01:05, 12.39it/s]2022-02-22 17:54:43.036223: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:54:43.158803: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:54:43.158821: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:54:43.158827: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:54:43.158829: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:54:43.158831: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:54:43.158833: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 17:54:39.756896 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.6838600635528564  train_perf : 489.49884033203125 

Epoch: 000, Step:  2000, Loss:  3.06624985, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2001
Iteration:  74%|███████▎  | 2039/2771 [05:52<00:56, 12.95it/s]DLL 2022-02-22 17:54:43.261416 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.0662498474121094  train_perf : 489.2718200683594 

Epoch: 000, Step:  2050, Loss:  2.91881680, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2051
DLL 2022-02-22 17:54:46.618177 - Training Epoch: 0 Training Iteration: 2050  step_loss : 2.9188168048858643  train_perf : 489.1979064941406 

Epoch: 000, Step:  2100, Loss:  3.25948048, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2101
Iteration:  76%|███████▋  | 2114/2771 [05:57<00:48, 13.49it/s]DLL 2022-02-22 17:54:49.957582 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.2594804763793945  train_perf : 489.1788635253906 

Epoch: 000, Step:  2150, Loss:  3.01839352, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2151
2022-02-22 17:54:55.168091: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 17:54:55.289302: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 17:54:55.289321: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 17:54:55.289345: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 17:54:55.289347: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 17:54:55.289349: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 17:54:55.289351: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:  79%|███████▉  | 2189/2771 [06:03<00:42, 13.78it/s]DLL 2022-02-22 17:54:53.303749 - Training Epoch: 0 Training Iteration: 2150  step_loss : 3.0183935165405273  train_perf : 489.1388854980469 

Epoch: 000, Step:  2200, Loss:  3.07808709, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=22, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-22 17:54:56.799446 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.078087091445923  train_perf : 488.9493408203125 

Epoch: 000, Step:  2250, Loss:  2.86685514, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=72, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  82%|████████▏ | 2264/2771 [06:08<00:35, 14.11it/s]DLL 2022-02-22 17:55:00.147977 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.8668551445007324  train_perf : 488.90692138671875 

Epoch: 000, Step:  2300, Loss:  3.02957249, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=122, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  84%|████████▍ | 2339/2771 [06:13<00:30, 14.35it/s]DLL 2022-02-22 17:55:03.492106 - Training Epoch: 0 Training Iteration: 2300  step_loss : 3.0295724868774414  train_perf : 488.8816223144531 

Epoch: 000, Step:  2350, Loss:  2.51363397, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=172, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-22 17:55:06.836037 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.513633966445923  train_perf : 488.854736328125 

Epoch: 000, Step:  2400, Loss:  2.79389286, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=222, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2414/2771 [06:18<00:24, 14.53it/s]DLL 2022-02-22 17:55:10.177179 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.7938928604125977  train_perf : 488.8354797363281 

Epoch: 000, Step:  2450, Loss:  3.10447860, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=272, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2489/2771 [06:23<00:19, 14.66it/s]DLL 2022-02-22 17:55:13.517067 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.104478597640991  train_perf : 488.8209533691406 

Epoch: 000, Step:  2500, Loss:  3.29721642, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=322, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-22 17:55:16.856149 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.2972164154052734  train_perf : 488.8126220703125 

Epoch: 000, Step:  2550, Loss:  2.97860909, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=372, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  93%|█████████▎| 2564/2771 [06:28<00:14, 14.76it/s]DLL 2022-02-22 17:55:20.190255 - Training Epoch: 0 Training Iteration: 2550  step_loss : 2.978609085083008  train_perf : 488.8150329589844 

Epoch: 000, Step:  2600, Loss:  3.05902267, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=422, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2639/2771 [06:33<00:08, 14.81it/s]DLL 2022-02-22 17:55:23.535196 - Training Epoch: 0 Training Iteration: 2600  step_loss : 3.0590226650238037  train_perf : 488.7901306152344 

Epoch: 000, Step:  2650, Loss:  2.91485643, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=472, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-22 17:55:26.881361 - Training Epoch: 0 Training Iteration: 2650  step_loss : 2.914856433868408  train_perf : 488.7618408203125 

Epoch: 000, Step:  2700, Loss:  3.01374292, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=522, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
Iteration:  98%|█████████▊| 2714/2771 [06:38<00:03, 14.86it/s]DLL 2022-02-22 17:55:30.219347 - Training Epoch: 0 Training Iteration: 2700  step_loss : 3.0137429237365723  train_perf : 488.7530517578125 

Epoch: 000, Step:  2750, Loss:  2.98001504, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=572, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:41<00:00,  6.89it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-22 17:55:33.560216 - Training Epoch: 0 Training Iteration: 2750  step_loss : 2.9800150394439697  train_perf : 488.7411804199219 
DLL 2022-02-22 17:55:34.832515 -  e2e_train_time : 401.94547152519226  training_sequences_per_second : 488.7351989746094  final_loss : 3.1726644039154053 

Epoch: 001, Step:     0, Loss:  2.44800019, Perf:  459, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=592, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-22 17:55:44.966228 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.448000192642212  train_perf : 458.7385559082031 

Epoch: 001, Step:    50, Loss:  2.52193546, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=642, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
Iteration:   3%|▎         | 71/2771 [00:05<03:11, 14.09it/s]DLL 2022-02-22 17:55:48.300812 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.52193546295166  train_perf : 488.2442626953125 

Epoch: 001, Step:   100, Loss:  2.41080832, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=692, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 147/2771 [00:10<03:02, 14.37it/s]DLL 2022-02-22 17:55:51.626611 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.4108083248138428  train_perf : 489.26141357421875 

Epoch: 001, Step:   150, Loss:  2.31829762, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=742, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-22 17:55:54.947381 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.3182976245880127  train_perf : 489.857421875 

Epoch: 001, Step:   200, Loss:  2.35846519, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=792, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 223/2771 [00:15<02:54, 14.56it/s]DLL 2022-02-22 17:55:58.272284 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.3584651947021484  train_perf : 489.927734375 

Epoch: 001, Step:   250, Loss:  2.47234106, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=842, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 298/2771 [00:20<02:48, 14.68it/s]DLL 2022-02-22 17:56:01.600213 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.4723410606384277  train_perf : 489.9753112792969 

Epoch: 001, Step:   300, Loss:  2.43146753, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=892, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-22 17:56:04.944262 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.4314675331115723  train_perf : 489.5741271972656 

Epoch: 001, Step:   350, Loss:  2.35009694, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=942, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  13%|█▎        | 373/2771 [00:25<02:42, 14.77it/s]DLL 2022-02-22 17:56:08.278871 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.3500969409942627  train_perf : 489.47998046875 

Epoch: 001, Step:   400, Loss:  1.94935894, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=992, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 448/2771 [00:30<02:36, 14.84it/s]DLL 2022-02-22 17:56:11.618563 - Training Epoch: 1 Training Iteration: 400  step_loss : 1.9493589401245117  train_perf : 489.3250427246094 

Epoch: 001, Step:   450, Loss:  2.13692141, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1042, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-22 17:56:14.948462 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.1369214057922363  train_perf : 489.3434143066406 

Epoch: 001, Step:   500, Loss:  2.46163440, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1092, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▉        | 523/2771 [00:35<02:31, 14.88it/s]DLL 2022-02-22 17:56:18.287281 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.461634397506714  train_perf : 489.25262451171875 

Epoch: 001, Step:   550, Loss:  2.11158085, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1142, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
Iteration:  22%|██▏       | 599/2771 [00:40<02:25, 14.92it/s]DLL 2022-02-22 17:56:21.619249 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.1115808486938477  train_perf : 489.24261474609375 

Epoch: 001, Step:   600, Loss:  1.90360272, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1192, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
DLL 2022-02-22 17:56:24.948600 - Training Epoch: 1 Training Iteration: 600  step_loss : 1.9036027193069458  train_perf : 489.259765625 

Epoch: 001, Step:   650, Loss:  2.84553528, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1242, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 674/2771 [00:45<02:20, 14.94it/s]DLL 2022-02-22 17:56:28.281895 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.8455352783203125  train_perf : 489.2395935058594 

Epoch: 001, Step:   700, Loss:  2.38391829, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1292, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
Iteration:  27%|██▋       | 749/2771 [00:50<02:15, 14.95it/s]DLL 2022-02-22 17:56:31.619557 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.383918285369873  train_perf : 489.1770324707031 

Epoch: 001, Step:   750, Loss:  2.02397370, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1342, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3520
DLL 2022-02-22 17:56:34.960108 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.0239737033843994  train_perf : 489.103515625 

Epoch: 001, Step:   800, Loss:  2.83660793, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1392, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3570
Iteration:  30%|██▉       | 824/2771 [00:55<02:10, 14.96it/s]DLL 2022-02-22 17:56:38.297260 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.8366079330444336  train_perf : 489.0612487792969 

Epoch: 001, Step:   850, Loss:  2.87843323, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1442, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3620
Iteration:  32%|███▏      | 899/2771 [01:00<02:05, 14.96it/s]DLL 2022-02-22 17:56:41.642850 - Training Epoch: 1 Training Iteration: 850  step_loss : 2.8784332275390625  train_perf : 488.95379638671875 

Epoch: 001, Step:   900, Loss:  2.51648545, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1492, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3670
DLL 2022-02-22 17:56:44.981248 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.5164854526519775  train_perf : 488.916259765625 

Epoch: 001, Step:   950, Loss:  2.68426466, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1542, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3720
Iteration:  35%|███▌      | 974/2771 [01:05<02:00, 14.96it/s]DLL 2022-02-22 17:56:48.321036 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.684264659881592  train_perf : 488.8761291503906 

Epoch: 001, Step:  1000, Loss:  2.67085838, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1592, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3770
Iteration:  38%|███▊      | 1049/2771 [01:10<01:55, 14.97it/s]DLL 2022-02-22 17:56:51.654368 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.670858383178711  train_perf : 488.8772888183594 

Epoch: 001, Step:  1050, Loss:  2.85506701, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1642, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3820
DLL 2022-02-22 17:56:54.993331 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.855067014694214  train_perf : 488.8510437011719 

Epoch: 001, Step:  1100, Loss:  3.38009501, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1692, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3870
Iteration:  41%|████      | 1124/2771 [01:15<01:50, 14.97it/s]DLL 2022-02-22 17:56:58.334454 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.3800950050354004  train_perf : 488.8049621582031 

Epoch: 001, Step:  1150, Loss:  2.86121893, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1742, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3920
Iteration:  43%|████▎     | 1199/2771 [01:20<01:45, 14.96it/s]DLL 2022-02-22 17:57:01.675929 - Training Epoch: 1 Training Iteration: 1150  step_loss : 2.8612189292907715  train_perf : 488.7657470703125 

Epoch: 001, Step:  1200, Loss:  2.47326612, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1792, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3970
DLL 2022-02-22 17:57:05.024067 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.473266124725342  train_perf : 488.6904602050781 

Epoch: 001, Step:  1250, Loss:  2.90917826, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1842, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4020
Iteration:  46%|████▌     | 1274/2771 [01:25<01:40, 14.96it/s]DLL 2022-02-22 17:57:08.369702 - Training Epoch: 1 Training Iteration: 1250  step_loss : 2.9091782569885254  train_perf : 488.627685546875 

Epoch: 001, Step:  1300, Loss:  2.57129860, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1892, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4070
Iteration:  49%|████▊     | 1349/2771 [01:30<01:35, 14.96it/s]DLL 2022-02-22 17:57:11.713636 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.571298599243164  train_perf : 488.5785827636719 

Epoch: 001, Step:  1350, Loss:  2.57682562, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1942, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4120
DLL 2022-02-22 17:57:15.055658 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.5768256187438965  train_perf : 488.5551452636719 

Epoch: 001, Step:  1400, Loss:  2.49990463, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1992, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4170
Iteration:  51%|█████▏    | 1424/2771 [01:35<01:30, 14.95it/s]DLL 2022-02-22 17:57:18.402688 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.4999046325683594  train_perf : 488.5028991699219 

Epoch: 001, Step:  1450, Loss:  2.31716299, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=42, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4220
Iteration:  54%|█████▍    | 1499/2771 [01:40<01:25, 14.96it/s]DLL 2022-02-22 17:57:21.745607 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.3171629905700684  train_perf : 488.46826171875 

Epoch: 001, Step:  1500, Loss:  3.37454724, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=92, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4270
DLL 2022-02-22 17:57:25.082776 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.374547243118286  train_perf : 488.4607238769531 

Epoch: 001, Step:  1550, Loss:  2.34083223, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=142, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4320
Iteration:  57%|█████▋    | 1574/2771 [01:45<01:20, 14.96it/s]DLL 2022-02-22 17:57:28.425660 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.340832233428955  train_perf : 488.4300537109375 

Epoch: 001, Step:  1600, Loss:  2.62055898, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=192, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4370
Iteration:  60%|█████▉    | 1649/2771 [01:50<01:15, 14.96it/s]DLL 2022-02-22 17:57:31.775743 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.620558977127075  train_perf : 488.3724670410156 

Epoch: 001, Step:  1650, Loss:  2.59621072, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=49, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4419
DLL 2022-02-22 17:57:35.115709 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.5962107181549072  train_perf : 488.3630065917969 

Epoch: 001, Step:  1700, Loss:  2.50322580, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=99, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4469
Iteration:  62%|██████▏   | 1724/2771 [01:55<01:09, 14.96it/s]DLL 2022-02-22 17:57:38.456492 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.503225803375244  train_perf : 488.3500061035156 

Epoch: 001, Step:  1750, Loss:  2.79674983, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=149, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4519
Iteration:  65%|██████▍   | 1799/2771 [02:00<01:04, 14.96it/s]DLL 2022-02-22 17:57:41.795883 - Training Epoch: 1 Training Iteration: 1750  step_loss : 2.7967498302459717  train_perf : 488.3349304199219 

Epoch: 001, Step:  1800, Loss:  2.43849516, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=199, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4569
DLL 2022-02-22 17:57:45.144475 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.43849515914917  train_perf : 488.29443359375 

Epoch: 001, Step:  1850, Loss:  2.59668899, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=249, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4619
Iteration:  68%|██████▊   | 1874/2771 [02:05<00:59, 14.95it/s]DLL 2022-02-22 17:57:48.485481 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.596688985824585  train_perf : 488.2823181152344 

Epoch: 001, Step:  1900, Loss:  2.50380564, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=299, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4669
Iteration:  70%|███████   | 1949/2771 [02:10<00:54, 14.95it/s]DLL 2022-02-22 17:57:51.835089 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.503805637359619  train_perf : 488.2420654296875 

Epoch: 001, Step:  1950, Loss:  2.39976645, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=349, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4719
DLL 2022-02-22 17:57:55.179366 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.399766445159912  train_perf : 488.2130432128906 

Epoch: 001, Step:  2000, Loss:  2.62275982, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=399, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4769
Iteration:  73%|███████▎  | 2024/2771 [02:15<00:49, 14.96it/s]DLL 2022-02-22 17:57:58.518015 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.6227598190307617  train_perf : 488.2137756347656 

Epoch: 001, Step:  2050, Loss:  2.32066607, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=449, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
Iteration:  76%|███████▌  | 2099/2771 [02:20<00:44, 14.95it/s]DLL 2022-02-22 17:58:01.861880 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.3206660747528076  train_perf : 488.1947021484375 

Epoch: 001, Step:  2100, Loss:  2.94281101, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=499, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-22 17:58:05.211707 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.9428110122680664  train_perf : 488.1509704589844 

Epoch: 001, Step:  2150, Loss:  2.67457676, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=549, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2174/2771 [02:25<00:39, 14.95it/s]DLL 2022-02-22 17:58:08.552831 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.674576759338379  train_perf : 488.1419677734375 

Epoch: 001, Step:  2200, Loss:  2.45054698, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=599, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2249/2771 [02:30<00:34, 14.95it/s]DLL 2022-02-22 17:58:11.904576 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.450546979904175  train_perf : 488.1017761230469 

Epoch: 001, Step:  2250, Loss:  2.40462255, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=649, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-22 17:58:15.246041 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.4046225547790527  train_perf : 488.0876159667969 

Epoch: 001, Step:  2300, Loss:  2.41117382, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=699, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▍ | 2324/2771 [02:35<00:29, 14.95it/s]DLL 2022-02-22 17:58:18.590343 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.4111738204956055  train_perf : 488.0629577636719 

Epoch: 001, Step:  2350, Loss:  2.19781542, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=749, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  87%|████████▋ | 2399/2771 [02:40<00:24, 14.95it/s]DLL 2022-02-22 17:58:21.937876 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.197815418243408  train_perf : 488.03668212890625 

Epoch: 001, Step:  2400, Loss:  2.34049249, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=799, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-22 17:58:25.283327 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.3404924869537354  train_perf : 488.0156555175781 

Epoch: 001, Step:  2450, Loss:  2.82134700, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=849, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2474/2771 [02:45<00:19, 14.95it/s]DLL 2022-02-22 17:58:28.629910 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.8213469982147217  train_perf : 487.9945068359375 

Epoch: 001, Step:  2500, Loss:  2.71424198, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=899, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2549/2771 [02:50<00:14, 14.94it/s]DLL 2022-02-22 17:58:31.976780 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.7142419815063477  train_perf : 487.974365234375 

Epoch: 001, Step:  2550, Loss:  2.61231899, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=949, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-22 17:58:35.323947 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.612318992614746  train_perf : 487.9535827636719 

Epoch: 001, Step:  2600, Loss:  2.96060514, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=999, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▍| 2624/2771 [02:55<00:09, 14.95it/s]DLL 2022-02-22 17:58:38.663713 - Training Epoch: 1 Training Iteration: 2600  step_loss : 2.9606051445007324  train_perf : 487.9481506347656 

Epoch: 001, Step:  2650, Loss:  2.75023937, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1049, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2699/2771 [03:00<00:04, 14.95it/s]DLL 2022-02-22 17:58:42.012399 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.750239372253418  train_perf : 487.9254150390625 

Epoch: 001, Step:  2700, Loss:  2.75411844, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1099, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-22 17:58:45.354399 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.7541184425354004  train_perf : 487.92193603515625 

Epoch: 001, Step:  2750, Loss:  2.61361241, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1149, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.94it/s]DLL 2022-02-22 17:58:48.700706 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.613612413406372  train_perf : 487.9049072265625 
DLL 2022-02-22 17:58:49.972263 -  e2e_train_time : 587.3234784603119  training_sequences_per_second : 487.8994140625  final_loss : 2.5922861099243164 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-22 17:58:58.557525: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 18:00:26.078129: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:00:27.132099: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:00:27.132120: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:00:27.132128: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:00:27.132132: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:00:27.132135: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:00:27.132138: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:32<32:30, 92.87s/it]Iteration:  14%|█▎        | 3/22 [01:38<20:51, 65.88s/it]Iteration:  23%|██▎       | 5/22 [01:44<13:19, 47.00s/it]Iteration:  32%|███▏      | 7/22 [01:50<08:27, 33.80s/it]Iteration:  41%|████      | 9/22 [01:56<05:19, 24.58s/it]Iteration:  50%|█████     | 11/22 [02:02<03:19, 18.13s/it]Iteration:  59%|█████▉    | 13/22 [02:09<02:02, 13.64s/it]Iteration:  68%|██████▊   | 15/22 [02:15<01:13, 10.51s/it]Iteration:  77%|███████▋  | 17/22 [02:22<00:41,  8.33s/it]Iteration:  86%|████████▋ | 19/22 [02:28<00:20,  6.81s/it]Iteration:  95%|█████████▌| 21/22 [02:35<00:05,  5.76s/it]2022-02-22 18:01:33.478575: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.76s/it]2022-02-22 18:02:45.755784: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:02:46.826950: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:02:46.826969: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:02:46.826975: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:02:46.826978: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:02:46.826980: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:02:46.826982: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:50<00:00, 26.53s/it]Iteration: 100%|██████████| 22/22 [03:50<00:00, 10.47s/it]{"exact_match": 13.093661305581834, "f1": 17.383309668830346}

Epoch: 001 Results: {"exact_match": 13.093661305581834, "f1": 17.383309668830346}

**EVAL SUMMARY** - Epoch: 001,  EM: 13.094, F1: 17.383, Infer_Perf: 1050 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 443.618 ms, 90%: 443.127 ms, 95%: 443.361 ms, 99%: 443.361 ms
DLL 2022-02-22 18:03:05.275496 -  inference_sequences_per_second : 1049.8516845703125  e2e_inference_time : 244.7960500717163 
**RESULTS SUMMARY** - EM: 13.094, F1: 17.383, Train_Time:  587 s, Train_Perf:  488 seq/s, Infer_Perf: 1050 seq/s

DLL 2022-02-22 18:03:05.276268 -  exact_match : 13.093661305581834  F1 : 17.383309668830346 
====================================  END results/models/test/checkpoints/ckpt-8994  ====================================
==================================== START results/models/test/checkpoints/ckpt-9494 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/test/checkpoints/ckpt-9494 at step 9493
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/test/checkpoints/discriminator/config.json
Model weights saved in results/models/test/checkpoints/discriminator/tf_model.h5: {'electra', 'discriminator_predictions'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=results/models/test/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-22 18:03:12.903044: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-22 18:03:13.761814: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-22 18:03:13.778183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:13.778640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 18:03:13.778657: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 18:03:13.780094: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 18:03:13.780677: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 18:03:13.780819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 18:03:13.782361: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 18:03:13.782701: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 18:03:13.782789: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 18:03:13.782837: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:13.783293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:13.783715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 18:03:13.788708: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-22 18:03:13.789162: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc494000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-22 18:03:13.789174: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-22 18:03:13.931177: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:13.931677: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc398000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-22 18:03:13.931688: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-22 18:03:13.931791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:13.932214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 18:03:13.932232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 18:03:13.932257: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 18:03:13.932269: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 18:03:13.932280: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 18:03:13.932291: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 18:03:13.932301: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 18:03:13.932311: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 18:03:13.932340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:13.932773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:13.933184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 18:03:13.933201: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 18:03:14.104544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-22 18:03:14.104566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-22 18:03:14.104571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-22 18:03:14.104686: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:14.105172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:03:14.105661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-22 18:03:13.761188 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: results/models/test/checkpoints/discriminator
loading configuration file results/models/test/checkpoints/discriminator/config.json
loading weights file results/models/test/checkpoints/discriminator/tf_model.h5
2022-02-22 18:03:14.171586: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-22 18:03:14.211336: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at results/models/test/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at results/models/test/checkpoints/discriminator and are newly initialized: ['end_logits', 'start_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 36%|███▌      | 158/442 [00:05<00:09, 31.39it/s] 65%|██████▍   | 286/442 [00:10<00:05, 29.37it/s] 96%|█████████▌| 423/442 [00:15<00:00, 28.75it/s]100%|██████████| 442/442 [00:15<00:00, 28.03it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 25.67it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-22 18:04:10.510162: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-22 18:04:11.389355: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 18:04:11.415462: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 18:04:12.035477: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-22 18:04:12.330765: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-22 18:07:25.173339: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:28.527689: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:07:28.527709: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:07:28.527715: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:07:28.527718: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:07:28.527720: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:07:28.527722: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 18:07:28.541805: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-22 18:07:30.542013: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:30.687322: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:07:30.687340: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:07:30.687345: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:07:30.687347: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:07:30.687349: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:07:30.687351: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 18:07:30.745900: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:30.936080: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:07:30.936099: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:07:30.936105: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:07:30.936108: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:07:30.936109: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:07:30.936111: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 18:07:31.169226: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:31.951481: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:07:31.951500: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:07:31.951507: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:07:31.951509: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:07:31.951511: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:07:31.951513: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.30202627, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:30<161:50:18, 210.33s/it]2022-02-22 18:07:35.109717: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 18:07:36.440321: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:39.768708: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:07:39.768730: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:07:39.768739: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:07:39.768743: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:07:39.768746: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:07:39.768749: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 18:07:41.733692: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:41.821614: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:42.117985: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:43.138129: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:07:43.265752: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:07:43.265771: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:07:43.265778: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:07:43.265782: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:07:43.265784: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:07:43.265787: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:41<115:45:49, 150.51s/it]DLL 2022-02-22 18:07:32.393250 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.302026271820068  train_perf : 0.1523638367652893 

Epoch: 000, Step:    50, Loss:  4.88154411, Perf:  469, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:46<78:49:31, 105.37s/it]DLL 2022-02-22 18:07:46.581595 - Training Epoch: 0 Training Iteration: 50  step_loss : 4.88154411315918  train_perf : 469.08160400390625 

Epoch: 000, Step:   100, Loss:  4.11470222, Perf:  482, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-22 18:07:49.876807 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.114702224731445  train_perf : 482.0440979003906 

Epoch: 000, Step:   150, Loss:  3.51568127, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 154/2771 [03:51<53:38:05, 73.78s/it]DLL 2022-02-22 18:07:53.178398 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.515681266784668  train_perf : 486.0972595214844 

Epoch: 000, Step:   200, Loss:  3.65312552, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 230/2771 [03:56<36:28:05, 51.67s/it]DLL 2022-02-22 18:07:56.469117 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.653125524520874  train_perf : 488.5120849609375 

Epoch: 000, Step:   250, Loss:  3.33433247, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-22 18:08:01.549087: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:08:01.670527: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:08:01.670546: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:08:01.670552: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:08:01.670555: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:08:01.670556: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:08:01.670558: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 18:07:59.773883 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.3343324661254883  train_perf : 489.5701904296875 

Epoch: 000, Step:   300, Loss:  3.27958226, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 304/2771 [04:01<24:47:53, 36.19s/it]DLL 2022-02-22 18:08:03.232941 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.2795822620391846  train_perf : 489.138916015625 

Epoch: 000, Step:   350, Loss:  3.17337894, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 380/2771 [04:06<16:50:14, 25.35s/it]DLL 2022-02-22 18:08:06.549228 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.1733789443969727  train_perf : 489.5899963378906 

Epoch: 000, Step:   400, Loss:  3.13598180, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-22 18:08:09.873480 - Training Epoch: 0 Training Iteration: 400  step_loss : 3.135981798171997  train_perf : 489.7265319824219 

Epoch: 000, Step:   450, Loss:  2.77745676, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 456/2771 [04:11<11:25:27, 17.77s/it]DLL 2022-02-22 18:08:13.198853 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.777456760406494  train_perf : 489.8231506347656 

Epoch: 000, Step:   500, Loss:  2.97043371, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=501, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:501
Iteration:  19%|█▉        | 532/2771 [04:16<7:44:48, 12.46s/it] DLL 2022-02-22 18:08:16.517116 - Training Epoch: 0 Training Iteration: 500  step_loss : 2.9704337120056152  train_perf : 490.005615234375 

Epoch: 000, Step:   550, Loss:  3.14532590, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=551, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:551
DLL 2022-02-22 18:08:19.837782 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.1453258991241455  train_perf : 490.11517333984375 

Epoch: 000, Step:   600, Loss:  2.82342720, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=601, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:601
Iteration:  22%|██▏       | 608/2771 [04:21<5:15:02,  8.74s/it]DLL 2022-02-22 18:08:23.158925 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.823427200317383  train_perf : 490.1943054199219 

Epoch: 000, Step:   650, Loss:  3.15542150, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=651, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:651
Iteration:  25%|██▍       | 684/2771 [04:26<3:33:28,  6.14s/it]DLL 2022-02-22 18:08:26.481443 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.155421495437622  train_perf : 490.2577819824219 

Epoch: 000, Step:   700, Loss:  2.84960413, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=701, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:701
2022-02-22 18:08:30.397706: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:08:30.518406: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:08:30.518425: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:08:30.518432: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:08:30.518434: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:08:30.518436: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:08:30.518438: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 18:08:29.804041 - Training Epoch: 0 Training Iteration: 700  step_loss : 2.8496041297912598  train_perf : 490.3169250488281 

Epoch: 000, Step:   750, Loss:  2.65282536, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=41, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:750
Iteration:  27%|██▋       | 757/2771 [04:31<2:24:53,  4.32s/it]DLL 2022-02-22 18:08:33.290233 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.652825355529785  train_perf : 489.77435302734375 

Epoch: 000, Step:   800, Loss:  3.04018950, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=91, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:800
Iteration:  30%|███       | 832/2771 [04:36<1:38:17,  3.04s/it]DLL 2022-02-22 18:08:36.627788 - Training Epoch: 0 Training Iteration: 800  step_loss : 3.040189504623413  train_perf : 489.7159118652344 

Epoch: 000, Step:   850, Loss:  3.40668511, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=141, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:850
DLL 2022-02-22 18:08:39.964633 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.4066851139068604  train_perf : 489.6708679199219 

Epoch: 000, Step:   900, Loss:  2.64265132, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=191, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:900
Iteration:  33%|███▎      | 908/2771 [04:41<1:06:43,  2.15s/it]DLL 2022-02-22 18:08:43.292901 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.642651319503784  train_perf : 489.6821594238281 

Epoch: 000, Step:   950, Loss:  3.43853903, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=241, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:950
Iteration:  36%|███▌      | 984/2771 [04:46<45:24,  1.52s/it]  DLL 2022-02-22 18:08:46.614692 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.4385390281677246  train_perf : 489.7512512207031 

Epoch: 000, Step:  1000, Loss:  3.05561686, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=291, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1000
DLL 2022-02-22 18:08:49.945048 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.055616855621338  train_perf : 489.7607421875 

Epoch: 000, Step:  1050, Loss:  3.03160048, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=341, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1050
Iteration:  38%|███▊      | 1060/2771 [04:51<30:59,  1.09s/it]DLL 2022-02-22 18:08:53.274037 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.0316004753112793  train_perf : 489.77252197265625 

Epoch: 000, Step:  1100, Loss:  3.47127485, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=391, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1100
Iteration:  41%|████      | 1136/2771 [04:56<21:16,  1.28it/s]DLL 2022-02-22 18:08:56.600581 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.4712748527526855  train_perf : 489.79931640625 

Epoch: 000, Step:  1150, Loss:  3.56191301, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=441, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1150
DLL 2022-02-22 18:08:59.929829 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.561913013458252  train_perf : 489.7935485839844 

Epoch: 000, Step:  1200, Loss:  3.02529073, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=491, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1200
Iteration:  44%|████▎     | 1211/2771 [05:01<14:43,  1.76it/s]DLL 2022-02-22 18:09:03.264354 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.0252907276153564  train_perf : 489.76727294921875 

Epoch: 000, Step:  1250, Loss:  3.41116381, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=541, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1250
Iteration:  46%|████▋     | 1286/2771 [05:06<10:18,  2.40it/s]DLL 2022-02-22 18:09:06.604846 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.411163806915283  train_perf : 489.70513916015625 

Epoch: 000, Step:  1300, Loss:  3.31938267, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=591, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1300
DLL 2022-02-22 18:09:09.938886 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.319382667541504  train_perf : 489.681640625 

Epoch: 000, Step:  1350, Loss:  2.96249795, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=641, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1350
Iteration:  49%|████▉     | 1361/2771 [05:11<07:19,  3.21it/s]DLL 2022-02-22 18:09:13.272165 - Training Epoch: 0 Training Iteration: 1350  step_loss : 2.9624979496002197  train_perf : 489.6656188964844 

Epoch: 000, Step:  1400, Loss:  3.19781876, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=691, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1400
Iteration:  52%|█████▏    | 1437/2771 [05:16<05:17,  4.20it/s]DLL 2022-02-22 18:09:16.603737 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.1978187561035156  train_perf : 489.65338134765625 

Epoch: 000, Step:  1450, Loss:  3.02431655, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=741, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1450
DLL 2022-02-22 18:09:19.941803 - Training Epoch: 0 Training Iteration: 1450  step_loss : 3.0243165493011475  train_perf : 489.615478515625 

Epoch: 000, Step:  1500, Loss:  3.56345201, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=791, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1500
Iteration:  55%|█████▍    | 1512/2771 [05:21<03:55,  5.35it/s]DLL 2022-02-22 18:09:23.285352 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.5634520053863525  train_perf : 489.5547180175781 

Epoch: 000, Step:  1550, Loss:  3.16512084, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=841, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1550
Iteration:  57%|█████▋    | 1587/2771 [05:26<02:58,  6.63it/s]DLL 2022-02-22 18:09:26.627199 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.165120840072632  train_perf : 489.50628662109375 

Epoch: 000, Step:  1600, Loss:  2.97829103, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=891, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1600
DLL 2022-02-22 18:09:29.967733 - Training Epoch: 0 Training Iteration: 1600  step_loss : 2.9782910346984863  train_perf : 489.4643249511719 

Epoch: 000, Step:  1650, Loss:  3.25751591, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=941, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1650
Iteration:  60%|█████▉    | 1662/2771 [05:31<02:19,  7.96it/s]DLL 2022-02-22 18:09:33.304819 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.2575159072875977  train_perf : 489.43829345703125 

Epoch: 000, Step:  1700, Loss:  3.11232734, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=991, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1700
Iteration:  63%|██████▎   | 1737/2771 [05:36<01:51,  9.27it/s]DLL 2022-02-22 18:09:36.641858 - Training Epoch: 0 Training Iteration: 1700  step_loss : 3.1123273372650146  train_perf : 489.41070556640625 

Epoch: 000, Step:  1750, Loss:  3.40483427, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1041, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1750
DLL 2022-02-22 18:09:39.981129 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.404834270477295  train_perf : 489.3841247558594 

Epoch: 000, Step:  1800, Loss:  2.99561620, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1091, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1800
Iteration:  65%|██████▌   | 1812/2771 [05:41<01:31, 10.46it/s]DLL 2022-02-22 18:09:43.319942 - Training Epoch: 0 Training Iteration: 1800  step_loss : 2.9956161975860596  train_perf : 489.3526916503906 

Epoch: 000, Step:  1850, Loss:  2.95231771, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1141, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1850
Iteration:  68%|██████▊   | 1887/2771 [05:47<01:16, 11.50it/s]DLL 2022-02-22 18:09:46.657000 - Training Epoch: 0 Training Iteration: 1850  step_loss : 2.952317714691162  train_perf : 489.32794189453125 

Epoch: 000, Step:  1900, Loss:  2.81210279, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1191, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1900
DLL 2022-02-22 18:09:50.003560 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.812102794647217  train_perf : 489.27239990234375 

Epoch: 000, Step:  1950, Loss:  2.59485483, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1241, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1950
Iteration:  71%|███████   | 1962/2771 [05:52<01:05, 12.36it/s]DLL 2022-02-22 18:09:53.344518 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.5948548316955566  train_perf : 489.244384765625 

Epoch: 000, Step:  2000, Loss:  3.16194248, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1291, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2000
Iteration:  74%|███████▎  | 2037/2771 [05:57<00:56, 13.04it/s]DLL 2022-02-22 18:09:56.684763 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.161942481994629  train_perf : 489.2193908691406 

Epoch: 000, Step:  2050, Loss:  3.03709030, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1341, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2050
DLL 2022-02-22 18:10:00.026372 - Training Epoch: 0 Training Iteration: 2050  step_loss : 3.037090301513672  train_perf : 489.1905212402344 

Epoch: 000, Step:  2100, Loss:  3.29637051, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1391, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2100
Iteration:  76%|███████▌  | 2112/2771 [06:02<00:48, 13.57it/s]DLL 2022-02-22 18:10:03.366814 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.296370506286621  train_perf : 489.1621398925781 

Epoch: 000, Step:  2150, Loss:  2.91480589, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1441, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2150
Iteration:  79%|███████▉  | 2187/2771 [06:07<00:41, 13.96it/s]DLL 2022-02-22 18:10:06.702478 - Training Epoch: 0 Training Iteration: 2150  step_loss : 2.9148058891296387  train_perf : 489.15753173828125 

Epoch: 000, Step:  2200, Loss:  3.09773731, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1491, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2200
DLL 2022-02-22 18:10:10.047439 - Training Epoch: 0 Training Iteration: 2200  step_loss : 3.0977373123168945  train_perf : 489.1197814941406 

Epoch: 000, Step:  2250, Loss:  2.68121862, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1541, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2250
Iteration:  82%|████████▏ | 2262/2771 [06:12<00:35, 14.24it/s]DLL 2022-02-22 18:10:13.388966 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.6812186241149902  train_perf : 489.0918884277344 

Epoch: 000, Step:  2300, Loss:  3.05502105, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1591, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2300
Iteration:  84%|████████▍ | 2337/2771 [06:17<00:30, 14.45it/s]DLL 2022-02-22 18:10:16.731884 - Training Epoch: 0 Training Iteration: 2300  step_loss : 3.055021047592163  train_perf : 489.060791015625 

Epoch: 000, Step:  2350, Loss:  2.64313030, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1641, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2350
DLL 2022-02-22 18:10:20.077439 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.643130302429199  train_perf : 489.0287170410156 

Epoch: 000, Step:  2400, Loss:  2.79331350, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1691, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2400
Iteration:  87%|████████▋ | 2412/2771 [06:22<00:24, 14.60it/s]DLL 2022-02-22 18:10:23.420770 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.793313503265381  train_perf : 489.0107116699219 

Epoch: 000, Step:  2450, Loss:  3.17727852, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1741, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2450
Iteration:  90%|████████▉ | 2487/2771 [06:27<00:19, 14.70it/s]DLL 2022-02-22 18:10:26.763234 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.177278518676758  train_perf : 488.98419189453125 

Epoch: 000, Step:  2500, Loss:  3.28666854, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1791, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2500
DLL 2022-02-22 18:10:30.104529 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.286668539047241  train_perf : 488.9657287597656 

Epoch: 000, Step:  2550, Loss:  2.99747658, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1841, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2550
Iteration:  92%|█████████▏| 2562/2771 [06:32<00:14, 14.78it/s]DLL 2022-02-22 18:10:33.443140 - Training Epoch: 0 Training Iteration: 2550  step_loss : 2.997476577758789  train_perf : 488.9549560546875 

Epoch: 000, Step:  2600, Loss:  2.95293665, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1891, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2600
Iteration:  95%|█████████▌| 2637/2771 [06:37<00:09, 14.83it/s]DLL 2022-02-22 18:10:36.795855 - Training Epoch: 0 Training Iteration: 2600  step_loss : 2.9529366493225098  train_perf : 488.90869140625 

Epoch: 000, Step:  2650, Loss:  2.95864201, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1941, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2650
DLL 2022-02-22 18:10:40.136451 - Training Epoch: 0 Training Iteration: 2650  step_loss : 2.95864200592041  train_perf : 488.88916015625 

Epoch: 000, Step:  2700, Loss:  2.94544268, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1991, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2700
2022-02-22 18:10:44.081451: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:10:44.202230: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:10:44.202249: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:10:44.202255: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:10:44.202257: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:10:44.202259: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:10:44.202260: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:  98%|█████████▊| 2712/2771 [06:42<00:04, 14.73it/s]DLL 2022-02-22 18:10:43.480235 - Training Epoch: 0 Training Iteration: 2700  step_loss : 2.9454426765441895  train_perf : 488.865234375 

Epoch: 000, Step:  2750, Loss:  3.25191784, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=41, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2750
Iteration: 100%|█████████▉| 2770/2771 [06:46<00:00,  6.82it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-22 18:10:46.988177 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.251917839050293  train_perf : 488.6966552734375 
DLL 2022-02-22 18:10:48.259407 -  e2e_train_time : 406.1972494125366  training_sequences_per_second : 488.69097900390625  final_loss : 3.1987385749816895 

Epoch: 001, Step:     0, Loss:  2.52117491, Perf:  459, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=61, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2770
DLL 2022-02-22 18:10:58.994849 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.521174907684326  train_perf : 459.3005676269531 

Epoch: 001, Step:    50, Loss:  2.64277601, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=111, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2820
Iteration:   3%|▎         | 71/2771 [00:05<03:11, 14.06it/s]DLL 2022-02-22 18:11:02.335116 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.6427760124206543  train_perf : 487.53125 

Epoch: 001, Step:   100, Loss:  2.60608721, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=161, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2870
Iteration:   5%|▌         | 147/2771 [00:10<03:02, 14.35it/s]DLL 2022-02-22 18:11:05.661076 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.6060872077941895  train_perf : 488.8514099121094 

Epoch: 001, Step:   150, Loss:  2.54216623, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=211, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2920
DLL 2022-02-22 18:11:08.981556 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.542166233062744  train_perf : 489.51910400390625 

Epoch: 001, Step:   200, Loss:  2.66552043, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=261, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2970
Iteration:   8%|▊         | 223/2771 [00:15<02:55, 14.54it/s]DLL 2022-02-22 18:11:12.309967 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.665520429611206  train_perf : 489.6094665527344 

Epoch: 001, Step:   250, Loss:  2.47139168, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=311, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3020
Iteration:  11%|█         | 299/2771 [00:20<02:48, 14.68it/s]DLL 2022-02-22 18:11:15.631335 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.4713916778564453  train_perf : 489.83892822265625 

Epoch: 001, Step:   300, Loss:  2.47516870, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=361, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3070
DLL 2022-02-22 18:11:18.963764 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.4751687049865723  train_perf : 489.7873229980469 

Epoch: 001, Step:   350, Loss:  2.44584846, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=411, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3120
Iteration:  14%|█▎        | 375/2771 [00:25<02:42, 14.79it/s]DLL 2022-02-22 18:11:22.290931 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.4458484649658203  train_perf : 489.8056335449219 

Epoch: 001, Step:   400, Loss:  2.02970338, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=461, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3170
Iteration:  16%|█▌        | 450/2771 [00:30<02:36, 14.85it/s]DLL 2022-02-22 18:11:25.620559 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.029703378677368  train_perf : 489.7851867675781 

Epoch: 001, Step:   450, Loss:  2.08483958, Perf:  490, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=511, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3220
DLL 2022-02-22 18:11:28.958897 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.0848395824432373  train_perf : 489.61175537109375 

Epoch: 001, Step:   500, Loss:  2.59616542, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=561, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3270
Iteration:  19%|█▉        | 525/2771 [00:35<02:30, 14.89it/s]DLL 2022-02-22 18:11:32.299787 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.596165418624878  train_perf : 489.45623779296875 

Epoch: 001, Step:   550, Loss:  2.07139087, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=611, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3320
Iteration:  22%|██▏       | 600/2771 [00:40<02:25, 14.92it/s]DLL 2022-02-22 18:11:35.633700 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.0713908672332764  train_perf : 489.4193420410156 

Epoch: 001, Step:   600, Loss:  1.99969590, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=661, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3370
DLL 2022-02-22 18:11:38.967226 - Training Epoch: 1 Training Iteration: 600  step_loss : 1.999695897102356  train_perf : 489.4085998535156 

Epoch: 001, Step:   650, Loss:  2.91201019, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=711, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3420
Iteration:  24%|██▍       | 675/2771 [00:45<02:20, 14.94it/s]DLL 2022-02-22 18:11:42.306435 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.9120101928710938  train_perf : 489.3313293457031 

Epoch: 001, Step:   700, Loss:  2.42326927, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=761, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3470
Iteration:  27%|██▋       | 750/2771 [00:50<02:15, 14.95it/s]DLL 2022-02-22 18:11:45.635405 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.423269271850586  train_perf : 489.3459777832031 

Epoch: 001, Step:   750, Loss:  2.05031800, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=811, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3520
DLL 2022-02-22 18:11:48.974047 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.0503180027008057  train_perf : 489.25872802734375 

Epoch: 001, Step:   800, Loss:  2.78738880, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=861, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3570
Iteration:  30%|██▉       | 825/2771 [00:55<02:10, 14.96it/s]DLL 2022-02-22 18:11:52.315438 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.787388801574707  train_perf : 489.1806335449219 

Epoch: 001, Step:   850, Loss:  2.98787284, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=911, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3620
Iteration:  32%|███▏      | 900/2771 [01:00<02:05, 14.96it/s]DLL 2022-02-22 18:11:55.654507 - Training Epoch: 1 Training Iteration: 850  step_loss : 2.987872838973999  train_perf : 489.1119384765625 

Epoch: 001, Step:   900, Loss:  2.59129047, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=961, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3670
DLL 2022-02-22 18:11:58.993032 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.5912904739379883  train_perf : 489.0624694824219 

Epoch: 001, Step:   950, Loss:  2.63271642, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1011, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3720
Iteration:  35%|███▌      | 975/2771 [01:05<02:00, 14.96it/s]DLL 2022-02-22 18:12:02.338747 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.632716417312622  train_perf : 488.96917724609375 

Epoch: 001, Step:  1000, Loss:  2.59214187, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1061, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3770
Iteration:  38%|███▊      | 1050/2771 [01:10<01:55, 14.96it/s]DLL 2022-02-22 18:12:05.676968 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.59214186668396  train_perf : 488.9292297363281 

Epoch: 001, Step:  1050, Loss:  2.85683942, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1111, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3820
DLL 2022-02-22 18:12:09.021030 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.856839418411255  train_perf : 488.8562316894531 

Epoch: 001, Step:  1100, Loss:  3.34050798, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1161, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3870
Iteration:  41%|████      | 1125/2771 [01:15<01:50, 14.96it/s]DLL 2022-02-22 18:12:12.361729 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.340507984161377  train_perf : 488.8092041015625 

Epoch: 001, Step:  1150, Loss:  2.92901707, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1211, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3920
Iteration:  43%|████▎     | 1200/2771 [01:20<01:45, 14.96it/s]DLL 2022-02-22 18:12:15.705696 - Training Epoch: 1 Training Iteration: 1150  step_loss : 2.9290170669555664  train_perf : 488.7477722167969 

Epoch: 001, Step:  1200, Loss:  2.66499090, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1261, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3970
DLL 2022-02-22 18:12:19.051635 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.6649909019470215  train_perf : 488.68511962890625 

Epoch: 001, Step:  1250, Loss:  3.10138607, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1311, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4020
Iteration:  46%|████▌     | 1275/2771 [01:25<01:40, 14.96it/s]DLL 2022-02-22 18:12:22.397909 - Training Epoch: 1 Training Iteration: 1250  step_loss : 3.101386070251465  train_perf : 488.6328430175781 

Epoch: 001, Step:  1300, Loss:  2.49606752, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1361, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4070
Iteration:  49%|████▊     | 1350/2771 [01:30<01:35, 14.95it/s]DLL 2022-02-22 18:12:25.741002 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.496067523956299  train_perf : 488.6000671386719 

Epoch: 001, Step:  1350, Loss:  2.62679243, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1411, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4120
DLL 2022-02-22 18:12:29.084311 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.6267924308776855  train_perf : 488.560546875 

Epoch: 001, Step:  1400, Loss:  2.49830151, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1461, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4170
Iteration:  51%|█████▏    | 1425/2771 [01:35<01:30, 14.96it/s]DLL 2022-02-22 18:12:32.425736 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.4983015060424805  train_perf : 488.528564453125 

Epoch: 001, Step:  1450, Loss:  2.33859420, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1511, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4220
Iteration:  54%|█████▍    | 1500/2771 [01:40<01:25, 14.95it/s]DLL 2022-02-22 18:12:35.773234 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.3385941982269287  train_perf : 488.4662780761719 

Epoch: 001, Step:  1500, Loss:  3.52927828, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1561, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4270
DLL 2022-02-22 18:12:39.116765 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.52927827835083  train_perf : 488.4347839355469 

Epoch: 001, Step:  1550, Loss:  2.40097570, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1611, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4320
Iteration:  57%|█████▋    | 1575/2771 [01:45<01:19, 14.95it/s]DLL 2022-02-22 18:12:42.463539 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.4009757041931152  train_perf : 488.3880310058594 

Epoch: 001, Step:  1600, Loss:  2.51674557, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1661, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4370
Iteration:  60%|█████▉    | 1650/2771 [01:50<01:14, 14.95it/s]DLL 2022-02-22 18:12:45.803372 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.5167455673217773  train_perf : 488.3846130371094 

Epoch: 001, Step:  1650, Loss:  2.51649404, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1711, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4420
DLL 2022-02-22 18:12:49.149271 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.516494035720825  train_perf : 488.34490966796875 

Epoch: 001, Step:  1700, Loss:  2.64715815, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1761, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4470
Iteration:  62%|██████▏   | 1725/2771 [01:55<01:09, 14.95it/s]DLL 2022-02-22 18:12:52.496099 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.647158145904541  train_perf : 488.3060302734375 

Epoch: 001, Step:  1750, Loss:  3.03567219, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1811, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4520
Iteration:  65%|██████▍   | 1800/2771 [02:00<01:04, 14.97it/s]DLL 2022-02-22 18:12:55.834911 - Training Epoch: 1 Training Iteration: 1750  step_loss : 3.035672187805176  train_perf : 488.3009338378906 

Epoch: 001, Step:  1800, Loss:  2.42402959, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1861, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4570
DLL 2022-02-22 18:12:59.166895 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.424029588699341  train_perf : 488.32171630859375 

Epoch: 001, Step:  1850, Loss:  2.66862440, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1911, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4620
Iteration:  68%|██████▊   | 1875/2771 [02:05<00:59, 14.96it/s]DLL 2022-02-22 18:13:02.505046 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.6686244010925293  train_perf : 488.3180236816406 

Epoch: 001, Step:  1900, Loss:  2.63931108, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1961, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4670
Iteration:  70%|███████   | 1950/2771 [02:10<00:54, 14.96it/s]DLL 2022-02-22 18:13:05.850709 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.6393110752105713  train_perf : 488.28851318359375 

Epoch: 001, Step:  1950, Loss:  2.39014387, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=11, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4720
DLL 2022-02-22 18:13:09.194389 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.390143871307373  train_perf : 488.2698974609375 

Epoch: 001, Step:  2000, Loss:  2.55577612, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=65536.0, num_good_steps=61, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4770
Iteration:  73%|███████▎  | 2025/2771 [02:15<00:49, 14.97it/s]DLL 2022-02-22 18:13:12.530686 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.5557761192321777  train_perf : 488.2673034667969 

Epoch: 001, Step:  2050, Loss:  2.31444216, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=49, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
Iteration:  76%|███████▌  | 2100/2771 [02:20<00:44, 14.97it/s]DLL 2022-02-22 18:13:15.870051 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.3144421577453613  train_perf : 488.2653503417969 

Epoch: 001, Step:  2100, Loss:  2.88252020, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=99, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-22 18:13:19.212794 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.8825201988220215  train_perf : 488.2488708496094 

Epoch: 001, Step:  2150, Loss:  2.71876240, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=149, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2175/2771 [02:25<00:39, 14.96it/s]DLL 2022-02-22 18:13:22.556589 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.7187623977661133  train_perf : 488.2226867675781 

Epoch: 001, Step:  2200, Loss:  2.60691404, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=199, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2250/2771 [02:30<00:34, 14.97it/s]DLL 2022-02-22 18:13:25.898175 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.6069140434265137  train_perf : 488.21246337890625 

Epoch: 001, Step:  2250, Loss:  2.46083140, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=249, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-22 18:13:29.236301 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.4608314037323  train_perf : 488.2137451171875 

Epoch: 001, Step:  2300, Loss:  2.55335236, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=299, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▍ | 2325/2771 [02:35<00:29, 14.97it/s]DLL 2022-02-22 18:13:32.577031 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.5533523559570312  train_perf : 488.19805908203125 

Epoch: 001, Step:  2350, Loss:  2.20383978, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=349, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  87%|████████▋ | 2400/2771 [02:40<00:24, 14.96it/s]DLL 2022-02-22 18:13:35.923206 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.2038397789001465  train_perf : 488.1754150390625 

Epoch: 001, Step:  2400, Loss:  2.18944240, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=399, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-22 18:13:39.269126 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.1894423961639404  train_perf : 488.1536865234375 

Epoch: 001, Step:  2450, Loss:  2.81213379, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=449, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2475/2771 [02:45<00:19, 14.96it/s]DLL 2022-02-22 18:13:42.609985 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.8121337890625  train_perf : 488.1488037109375 

Epoch: 001, Step:  2500, Loss:  2.78275418, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=499, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2550/2771 [02:50<00:14, 14.95it/s]DLL 2022-02-22 18:13:45.957965 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.7827541828155518  train_perf : 488.1253967285156 

Epoch: 001, Step:  2550, Loss:  2.65486979, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=549, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-22 18:13:49.302577 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.654869794845581  train_perf : 488.1120300292969 

Epoch: 001, Step:  2600, Loss:  2.97260189, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=599, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▍| 2625/2771 [02:55<00:09, 14.95it/s]DLL 2022-02-22 18:13:52.647215 - Training Epoch: 1 Training Iteration: 2600  step_loss : 2.972601890563965  train_perf : 488.0971374511719 

Epoch: 001, Step:  2650, Loss:  2.78653002, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=649, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2700/2771 [03:00<00:04, 14.95it/s]DLL 2022-02-22 18:13:55.996408 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.786530017852783  train_perf : 488.0726013183594 

Epoch: 001, Step:  2700, Loss:  2.74117351, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=699, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-22 18:13:59.339649 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.741173505783081  train_perf : 488.0621032714844 

Epoch: 001, Step:  2750, Loss:  2.61908531, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=749, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:05<00:00, 14.95it/s]DLL 2022-02-22 18:14:02.682532 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.6190853118896484  train_perf : 488.05242919921875 
DLL 2022-02-22 18:14:03.952507 -  e2e_train_time : 591.5333976745605  training_sequences_per_second : 488.05157470703125  final_loss : 2.637443780899048 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-22 18:14:11.064907: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 18:15:39.146586: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:15:40.203176: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:15:40.203196: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:15:40.203203: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:15:40.203206: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:15:40.203208: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:15:40.203210: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:33<32:41, 93.42s/it]Iteration:  14%|█▎        | 3/22 [01:39<20:58, 66.26s/it]Iteration:  23%|██▎       | 5/22 [01:45<13:23, 47.26s/it]Iteration:  32%|███▏      | 7/22 [01:51<08:29, 33.99s/it]Iteration:  41%|████      | 9/22 [01:57<05:21, 24.72s/it]Iteration:  50%|█████     | 11/22 [02:03<03:20, 18.24s/it]Iteration:  59%|█████▉    | 13/22 [02:09<02:03, 13.73s/it]Iteration:  68%|██████▊   | 15/22 [02:16<01:14, 10.59s/it]Iteration:  77%|███████▋  | 17/22 [02:23<00:41,  8.40s/it]Iteration:  86%|████████▋ | 19/22 [02:29<00:20,  6.88s/it]Iteration:  95%|█████████▌| 21/22 [02:36<00:05,  5.84s/it]2022-02-22 18:16:47.271056: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.84s/it]2022-02-22 18:18:00.049763: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:18:01.116349: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:18:01.116368: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:18:01.116392: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:18:01.116394: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:18:01.116396: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:18:01.116398: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:52<00:00, 26.74s/it]Iteration: 100%|██████████| 22/22 [03:52<00:00, 10.55s/it]{"exact_match": 12.526017029328287, "f1": 16.987853235198553}

Epoch: 001 Results: {"exact_match": 12.526017029328287, "f1": 16.987853235198553}

**EVAL SUMMARY** - Epoch: 001,  EM: 12.526, F1: 16.988, Infer_Perf: 1051 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 442.921 ms, 90%: 442.311 ms, 95%: 442.630 ms, 99%: 442.630 ms
DLL 2022-02-22 18:18:18.539306 -  inference_sequences_per_second : 1051.4962158203125  e2e_inference_time : 246.3344042301178 
**RESULTS SUMMARY** - EM: 12.526, F1: 16.988, Train_Time:  592 s, Train_Perf:  488 seq/s, Infer_Perf: 1051 seq/s

DLL 2022-02-22 18:18:18.540118 -  exact_match : 12.526017029328287  F1 : 16.987853235198553 
====================================  END results/models/test/checkpoints/ckpt-9494  ====================================
==================================== START results/models/test/checkpoints/ckpt-9994 ====================================
Compute dtype: float16
Variable dtype: float32
 ** Restored from results/models/test/checkpoints/ckpt-9994 at step 9993
================================================================================
 ** Saving discriminator
================================================================================
Configuration saved in results/models/test/checkpoints/discriminator/config.json
Model weights saved in results/models/test/checkpoints/discriminator/tf_model.h5: {'discriminator_predictions', 'electra'}
Container nvidia build =  14714731
out dir is test_results/
mixed-precision training and xla activated!
Running SQuAD-v1.1
   python run_tf_squad.py --init_checkpoint=checkpoints/electra_base_qa_v2_False_epoch_2_ckpt  --do_train  --train_batch_size=32 --do_predict --predict_batch_size=512 --eval_script=/workspace/electra/data/download/squad/v1.1/evaluate-v1.1.py --do_eval    --data_dir /workspace/electra/data/download/squad/v1.1  --do_lower_case  --electra_model=results/models/test/checkpoints/discriminator  --learning_rate=8e-4  --warmup_proportion 0.05  --weight_decay_rate 0.01  --layerwise_lr_decay 0.8  --seed=1  --num_train_epochs=2  --max_seq_length=384  --doc_stride=128  --beam_size 5  --joint_head False  --null_score_diff_threshold -5.6  --output_dir=test_results/   --amp --xla  --cache_dir=/workspace/electra/data/download/squad/v1.1  --max_steps=-1  --vocab_file=/workspace/electra/vocab/vocab.txt  |& tee test_results//logfile.txt
2022-02-22 18:18:25.449464: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
Running total processes: 1
Starting process: 0
2022-02-22 18:18:26.307841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2022-02-22 18:18:26.324654: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.325092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 18:18:26.325107: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 18:18:26.326578: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 18:18:26.327205: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 18:18:26.327368: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 18:18:26.328839: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 18:18:26.329191: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 18:18:26.329285: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 18:18:26.329328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.329872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.330295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 18:18:26.335505: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3600000000 Hz
2022-02-22 18:18:26.335959: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f828c000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2022-02-22 18:18:26.335969: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2022-02-22 18:18:26.479149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.479660: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8190000b20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-02-22 18:18:26.479672: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 3090, Compute Capability 8.6
2022-02-22 18:18:26.479810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.480260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 82 deviceMemorySize: 23.70GiB deviceMemoryBandwidth: 871.81GiB/s
2022-02-22 18:18:26.480293: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 18:18:26.480316: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2022-02-22 18:18:26.480326: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2022-02-22 18:18:26.480347: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2022-02-22 18:18:26.480371: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2022-02-22 18:18:26.480378: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2022-02-22 18:18:26.480386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 18:18:26.480417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.480915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.481346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2022-02-22 18:18:26.481377: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2022-02-22 18:18:26.651091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2022-02-22 18:18:26.651117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2022-02-22 18:18:26.651121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2022-02-22 18:18:26.651237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.651684: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-22 18:18:26.652105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22444 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
DLL 2022-02-22 18:18:26.307014 - PARAMETER SEED : 1 
Compute dtype: float16
Variable dtype: float32
***** Loading tokenizer and model *****
model: results/models/test/checkpoints/discriminator
loading configuration file results/models/test/checkpoints/discriminator/config.json
loading weights file results/models/test/checkpoints/discriminator/tf_model.h5
2022-02-22 18:18:26.717531: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2022-02-22 18:18:26.757060: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
WARNING:tensorflow:Layer activation_2 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

Some weights of the model checkpoint at results/models/test/checkpoints/discriminator were not used when initializing TFElectraForQuestionAnswering: ['discriminator_predictions']

Some weights of TFElectraForQuestionAnswering were not initialized from the model checkpoint at results/models/test/checkpoints/discriminator and are newly initialized: ['start_logits', 'end_logits']

***** Loading dataset *****
  0%|          | 0/442 [00:00<?, ?it/s] 36%|███▌      | 159/442 [00:05<00:08, 31.79it/s] 66%|██████▌   | 291/442 [00:10<00:05, 29.93it/s] 97%|█████████▋| 428/442 [00:15<00:00, 29.12it/s]100%|██████████| 442/442 [00:15<00:00, 28.53it/s]
  0%|          | 0/48 [00:00<?, ?it/s]100%|██████████| 48/48 [00:01<00:00, 26.25it/s]***** Loading features *****
***** Running training *****
  Num examples =  88641
  Num Epochs =  2
  Instantaneous batch size per GPU =  32
  Total train batch size (w. parallel, distributed & accumulation) =  32
  Total optimization steps = 5541

Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]2022-02-22 18:19:25.798035: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1631] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2022-02-22 18:19:26.675899: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 18:19:26.702174: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2022-02-22 18:19:27.335929: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2022-02-22 18:19:27.641907: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 65280, output: ptxas fatal   : Value 'sm_86' is not defined for option 'gpu-name'

Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2022-02-22 18:22:44.373553: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:22:47.670148: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:22:47.670167: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:22:47.670173: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:22:47.670175: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:22:47.670176: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:22:47.670178: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 18:22:47.684585: I tensorflow/compiler/jit/xla_compilation_cache.cc:241] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2022-02-22 18:22:49.720657: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:22:49.875008: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:22:49.875026: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:22:49.875034: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:22:49.875037: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:22:49.875040: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:22:49.875043: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 18:22:49.933728: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:22:50.126381: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:22:50.126399: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:22:50.126405: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:22:50.126407: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:22:50.126409: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:22:50.126411: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 18:22:50.357233: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:22:51.140064: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:22:51.140085: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:22:51.140090: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:22:51.140093: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:22:51.140094: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:22:51.140096: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.

Epoch: 000, Step:     0, Loss:  5.26400423, Perf:    0, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=1, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
Iteration:   0%|          | 1/2771 [03:33<164:39:16, 213.99s/it]2022-02-22 18:22:54.277865: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 18:22:55.604247: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:22:58.830872: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:22:58.830890: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:22:58.830897: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:22:58.830899: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:22:58.830901: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:22:58.830902: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2022-02-22 18:23:00.821176: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:23:00.908100: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:23:01.209562: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:23:02.229241: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:23:02.361762: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:23:02.361780: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:23:02.361786: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:23:02.361788: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:23:02.361789: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:23:02.361791: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   0%|          | 2/2771 [03:44<117:42:41, 153.04s/it]DLL 2022-02-22 18:22:51.586914 - Training Epoch: 0 Training Iteration: 0  step_loss : 5.264004230499268  train_perf : 0.14974728226661682 

Epoch: 000, Step:    50, Loss:  4.86453629, Perf:  469, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=51, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:51
Iteration:   3%|▎         | 78/2771 [03:49<80:09:05, 107.15s/it]DLL 2022-02-22 18:23:05.676141 - Training Epoch: 0 Training Iteration: 50  step_loss : 4.864536285400391  train_perf : 469.1625061035156 

Epoch: 000, Step:   100, Loss:  4.13422728, Perf:  481, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=101, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:101
DLL 2022-02-22 18:23:08.982505 - Training Epoch: 0 Training Iteration: 100  step_loss : 4.134227275848389  train_perf : 481.18701171875 

Epoch: 000, Step:   150, Loss:  3.51098204, Perf:  485, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=151, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:151
Iteration:   6%|▌         | 154/2771 [03:54<54:32:13, 75.02s/it]DLL 2022-02-22 18:23:12.291876 - Training Epoch: 0 Training Iteration: 150  step_loss : 3.510982036590576  train_perf : 485.11114501953125 

Epoch: 000, Step:   200, Loss:  3.71116495, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=201, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:201
Iteration:   8%|▊         | 230/2771 [03:59<37:04:52, 52.54s/it]DLL 2022-02-22 18:23:15.605898 - Training Epoch: 0 Training Iteration: 200  step_loss : 3.711164951324463  train_perf : 486.8732604980469 

Epoch: 000, Step:   250, Loss:  3.38125682, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=251, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:251
2022-02-22 18:23:20.710668: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:23:20.839038: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:23:20.839070: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:23:20.839077: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:23:20.839079: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:23:20.839080: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:23:20.839082: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 18:23:18.923871 - Training Epoch: 0 Training Iteration: 250  step_loss : 3.3812568187713623  train_perf : 487.80816650390625 

Epoch: 000, Step:   300, Loss:  3.20666981, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=301, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:301
Iteration:  11%|█         | 303/2771 [04:04<25:13:31, 36.80s/it]DLL 2022-02-22 18:23:22.414025 - Training Epoch: 0 Training Iteration: 300  step_loss : 3.206669807434082  train_perf : 487.056640625 

Epoch: 000, Step:   350, Loss:  3.05272222, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=351, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:351
Iteration:  14%|█▎        | 379/2771 [04:09<17:07:37, 25.78s/it]DLL 2022-02-22 18:23:25.733872 - Training Epoch: 0 Training Iteration: 350  step_loss : 3.052722215652466  train_perf : 487.66558837890625 

Epoch: 000, Step:   400, Loss:  2.96874571, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=401, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:401
DLL 2022-02-22 18:23:29.052576 - Training Epoch: 0 Training Iteration: 400  step_loss : 2.968745708465576  train_perf : 488.13177490234375 

Epoch: 000, Step:   450, Loss:  2.70133924, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=451, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:451
Iteration:  16%|█▋        | 455/2771 [04:15<11:37:15, 18.06s/it]2022-02-22 18:23:35.701338: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:23:35.829801: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:23:35.829834: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:23:35.829840: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:23:35.829842: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:23:35.829844: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:23:35.829845: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 18:23:32.380029 - Training Epoch: 0 Training Iteration: 450  step_loss : 2.7013392448425293  train_perf : 488.3575134277344 

Epoch: 000, Step:   500, Loss:  3.39787197, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=0, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:500
Iteration:  19%|█▉        | 528/2771 [04:20<7:53:28, 12.67s/it] DLL 2022-02-22 18:23:35.863542 - Training Epoch: 0 Training Iteration: 500  step_loss : 3.397871971130371  train_perf : 487.8890380859375 

Epoch: 000, Step:   550, Loss:  3.13672304, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=28, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:549
DLL 2022-02-22 18:23:39.199545 - Training Epoch: 0 Training Iteration: 550  step_loss : 3.136723041534424  train_perf : 487.99114990234375 

Epoch: 000, Step:   600, Loss:  2.89409065, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=78, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:599
Iteration:  22%|██▏       | 604/2771 [04:25<5:20:55,  8.89s/it]DLL 2022-02-22 18:23:42.528027 - Training Epoch: 0 Training Iteration: 600  step_loss : 2.8940906524658203  train_perf : 488.1573791503906 

Epoch: 000, Step:   650, Loss:  3.17730236, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=128, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:649
Iteration:  25%|██▍       | 680/2771 [04:30<3:37:27,  6.24s/it]DLL 2022-02-22 18:23:45.854089 - Training Epoch: 0 Training Iteration: 650  step_loss : 3.177302360534668  train_perf : 488.3263854980469 

Epoch: 000, Step:   700, Loss:  2.99169302, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=178, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:699
DLL 2022-02-22 18:23:49.188017 - Training Epoch: 0 Training Iteration: 700  step_loss : 2.9916930198669434  train_perf : 488.3914489746094 

Epoch: 000, Step:   750, Loss:  2.90087080, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=228, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:749
Iteration:  27%|██▋       | 755/2771 [04:35<2:27:26,  4.39s/it]DLL 2022-02-22 18:23:52.518840 - Training Epoch: 0 Training Iteration: 750  step_loss : 2.9008708000183105  train_perf : 488.4696960449219 

Epoch: 000, Step:   800, Loss:  2.98184872, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=278, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:799
Iteration:  30%|██▉       | 831/2771 [04:40<1:39:57,  3.09s/it]DLL 2022-02-22 18:23:55.846964 - Training Epoch: 0 Training Iteration: 800  step_loss : 2.98184871673584  train_perf : 488.57537841796875 

Epoch: 000, Step:   850, Loss:  3.56569028, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=328, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:849
DLL 2022-02-22 18:23:59.179451 - Training Epoch: 0 Training Iteration: 850  step_loss : 3.565690279006958  train_perf : 488.6298522949219 

Epoch: 000, Step:   900, Loss:  2.54747915, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=378, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:899
Iteration:  33%|███▎      | 906/2771 [04:45<1:07:53,  2.18s/it]DLL 2022-02-22 18:24:02.515516 - Training Epoch: 0 Training Iteration: 900  step_loss : 2.5474791526794434  train_perf : 488.63397216796875 

Epoch: 000, Step:   950, Loss:  3.21798158, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=428, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:949
Iteration:  35%|███▌      | 981/2771 [04:50<46:12,  1.55s/it]  DLL 2022-02-22 18:24:05.855335 - Training Epoch: 0 Training Iteration: 950  step_loss : 3.2179815769195557  train_perf : 488.609130859375 

Epoch: 000, Step:  1000, Loss:  3.00512266, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=478, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:999
DLL 2022-02-22 18:24:09.192767 - Training Epoch: 0 Training Iteration: 1000  step_loss : 3.005122661590576  train_perf : 488.6031188964844 

Epoch: 000, Step:  1050, Loss:  3.18213415, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=528, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1049
Iteration:  38%|███▊      | 1056/2771 [04:55<31:33,  1.10s/it]DLL 2022-02-22 18:24:12.533396 - Training Epoch: 0 Training Iteration: 1050  step_loss : 3.1821341514587402  train_perf : 488.5813293457031 

Epoch: 000, Step:  1100, Loss:  3.43983078, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=578, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1099
Iteration:  41%|████      | 1131/2771 [05:00<21:40,  1.26it/s]DLL 2022-02-22 18:24:15.872433 - Training Epoch: 0 Training Iteration: 1100  step_loss : 3.439830780029297  train_perf : 488.58367919921875 

Epoch: 000, Step:  1150, Loss:  3.60872889, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=628, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1149
DLL 2022-02-22 18:24:19.208937 - Training Epoch: 0 Training Iteration: 1150  step_loss : 3.6087288856506348  train_perf : 488.59014892578125 

Epoch: 000, Step:  1200, Loss:  3.07790279, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=678, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1199
Iteration:  44%|████▎     | 1206/2771 [05:05<15:00,  1.74it/s]DLL 2022-02-22 18:24:22.551600 - Training Epoch: 0 Training Iteration: 1200  step_loss : 3.0779027938842773  train_perf : 488.5637512207031 

Epoch: 000, Step:  1250, Loss:  3.48222017, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=728, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1249
Iteration:  46%|████▌     | 1281/2771 [05:10<10:29,  2.37it/s]DLL 2022-02-22 18:24:25.886941 - Training Epoch: 0 Training Iteration: 1250  step_loss : 3.48222017288208  train_perf : 488.58203125 

Epoch: 000, Step:  1300, Loss:  3.03294420, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=778, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1299
DLL 2022-02-22 18:24:29.224173 - Training Epoch: 0 Training Iteration: 1300  step_loss : 3.0329442024230957  train_perf : 488.5851135253906 

Epoch: 000, Step:  1350, Loss:  3.03915262, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=828, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1349
Iteration:  49%|████▉     | 1356/2771 [05:15<07:26,  3.17it/s]DLL 2022-02-22 18:24:32.553114 - Training Epoch: 0 Training Iteration: 1350  step_loss : 3.0391526222229004  train_perf : 488.6316223144531 

Epoch: 000, Step:  1400, Loss:  3.24016118, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=878, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1399
Iteration:  52%|█████▏    | 1431/2771 [05:20<05:23,  4.15it/s]DLL 2022-02-22 18:24:35.893307 - Training Epoch: 0 Training Iteration: 1400  step_loss : 3.240161180496216  train_perf : 488.61468505859375 

Epoch: 000, Step:  1450, Loss:  2.98386264, Perf:  489, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=928, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1449
DLL 2022-02-22 18:24:39.248128 - Training Epoch: 0 Training Iteration: 1450  step_loss : 2.9838626384735107  train_perf : 488.5249328613281 

Epoch: 000, Step:  1500, Loss:  3.62798977, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=978, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1499
Iteration:  54%|█████▍    | 1506/2771 [05:25<03:58,  5.29it/s]DLL 2022-02-22 18:24:42.596712 - Training Epoch: 0 Training Iteration: 1500  step_loss : 3.6279897689819336  train_perf : 488.4632263183594 

Epoch: 000, Step:  1550, Loss:  3.29599857, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1028, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1549
Iteration:  57%|█████▋    | 1581/2771 [05:30<03:01,  6.57it/s]DLL 2022-02-22 18:24:45.946013 - Training Epoch: 0 Training Iteration: 1550  step_loss : 3.2959985733032227  train_perf : 488.4091796875 

Epoch: 000, Step:  1600, Loss:  3.25199366, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1078, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1599
DLL 2022-02-22 18:24:49.285360 - Training Epoch: 0 Training Iteration: 1600  step_loss : 3.2519936561584473  train_perf : 488.3988037109375 

Epoch: 000, Step:  1650, Loss:  3.38048387, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1128, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1649
Iteration:  60%|█████▉    | 1656/2771 [05:35<02:21,  7.89it/s]DLL 2022-02-22 18:24:52.636964 - Training Epoch: 0 Training Iteration: 1650  step_loss : 3.380483865737915  train_perf : 488.3388366699219 

Epoch: 000, Step:  1700, Loss:  2.91380644, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1178, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1699
Iteration:  62%|██████▏   | 1731/2771 [05:40<01:53,  9.19it/s]DLL 2022-02-22 18:24:55.990964 - Training Epoch: 0 Training Iteration: 1700  step_loss : 2.913806438446045  train_perf : 488.27557373046875 

Epoch: 000, Step:  1750, Loss:  3.23375559, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1228, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1749
DLL 2022-02-22 18:24:59.339124 - Training Epoch: 0 Training Iteration: 1750  step_loss : 3.233755588531494  train_perf : 488.2338562011719 

Epoch: 000, Step:  1800, Loss:  3.05969810, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1278, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1799
Iteration:  65%|██████▌   | 1806/2771 [05:45<01:32, 10.39it/s]DLL 2022-02-22 18:25:02.687552 - Training Epoch: 0 Training Iteration: 1800  step_loss : 3.0596981048583984  train_perf : 488.2021484375 

Epoch: 000, Step:  1850, Loss:  2.87870073, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1328, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1849
Iteration:  68%|██████▊   | 1881/2771 [05:50<01:17, 11.43it/s]DLL 2022-02-22 18:25:06.045772 - Training Epoch: 0 Training Iteration: 1850  step_loss : 2.8787007331848145  train_perf : 488.13360595703125 

Epoch: 000, Step:  1900, Loss:  2.60270858, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1378, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1899
DLL 2022-02-22 18:25:09.396126 - Training Epoch: 0 Training Iteration: 1900  step_loss : 2.602708578109741  train_perf : 488.0939025878906 

Epoch: 000, Step:  1950, Loss:  2.72277665, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1428, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1949
Iteration:  71%|███████   | 1956/2771 [05:55<01:06, 12.29it/s]DLL 2022-02-22 18:25:12.748413 - Training Epoch: 0 Training Iteration: 1950  step_loss : 2.7227766513824463  train_perf : 488.0538330078125 

Epoch: 000, Step:  2000, Loss:  3.10294628, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1478, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:1999
Iteration:  73%|███████▎  | 2031/2771 [06:00<00:57, 12.98it/s]DLL 2022-02-22 18:25:16.098330 - Training Epoch: 0 Training Iteration: 2000  step_loss : 3.1029462814331055  train_perf : 488.0188293457031 

Epoch: 000, Step:  2050, Loss:  2.89043713, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1528, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2049
DLL 2022-02-22 18:25:19.444240 - Training Epoch: 0 Training Iteration: 2050  step_loss : 2.890437126159668  train_perf : 488.0011291503906 

Epoch: 000, Step:  2100, Loss:  3.32064772, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1578, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2099
Iteration:  76%|███████▌  | 2106/2771 [06:05<00:49, 13.50it/s]DLL 2022-02-22 18:25:22.798668 - Training Epoch: 0 Training Iteration: 2100  step_loss : 3.320647716522217  train_perf : 487.9559326171875 

Epoch: 000, Step:  2150, Loss:  2.97706127, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1628, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2149
Iteration:  79%|███████▊  | 2181/2771 [06:10<00:42, 13.90it/s]DLL 2022-02-22 18:25:26.150423 - Training Epoch: 0 Training Iteration: 2150  step_loss : 2.9770612716674805  train_perf : 487.91729736328125 

Epoch: 000, Step:  2200, Loss:  2.99954939, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1678, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2199
DLL 2022-02-22 18:25:29.508146 - Training Epoch: 0 Training Iteration: 2200  step_loss : 2.999549388885498  train_perf : 487.8615417480469 

Epoch: 000, Step:  2250, Loss:  2.74646664, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1728, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2249
Iteration:  81%|████████▏ | 2256/2771 [06:15<00:36, 14.18it/s]DLL 2022-02-22 18:25:32.863189 - Training Epoch: 0 Training Iteration: 2250  step_loss : 2.746466636657715  train_perf : 487.82281494140625 

Epoch: 000, Step:  2300, Loss:  3.15250683, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1778, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2299
Iteration:  84%|████████▍ | 2331/2771 [06:20<00:30, 14.40it/s]DLL 2022-02-22 18:25:36.213615 - Training Epoch: 0 Training Iteration: 2300  step_loss : 3.1525068283081055  train_perf : 487.7884216308594 

Epoch: 000, Step:  2350, Loss:  2.70953393, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1828, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2349
DLL 2022-02-22 18:25:39.561215 - Training Epoch: 0 Training Iteration: 2350  step_loss : 2.709533929824829  train_perf : 487.7663879394531 

Epoch: 000, Step:  2400, Loss:  2.87676859, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1878, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2399
Iteration:  87%|████████▋ | 2406/2771 [06:25<00:25, 14.55it/s]DLL 2022-02-22 18:25:42.921237 - Training Epoch: 0 Training Iteration: 2400  step_loss : 2.8767685890197754  train_perf : 487.7146911621094 

Epoch: 000, Step:  2450, Loss:  3.29486918, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1928, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2449
Iteration:  90%|████████▉ | 2481/2771 [06:30<00:19, 14.65it/s]DLL 2022-02-22 18:25:46.273905 - Training Epoch: 0 Training Iteration: 2450  step_loss : 3.2948691844940186  train_perf : 487.68511962890625 

Epoch: 000, Step:  2500, Loss:  3.13844442, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=8192.0, num_good_steps=1978, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2499
2022-02-22 18:25:51.103095: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:25:51.251581: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:25:51.251600: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:25:51.251620: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:25:51.251623: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:25:51.251624: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:25:51.251626: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
DLL 2022-02-22 18:25:49.628528 - Training Epoch: 0 Training Iteration: 2500  step_loss : 3.138444423675537  train_perf : 487.64984130859375 

Epoch: 000, Step:  2550, Loss:  3.09278417, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=28, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2549
Iteration:  92%|█████████▏| 2556/2771 [06:35<00:14, 14.57it/s]DLL 2022-02-22 18:25:53.168506 - Training Epoch: 0 Training Iteration: 2550  step_loss : 3.0927841663360596  train_perf : 487.4686279296875 

Epoch: 000, Step:  2600, Loss:  2.96678305, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=78, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2599
Iteration:  95%|█████████▍| 2631/2771 [06:40<00:09, 14.66it/s]DLL 2022-02-22 18:25:56.527767 - Training Epoch: 0 Training Iteration: 2600  step_loss : 2.966783046722412  train_perf : 487.4293212890625 

Epoch: 000, Step:  2650, Loss:  3.21092129, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=128, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2649
DLL 2022-02-22 18:25:59.887562 - Training Epoch: 0 Training Iteration: 2650  step_loss : 3.210921287536621  train_perf : 487.3880615234375 

Epoch: 000, Step:  2700, Loss:  2.96181440, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=178, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2699
Iteration:  98%|█████████▊| 2706/2771 [06:45<00:04, 14.73it/s]DLL 2022-02-22 18:26:03.249852 - Training Epoch: 0 Training Iteration: 2700  step_loss : 2.9618144035339355  train_perf : 487.3399353027344 

Epoch: 000, Step:  2750, Loss:  3.08309174, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=228, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2749
Iteration: 100%|█████████▉| 2770/2771 [06:50<00:00,  6.75it/s]
Iteration:   0%|          | 0/2771 [00:00<?, ?it/s]DLL 2022-02-22 18:26:06.604956 - Training Epoch: 0 Training Iteration: 2750  step_loss : 3.0830917358398438  train_perf : 487.3128356933594 
DLL 2022-02-22 18:26:07.876763 -  e2e_train_time : 410.28069281578064  training_sequences_per_second : 487.31201171875  final_loss : 3.1873226165771484 

Epoch: 001, Step:     0, Loss:  2.52349234, Perf:  461, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=248, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2769
DLL 2022-02-22 18:26:14.382254 - Training Epoch: 1 Training Iteration: 0  step_loss : 2.5234923362731934  train_perf : 460.6515197753906 

Epoch: 001, Step:    50, Loss:  2.62079954, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=298, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2819
Iteration:   3%|▎         | 71/2771 [00:05<03:12, 14.05it/s]DLL 2022-02-22 18:26:17.733140 - Training Epoch: 1 Training Iteration: 50  step_loss : 2.6207995414733887  train_perf : 486.0066223144531 

Epoch: 001, Step:   100, Loss:  2.46048450, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=348, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2869
Iteration:   5%|▌         | 146/2771 [00:10<03:03, 14.31it/s]DLL 2022-02-22 18:26:21.073940 - Training Epoch: 1 Training Iteration: 100  step_loss : 2.460484504699707  train_perf : 486.8965759277344 

Epoch: 001, Step:   150, Loss:  2.38028216, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=398, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2919
DLL 2022-02-22 18:26:24.414719 - Training Epoch: 1 Training Iteration: 150  step_loss : 2.380282163619995  train_perf : 487.27545166015625 

Epoch: 001, Step:   200, Loss:  2.48301053, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=448, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:2969
Iteration:   8%|▊         | 221/2771 [00:15<02:55, 14.51it/s]DLL 2022-02-22 18:26:27.751641 - Training Epoch: 1 Training Iteration: 200  step_loss : 2.4830105304718018  train_perf : 487.52410888671875 

Epoch: 001, Step:   250, Loss:  2.60319710, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=498, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3019
Iteration:  11%|█         | 296/2771 [00:20<02:49, 14.64it/s]DLL 2022-02-22 18:26:31.089281 - Training Epoch: 1 Training Iteration: 250  step_loss : 2.6031970977783203  train_perf : 487.71630859375 

Epoch: 001, Step:   300, Loss:  2.39881635, Perf:  488, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=548, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3069
DLL 2022-02-22 18:26:34.437944 - Training Epoch: 1 Training Iteration: 300  step_loss : 2.3988163471221924  train_perf : 487.5238342285156 

Epoch: 001, Step:   350, Loss:  2.41967607, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=598, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3119
Iteration:  13%|█▎        | 371/2771 [00:25<02:43, 14.72it/s]DLL 2022-02-22 18:26:37.786157 - Training Epoch: 1 Training Iteration: 350  step_loss : 2.4196760654449463  train_perf : 487.4356384277344 

Epoch: 001, Step:   400, Loss:  2.07740974, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=648, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3169
Iteration:  16%|█▌        | 446/2771 [00:30<02:37, 14.78it/s]DLL 2022-02-22 18:26:41.138100 - Training Epoch: 1 Training Iteration: 400  step_loss : 2.0774097442626953  train_perf : 487.2840881347656 

Epoch: 001, Step:   450, Loss:  2.13336325, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=698, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3219
DLL 2022-02-22 18:26:44.490455 - Training Epoch: 1 Training Iteration: 450  step_loss : 2.1333632469177246  train_perf : 487.17523193359375 

Epoch: 001, Step:   500, Loss:  2.45079565, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=748, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3269
Iteration:  19%|█▉        | 521/2771 [00:35<02:31, 14.82it/s]DLL 2022-02-22 18:26:47.843862 - Training Epoch: 1 Training Iteration: 500  step_loss : 2.4507956504821777  train_perf : 487.0494079589844 

Epoch: 001, Step:   550, Loss:  2.06284976, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=798, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3319
Iteration:  22%|██▏       | 596/2771 [00:40<02:26, 14.85it/s]DLL 2022-02-22 18:26:51.196950 - Training Epoch: 1 Training Iteration: 550  step_loss : 2.062849760055542  train_perf : 486.975830078125 

Epoch: 001, Step:   600, Loss:  2.12019777, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=848, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3369
DLL 2022-02-22 18:26:54.548071 - Training Epoch: 1 Training Iteration: 600  step_loss : 2.1201977729797363  train_perf : 486.9250183105469 

Epoch: 001, Step:   650, Loss:  2.81781960, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=898, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3419
Iteration:  24%|██▍       | 671/2771 [00:45<02:21, 14.87it/s]DLL 2022-02-22 18:26:57.898302 - Training Epoch: 1 Training Iteration: 650  step_loss : 2.817819595336914  train_perf : 486.8860778808594 

Epoch: 001, Step:   700, Loss:  2.38919353, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=948, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3469
Iteration:  27%|██▋       | 746/2771 [00:50<02:15, 14.89it/s]DLL 2022-02-22 18:27:01.251855 - Training Epoch: 1 Training Iteration: 700  step_loss : 2.389193534851074  train_perf : 486.8222961425781 

Epoch: 001, Step:   750, Loss:  2.03524923, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=998, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3519
DLL 2022-02-22 18:27:04.594605 - Training Epoch: 1 Training Iteration: 750  step_loss : 2.0352492332458496  train_perf : 486.8712158203125 

Epoch: 001, Step:   800, Loss:  2.74340582, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1048, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3569
Iteration:  30%|██▉       | 821/2771 [00:55<02:10, 14.90it/s]DLL 2022-02-22 18:27:07.945647 - Training Epoch: 1 Training Iteration: 800  step_loss : 2.743405818939209  train_perf : 486.8429260253906 

Epoch: 001, Step:   850, Loss:  2.88379145, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1098, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3619
Iteration:  32%|███▏      | 896/2771 [01:00<02:05, 14.90it/s]DLL 2022-02-22 18:27:11.300636 - Training Epoch: 1 Training Iteration: 850  step_loss : 2.883791446685791  train_perf : 486.77996826171875 

Epoch: 001, Step:   900, Loss:  2.58879256, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1148, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3669
DLL 2022-02-22 18:27:14.654386 - Training Epoch: 1 Training Iteration: 900  step_loss : 2.588792562484741  train_perf : 486.7298278808594 

Epoch: 001, Step:   950, Loss:  2.67009091, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1198, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3719
Iteration:  35%|███▌      | 971/2771 [01:05<02:00, 14.91it/s]DLL 2022-02-22 18:27:18.005976 - Training Epoch: 1 Training Iteration: 950  step_loss : 2.670090913772583  train_perf : 486.6955871582031 

Epoch: 001, Step:  1000, Loss:  2.60714245, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1248, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3769
Iteration:  38%|███▊      | 1046/2771 [01:10<01:55, 14.91it/s]DLL 2022-02-22 18:27:21.357173 - Training Epoch: 1 Training Iteration: 1000  step_loss : 2.607142448425293  train_perf : 486.6761779785156 

Epoch: 001, Step:  1050, Loss:  2.89759541, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1298, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3819
DLL 2022-02-22 18:27:24.711956 - Training Epoch: 1 Training Iteration: 1050  step_loss : 2.8975954055786133  train_perf : 486.62579345703125 

Epoch: 001, Step:  1100, Loss:  3.45318985, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1348, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3869
Iteration:  40%|████      | 1121/2771 [01:15<01:50, 14.91it/s]DLL 2022-02-22 18:27:28.064749 - Training Epoch: 1 Training Iteration: 1100  step_loss : 3.4531898498535156  train_perf : 486.60546875 

Epoch: 001, Step:  1150, Loss:  2.95557356, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1398, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3919
Iteration:  43%|████▎     | 1196/2771 [01:20<01:45, 14.91it/s]DLL 2022-02-22 18:27:31.415813 - Training Epoch: 1 Training Iteration: 1150  step_loss : 2.955573558807373  train_perf : 486.58428955078125 

Epoch: 001, Step:  1200, Loss:  2.50364733, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1448, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:3969
DLL 2022-02-22 18:27:34.772799 - Training Epoch: 1 Training Iteration: 1200  step_loss : 2.5036473274230957  train_perf : 486.5362243652344 

Epoch: 001, Step:  1250, Loss:  2.95250702, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1498, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4019
Iteration:  46%|████▌     | 1271/2771 [01:25<01:40, 14.91it/s]DLL 2022-02-22 18:27:38.122135 - Training Epoch: 1 Training Iteration: 1250  step_loss : 2.9525070190429688  train_perf : 486.5359802246094 

Epoch: 001, Step:  1300, Loss:  2.44887686, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1548, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4069
Iteration:  49%|████▊     | 1346/2771 [01:30<01:35, 14.91it/s]DLL 2022-02-22 18:27:41.478406 - Training Epoch: 1 Training Iteration: 1300  step_loss : 2.4488768577575684  train_perf : 486.4978332519531 

Epoch: 001, Step:  1350, Loss:  2.38105369, Perf:  487, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1598, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4119
DLL 2022-02-22 18:27:44.827192 - Training Epoch: 1 Training Iteration: 1350  step_loss : 2.3810536861419678  train_perf : 486.5074462890625 

Epoch: 001, Step:  1400, Loss:  2.54192400, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1648, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4169
Iteration:  51%|█████▏    | 1421/2771 [01:35<01:30, 14.91it/s]DLL 2022-02-22 18:27:48.181652 - Training Epoch: 1 Training Iteration: 1400  step_loss : 2.541923999786377  train_perf : 486.4909973144531 

Epoch: 001, Step:  1450, Loss:  2.14991784, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1698, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4219
Iteration:  54%|█████▍    | 1496/2771 [01:40<01:25, 14.91it/s]DLL 2022-02-22 18:27:51.538466 - Training Epoch: 1 Training Iteration: 1450  step_loss : 2.1499178409576416  train_perf : 486.4560546875 

Epoch: 001, Step:  1500, Loss:  3.28560519, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1748, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4269
DLL 2022-02-22 18:27:54.888979 - Training Epoch: 1 Training Iteration: 1500  step_loss : 3.2856051921844482  train_perf : 486.4551696777344 

Epoch: 001, Step:  1550, Loss:  2.57406759, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1798, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4319
Iteration:  57%|█████▋    | 1571/2771 [01:45<01:20, 14.91it/s]DLL 2022-02-22 18:27:58.243301 - Training Epoch: 1 Training Iteration: 1550  step_loss : 2.5740675926208496  train_perf : 486.4329528808594 

Epoch: 001, Step:  1600, Loss:  2.57334566, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1848, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4369
Iteration:  59%|█████▉    | 1646/2771 [01:50<01:15, 14.91it/s]DLL 2022-02-22 18:28:01.598046 - Training Epoch: 1 Training Iteration: 1600  step_loss : 2.57334566116333  train_perf : 486.4175109863281 

Epoch: 001, Step:  1650, Loss:  2.61596060, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1898, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4419
DLL 2022-02-22 18:28:04.953182 - Training Epoch: 1 Training Iteration: 1650  step_loss : 2.6159605979919434  train_perf : 486.3999328613281 

Epoch: 001, Step:  1700, Loss:  2.66932487, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1948, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4469
Iteration:  62%|██████▏   | 1721/2771 [01:55<01:10, 14.91it/s]DLL 2022-02-22 18:28:08.303463 - Training Epoch: 1 Training Iteration: 1700  step_loss : 2.6693248748779297  train_perf : 486.3949279785156 

Epoch: 001, Step:  1750, Loss:  2.87196350, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=16384.0, num_good_steps=1998, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4519
Iteration:  65%|██████▍   | 1796/2771 [02:00<01:05, 14.91it/s]DLL 2022-02-22 18:28:11.656558 - Training Epoch: 1 Training Iteration: 1750  step_loss : 2.8719635009765625  train_perf : 486.385498046875 

Epoch: 001, Step:  1800, Loss:  2.34838724, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=48, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4569
DLL 2022-02-22 18:28:15.008322 - Training Epoch: 1 Training Iteration: 1800  step_loss : 2.3483872413635254  train_perf : 486.3772888183594 

Epoch: 001, Step:  1850, Loss:  2.63937068, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=98, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4619
Iteration:  68%|██████▊   | 1871/2771 [02:05<01:00, 14.91it/s]DLL 2022-02-22 18:28:18.361477 - Training Epoch: 1 Training Iteration: 1850  step_loss : 2.6393706798553467  train_perf : 486.3654479980469 

Epoch: 001, Step:  1900, Loss:  2.62165689, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=148, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4669
Iteration:  70%|███████   | 1946/2771 [02:10<00:55, 14.91it/s]DLL 2022-02-22 18:28:21.714917 - Training Epoch: 1 Training Iteration: 1900  step_loss : 2.621656894683838  train_perf : 486.35723876953125 

Epoch: 001, Step:  1950, Loss:  2.48313904, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=198, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4719
DLL 2022-02-22 18:28:25.070489 - Training Epoch: 1 Training Iteration: 1950  step_loss : 2.4831390380859375  train_perf : 486.3420104980469 

Epoch: 001, Step:  2000, Loss:  2.56986666, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=248, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4769
Iteration:  73%|███████▎  | 2021/2771 [02:15<00:50, 14.90it/s]DLL 2022-02-22 18:28:28.427923 - Training Epoch: 1 Training Iteration: 2000  step_loss : 2.56986665725708  train_perf : 486.31732177734375 

Epoch: 001, Step:  2050, Loss:  2.34038186, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=298, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4819
Iteration:  76%|███████▌  | 2096/2771 [02:20<00:45, 14.90it/s]DLL 2022-02-22 18:28:31.784039 - Training Epoch: 1 Training Iteration: 2050  step_loss : 2.3403818607330322  train_perf : 486.29461669921875 

Epoch: 001, Step:  2100, Loss:  2.90681696, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=348, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4869
DLL 2022-02-22 18:28:35.137615 - Training Epoch: 1 Training Iteration: 2100  step_loss : 2.9068169593811035  train_perf : 486.2789306640625 

Epoch: 001, Step:  2150, Loss:  2.58889961, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=398, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4919
Iteration:  78%|███████▊  | 2171/2771 [02:25<00:40, 14.90it/s]DLL 2022-02-22 18:28:38.495388 - Training Epoch: 1 Training Iteration: 2150  step_loss : 2.588899612426758  train_perf : 486.2532958984375 

Epoch: 001, Step:  2200, Loss:  2.50134873, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=448, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:4969
Iteration:  81%|████████  | 2246/2771 [02:30<00:35, 14.91it/s]DLL 2022-02-22 18:28:41.851058 - Training Epoch: 1 Training Iteration: 2200  step_loss : 2.5013487339019775  train_perf : 486.2405700683594 

Epoch: 001, Step:  2250, Loss:  2.45122623, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=498, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5019
DLL 2022-02-22 18:28:45.201563 - Training Epoch: 1 Training Iteration: 2250  step_loss : 2.451226234436035  train_perf : 486.2430114746094 

Epoch: 001, Step:  2300, Loss:  2.39494753, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=548, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5069
Iteration:  84%|████████▍ | 2321/2771 [02:35<00:30, 14.90it/s]DLL 2022-02-22 18:28:48.558773 - Training Epoch: 1 Training Iteration: 2300  step_loss : 2.3949475288391113  train_perf : 486.22686767578125 

Epoch: 001, Step:  2350, Loss:  2.26147747, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=598, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5119
Iteration:  86%|████████▋ | 2396/2771 [02:40<00:25, 14.90it/s]DLL 2022-02-22 18:28:51.913758 - Training Epoch: 1 Training Iteration: 2350  step_loss : 2.261477470397949  train_perf : 486.2156982421875 

Epoch: 001, Step:  2400, Loss:  2.28507733, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=648, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5169
DLL 2022-02-22 18:28:55.269852 - Training Epoch: 1 Training Iteration: 2400  step_loss : 2.2850773334503174  train_perf : 486.2033996582031 

Epoch: 001, Step:  2450, Loss:  2.76930189, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=698, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5219
Iteration:  89%|████████▉ | 2471/2771 [02:45<00:20, 14.90it/s]DLL 2022-02-22 18:28:58.629635 - Training Epoch: 1 Training Iteration: 2450  step_loss : 2.7693018913269043  train_perf : 486.1813659667969 

Epoch: 001, Step:  2500, Loss:  2.56662846, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=748, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5269
Iteration:  92%|█████████▏| 2546/2771 [02:50<00:15, 14.90it/s]DLL 2022-02-22 18:29:01.987029 - Training Epoch: 1 Training Iteration: 2500  step_loss : 2.5666284561157227  train_perf : 486.1660461425781 

Epoch: 001, Step:  2550, Loss:  2.74409389, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=798, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5319
DLL 2022-02-22 18:29:05.344328 - Training Epoch: 1 Training Iteration: 2550  step_loss : 2.744093894958496  train_perf : 486.1515197753906 

Epoch: 001, Step:  2600, Loss:  2.86788797, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=848, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5369
Iteration:  95%|█████████▍| 2621/2771 [02:56<00:10, 14.89it/s]DLL 2022-02-22 18:29:08.701223 - Training Epoch: 1 Training Iteration: 2600  step_loss : 2.8678879737854004  train_perf : 486.1383056640625 

Epoch: 001, Step:  2650, Loss:  2.73425102, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=898, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5419
Iteration:  97%|█████████▋| 2696/2771 [03:01<00:05, 14.89it/s]DLL 2022-02-22 18:29:12.057668 - Training Epoch: 1 Training Iteration: 2650  step_loss : 2.734251022338867  train_perf : 486.12481689453125 

Epoch: 001, Step:  2700, Loss:  2.78274870, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=948, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5469
DLL 2022-02-22 18:29:15.417673 - Training Epoch: 1 Training Iteration: 2700  step_loss : 2.7827486991882324  train_perf : 486.10418701171875 

Epoch: 001, Step:  2750, Loss:  2.57051730, Perf:  486, loss_scale:DynamicLossScale(current_loss_scale=32768.0, num_good_steps=998, initial_loss_scale=32768.0, increment_period=2000, multiplier=2.0), opt_step:5519
Iteration: 100%|█████████▉| 2770/2771 [03:06<00:00, 14.89it/s]DLL 2022-02-22 18:29:18.771363 - Training Epoch: 1 Training Iteration: 2750  step_loss : 2.5705173015594482  train_perf : 486.0989685058594 
DLL 2022-02-22 18:29:20.046471 -  e2e_train_time : 596.310468673706  training_sequences_per_second : 486.0984802246094  final_loss : 2.6173834800720215 
***** Running evaluation *****
  Num Batches =  22
  Batch size =  512

Iteration:   0%|          | 0/22 [00:00<?, ?it/s]2022-02-22 18:29:29.024968: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
2022-02-22 18:31:00.345514: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:31:01.410615: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:31:01.410635: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:31:01.410641: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:31:01.410643: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:31:01.410645: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:31:01.410647: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration:   5%|▍         | 1/22 [01:36<33:48, 96.60s/it]Iteration:  14%|█▎        | 3/22 [01:42<21:41, 68.48s/it]Iteration:  23%|██▎       | 5/22 [01:48<13:49, 48.82s/it]Iteration:  32%|███▏      | 7/22 [01:54<08:46, 35.08s/it]Iteration:  41%|████      | 9/22 [02:00<05:31, 25.47s/it]Iteration:  50%|█████     | 11/22 [02:06<03:26, 18.76s/it]Iteration:  59%|█████▉    | 13/22 [02:12<02:06, 14.08s/it]Iteration:  68%|██████▊   | 15/22 [02:19<01:15, 10.81s/it]Iteration:  77%|███████▋  | 17/22 [02:25<00:42,  8.54s/it]Iteration:  86%|████████▋ | 19/22 [02:32<00:20,  6.96s/it]Iteration:  95%|█████████▌| 21/22 [02:38<00:05,  5.86s/it]2022-02-22 18:32:07.503833: W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.
Iteration:  95%|█████████▌| 21/22 [02:50<00:05,  5.86s/it]2022-02-22 18:33:23.006351: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:95] Unknown compute capability (8, 6) .Defaulting to telling LLVM that we're compiling for sm_75
2022-02-22 18:33:24.090961: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version. Custom ptxas location can be specified using $PATH.
2022-02-22 18:33:24.090980: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:71] Searched for CUDA in the following directories:
2022-02-22 18:33:24.090986: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   ./cuda_sdk_lib
2022-02-22 18:33:24.090988: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   /usr/local/cuda
2022-02-22 18:33:24.090990: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74]   .
2022-02-22 18:33:24.090992: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:76] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
Iteration: 100%|██████████| 22/22 [03:57<00:00, 27.57s/it]Iteration: 100%|██████████| 22/22 [03:57<00:00, 10.77s/it]{"exact_match": 12.620624408703879, "f1": 16.99140875290929}

Epoch: 001 Results: {"exact_match": 12.620624408703879, "f1": 16.99140875290929}

**EVAL SUMMARY** - Epoch: 001,  EM: 12.621, F1: 16.991, Infer_Perf: 1051 seq/s
**LATENCY SUMMARY** - Epoch: 001,  Ave: 442.953 ms, 90%: 442.418 ms, 95%: 442.680 ms, 99%: 442.680 ms
DLL 2022-02-22 18:33:41.666951 -  inference_sequences_per_second : 1051.3836669921875  e2e_inference_time : 251.40503644943237 
**RESULTS SUMMARY** - EM: 12.621, F1: 16.991, Train_Time:  596 s, Train_Perf:  486 seq/s, Infer_Perf: 1051 seq/s

DLL 2022-02-22 18:33:41.667722 -  exact_match : 12.620624408703879  F1 : 16.99140875290929 
====================================  END results/models/test/checkpoints/ckpt-9994  ====================================
